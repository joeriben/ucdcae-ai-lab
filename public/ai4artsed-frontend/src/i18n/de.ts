export const de = {
  app: {
    title: 'UCDCAE AI LAB',
    subtitle: 'Kreative KI-Transformationen'
  },
  form: {
    inputLabel: 'Dein Text',
    inputPlaceholder: 'z.B. Eine Blume auf der Wiese',
    schemaLabel: 'Transformationsstil',
    executeModeLabel: 'Ausführungsmodus',
    safetyLabel: 'Sicherheitsstufe',
    generateButton: 'Generieren'
  },
  schemas: {
    dada: 'Dada (Zufällig & Absurd)',
    bauhaus: 'Bauhaus (Geometrisch)',
    stillepost: 'Stille Post (Iterativ)'
  },
  safetyLevels: {
    kids: 'Kinder',
    youth: 'Jugend',
    adult: 'Erwachsene',
    research: 'Forschung'
  },
  stages: {
    pipeline_starting: 'Pipeline startet',
    translation_and_safety: 'Übersetzung & Sicherheit',
    interception: 'Transformation',
    pre_output_safety: 'Ausgabe-Sicherheit',
    media_generation: 'Bild-Generierung',
    completed: 'Abgeschlossen'
  },
  status: {
    idle: 'Bereit',
    executing: 'Pipeline läuft...',
    connectionSlow: 'Verbindung langsam, Versuch läuft...',
    completed: 'Pipeline abgeschlossen!',
    error: 'Fehler aufgetreten'
  },
  entities: {
    input: 'Eingabe',
    translation: 'Übersetzung',
    safety: 'Sicherheitscheck',
    interception: 'Transformation',
    safety_pre_output: 'Ausgabe-Sicherheit',
    media: 'Generiertes Bild'
  },
  properties: {
    chill: 'chillig',
    chaotic: 'wild',
    narrative: 'Geschichten erzählen',
    algorithmic: 'nach Regeln gehen',
    historical: 'Geschichte',
    contemporary: 'Gegenwart',
    explore: 'KI austesten',
    create: 'Kunst machen',
    playful: 'bisschen verrückt',
    serious: 'eher ernst'
  },
  phase2: {
    title: 'Prompt-Eingabe',
    userInput: 'Dein Input',
    yourInput: 'Dein Input',
    yourIdea: 'Deine Idee: Um WAS soll es hier gehen?',
    rules: 'Deine Regeln: WIE soll Deine Idee umgesetzt werden?',
    yourInstructions: 'Deine Anweisungen',
    what: 'WAS',
    how: 'WIE',
    userInputPlaceholder: 'z.B. Eine Blume auf der Wiese',
    inputPlaceholder: 'Dein Text erscheint hier...',
    metaPrompt: 'Künstlerische Anweisung',
    instruction: 'Instruction',
    transformation: 'Künstlerische Transformation',
    metaPromptPlaceholder: 'Beschreibe die Transformation...',
    result: 'Ergebnis',
    expectedResult: 'Erwartetes Ergebnis',
    execute: 'Pipeline ausführen',
    executing: 'Läuft...',
    transforming: 'LLM transformiert...',
    startTransformation: 'Transformation starten',
    letsGo: 'Ok, leg los!',
    modified: 'Geändert',
    reset: 'Zurücksetzen',
    loadingConfig: 'Lade Konfiguration...',
    loadingMetaPrompt: 'Lade Meta-Prompt...',
    errorLoadingConfig: 'Fehler beim Laden der Konfiguration',
    errorLoadingMetaPrompt: 'Fehler beim Laden des Meta-Prompts',
    threeForces: '3 Kräfte wirken zusammen',
    twoForces: 'WAS + WIE → LLM → Ergebnis',
    yourPrompt: 'Dein Prompt:',
    writeYourText: 'Schreibe deinen Text...',
    examples: 'Beispiele',
    estimatedTime: '~12 Sekunden',
    stage12Time: '~5-10 Sekunden',
    willAppearAfterExecution: 'Wird nach Ausführung erscheinen...',
    back: 'Zurück',
    retry: 'Erneut versuchen',
    transformedPrompt: 'Transformierter Prompt',
    notYetTransformed: 'Noch nicht transformiert...',
    transform: 'Transformieren',
    reTransform: 'Noch mal anders',
    startAI: 'KI, bearbeite meine Eingabe',
    aiWorking: 'KI arbeitet...',
    continueToMedia: 'Weiter zum Bild generieren',
    readyForMedia: 'Bereit für Bildgenerierung',
    stage1: 'Stage 1: Übersetzung + Sicherheit...',
    stage2: 'Stage 2: Transformation...',
    selectMedia: 'Wähle dein Medium:',
    mediaImage: 'Bild',
    mediaAudio: 'Sound',
    mediaVideo: 'Video',
    media3D: '3D',
    comingSoon: 'Bald verfügbar',
    generateMedia: 'Start!'
  },
  phase3: {
    generating: 'Bild wird generiert...',
    generatingHint: '~30 Sekunden'
  },
  common: {
    back: 'Zurück',
    loading: 'Lädt...',
    error: 'Fehler',
    retry: 'Erneut versuchen',
    cancel: 'Abbrechen',
    checkingSafety: 'Prüft...'
  },
  gallery: {
    title: 'Favoriten',  // Session 145: "Meine" redundant mit Switch
    empty: 'Noch keine Favoriten',
    favorite: 'Zu Favoriten',
    unfavorite: 'Aus Favoriten entfernen',
    continue: 'Weiterentwickeln',
    restore: 'Wiederherstellen',
    viewMine: 'Meine Favoriten',  // Session 145
    viewAll: 'Alle Favoriten'  // Session 145
  },
  settings: {
    authRequired: 'Authentifizierung erforderlich',
    authPrompt: 'Bitte geben Sie das Passwort ein, um auf die Einstellungen zuzugreifen:',
    passwordPlaceholder: 'Passwort eingeben...',
    authenticate: 'Anmelden',
    authenticating: 'Authentifiziere...',
    // Admin page
    title: 'Administration',
    tabs: {
      export: 'Forschungsdaten',
      config: 'Konfiguration',
      demos: 'Minigame-Demo',
      matrix: 'Modell-Matrix'
    },
    loading: 'Einstellungen laden...',
    presets: {
      title: 'Modell-Presets',
      help: 'Verwenden Sie den Reiter <strong>Modell-Matrix</strong>, um alle verfügbaren Presets zu sehen und mit einem Klick anzuwenden.',
      openMatrix: 'Modell-Matrix öffnen'
    },
    testingTools: {
      title: 'Testtools für Pädagog*innen',
      help: 'Testen und erkunden Sie die pädagogischen Minigames und Animationen, bevor Sie sie mit Lernenden nutzen.',
      openPreview: 'Minigame-Vorschau öffnen',
      pixelEditor: 'Pixel Template Editor',
      includes: 'Enthält: Pixel-Animation, Eisberg-Schmelzen, Wald-Spiel, Seltene Erden'
    },
    general: {
      title: 'Allgemeine Konfiguration',
      uiMode: 'UI-Modus',
      uiModeHelp: 'Komplexitätsstufe der Oberfläche',
      kids: 'Kinder (8–12)',
      youth: 'Jugend (13–17)',
      expert: 'Expert',
      safetyLevel: 'Sicherheitsstufe',
      defaultLanguage: 'Standardsprache',
      germanDe: 'Deutsch (de)',
      englishEn: 'Englisch (en)',
      turkishTr: 'Türkisch (tr)',
      koreanKo: 'Koreanisch (ko)',
      ukrainianUk: 'Ukrainisch (uk)',
      frenchFr: 'Französisch (fr)'
    },
    safety: {
      kidsTitle: 'Kinder (8–12)',
      kidsDesc: 'Alle Filter aktiv: §86a, DSGVO, Jugendschutz (altersgerechte Parameter), VLM-Bildcheck',
      youthTitle: 'Jugend (13–17)',
      youthDesc: 'Alle Filter aktiv: §86a, DSGVO, Jugendschutz (Youth-Parameter), VLM-Bildcheck',
      adultTitle: 'Erwachsene',
      adultDesc: '§86a + DSGVO aktiv. Kein Jugendschutz, kein VLM-Bildcheck.',
      researchTitle: 'Forschungsmodus',
      researchDesc: 'KEINE Sicherheitsfilter aktiv. Ausschließlich zur Nutzung durch Forschungseinrichtungen im Rahmen wissenschaftlicher Forschungsprojekte zulässig.'
    },
    safetyModels: {
      title: 'Lokale Sicherheitsmodelle',
      help: 'Lokal via Ollama — Personennamen und Safety-Checks verlassen nie das System',
      safetyModel: 'Sicherheitsmodell',
      safetyModelHelp: 'Guard-Modell für Content-Safety (§86a, Jugendschutz)',
      dsgvoModel: 'DSGVO-Verifikationsmodell',
      dsgvoModelHelp: 'General-Purpose-Modell für DSGVO-NER-Verifikation (kein Guard-Modell)',
      vlmModel: 'VLM-Sicherheitsmodell',
      vlmModelHelp: 'Vision-Modell für Post-Generierungs-Bildsicherheitsprüfung (kids/youth)',
      fast: 'schnell, minimal',
      recommended: 'empfohlen'
    },
    dsgvo: {
      title: 'DSGVO-Warnung',
      notCompliant: 'Die folgenden Modelle sind <strong>NICHT DSGVO-konform</strong> (Daten werden außerhalb der EU verarbeitet):',
      compliantHint: 'DSGVO-konforme Optionen:'
    },
    models: {
      title: 'Modellkonfiguration',
      help: 'Modell-Bezeichner mit Anbieter-Präfix: local/, mistral/, anthropic/, openai/, openrouter/',
      matrixAdvised: 'Die Nutzung der Modell-Matrix wird empfohlen. Sie können Ihre Einstellungen hier aber frei konfigurieren.',
      ollamaAvailable: '{count} Ollama-Modelle verfügbar (eingeben oder aus Dropdown wählen)',
      stage1Text: 'Stufe 1 – Textmodell',
      stage1Vision: 'Stufe 1 – Vision-Modell',
      stage2Interception: 'Stufe 2 – Interception-Modell',
      stage2Optimization: 'Stufe 2 – Optimierungsmodell',
      stage3: 'Stufe 3 – Übersetzungs-/Sicherheitsmodell',
      stage4Legacy: 'Stufe 4 – Legacy-Modell',
      chatHelper: 'Chat-Hilfsmodell',
      imageAnalysis: 'Bildanalyse-Modell',
      coding: 'Code-Generierung (Tone.js, p5.js)'
    },
    api: {
      title: 'API-Konfiguration',
      llmProvider: 'LLM-Anbieter',
      localFramework: 'Lokales LLM-Framework',
      externalProvider: 'Externer LLM-Anbieter',
      cloudProvider: 'Cloud-LLM-Anbieter – API-Schlüssel erforderlich',
      noneLocal: 'Keine (nur lokal, DSGVO)',
      mistralEu: 'Mistral AI (EU-basiert)',
      anthropicDirect: 'Anthropic Direct API (NICHT DSGVO)',
      openaiDirect: 'OpenAI Direct API (NICHT DSGVO)',
      openrouterDirect: 'OpenRouter (NICHT DSGVO, EU-Routing verfügbar)',
      mistralInfo: 'Mistral AI (EU-basiert)',
      mistralDsgvo: 'DSGVO-konform (EU-Infrastruktur)',
      anthropicInfo: 'Anthropic Direct API',
      anthropicNotDsgvo: 'NICHT DSGVO-konform',
      anthropicWarning: 'Daten werden außerhalb der EU verarbeitet. Nur für nicht-pädagogische Kontexte verwenden.',
      openaiInfo: 'OpenAI Direct API',
      openaiNotDsgvo: 'NICHT DSGVO-konform (US-basiert)',
      openaiWarning: 'Daten werden in den USA verarbeitet. Nur für nicht-pädagogische Kontexte verwenden.',
      openrouterInfo: 'OpenRouter',
      openrouterNotDsgvo: 'NICHT DSGVO-konform (US-Unternehmen)',
      openrouterWarning: 'EU-Server-Routing in den OpenRouter-Einstellungen konfigurierbar, aber Unternehmen sitzt in den USA.',
      storedIn: 'Gespeichert in',
      currentKey: 'Aktuell'
    },
    save: {
      saveApply: 'Speichern & Anwenden',
      saving: 'Speichere...',
      applying: 'Anwenden...',
      success: 'Einstellungen gespeichert und angewendet',
      presetApplied: 'Preset angewendet: {preset}'
    }
  },
  pipeline: {
    yourInput: 'Dein Input',
    result: 'Ergebnis',
    generatedMedia: 'Erzeugtes Bild'
  },
  landing: {
    subtitlePrefix: 'Pädagogisch-künstlerische Experimentierplattform des',
    subtitleSuffix: 'für den explorativen Einsatz von generativer KI in der kulturell-ästhetischen Medienbildung',
    research: '',
    features: {
      textTransformation: {
        title: 'Text-Transformation',
        description: 'Perspektivwechsel durch KI: Finde und verändere Deine Ideen durch künstlerische Haltungen und Verfremdungen.'
      },
      imageTransformation: {
        title: 'Bild-Transformation',
        description: 'Bilder durch verschiedene Modelle und Perspektiven in neue Bilder und Videos verwandeln.'
      },
      multiImage: {
        title: 'Bildfusion',
        description: 'Mehrere Bilder kombinieren und durch KI-Modelle zu neuen Bild-Kompositionen verschmelzen.'
      },
      canvas: {
        title: 'Canvas Workflow',
        description: 'Visuelle Workflow-Komposition — Module per Drag & Drop zu eigenen KI-Pipelines verbinden.'
      },
      music: {
        title: 'Musikgenerierung',
        description: 'Experimentiere mit Musik, Sound und Lyrics.'
      },
      latentLab: {
        title: 'Latent Lab',
        description: 'Vektorraum-Forschung — Surrealisierung, Dimensionselimination, Embedding-Interpolation.'
      }
    }
  },
  research: {
    locked: 'Nur im Forschungsmodus verfügbar',
    lockedHint: 'Erfordert Safety-Level „Erwachsene" oder „Forschung" (config.py)',
    complianceTitle: 'Hinweis zum Forschungsmodus',
    complianceWarning: 'Im Forschungsmodus sind keine Sicherheitsfilter für Prompts und generierte Bilder aktiv. Es können unerwartete oder unangemessene Ergebnisse entstehen.',
    complianceAge: 'Dieser Modus ist nicht empfohlen für Personen unter 16 Jahren.',
    complianceConfirm: 'Ich bestätige, dass ich die Hinweise verstanden habe',
    complianceCancel: 'Abbrechen',
    complianceProceed: 'Fortfahren'
  },
  presetOverlay: {
    title: 'Perspektive wählen',
    close: 'Schließen'
  },
  imageUpload: {
    clickHere: 'Klicke hier',
    orDragImage: 'oder ziehe ein Bild hierher',
    formatHint: 'PNG, JPG, WEBP (max 10MB)',
    invalidFormat: 'Ungültiges Dateiformat. Nur PNG, JPG und WEBP erlaubt.',
    fileTooLarge: 'Datei zu groß. Maximum: {max}MB',
    uploadFailed: 'Upload fehlgeschlagen',
    infoOriginal: 'Original:',
    infoSize: 'Größe:'
  },
  mediaInput: {
    choosePreset: 'Perspektive wählen',
    translateToEnglish: 'Ins Englische übersetzen',
    copy: 'Kopieren',
    paste: 'Einfügen',
    delete: 'Löschen',
    loading: 'Lädt...',
    contentBlocked: 'Inhalt blockiert'
  },
  nav: {
    about: 'Über das Projekt',
    impressum: 'Impressum',
    privacy: 'Datenschutz',
    docs: 'Dokumentation',
    language: 'Sprache wechseln',
    settings: 'Einstellungen',
    canvas: 'Canvas Workflow'
  },
  canvas: {
    title: 'Canvas Workflow',
    newWorkflow: 'Neuer Workflow',
    importWorkflow: 'Importieren',
    exportWorkflow: 'Exportieren',
    execute: 'Ausführen',
    ready: 'Bereit',
    errors: 'Fehler',
    discardWorkflow: 'Aktuellen Workflow verwerfen?',
    importError: 'Fehler beim Importieren der Datei',
    selectTransformation: 'Transformation wählen',
    selectOutput: 'Ausgabe-Modell wählen',
    search: 'Suchen...',
    noResults: 'Keine Ergebnisse gefunden',
    dragHint: 'Klicke oder ziehe Module auf die Arbeitsfläche',
    editNameHint: '(doppelklicken zum Bearbeiten)',
    modules: 'Module',
    toggleSidebar: 'Sidebar ein/aus',
    dsgvoTooltip: 'Canvas-Workflows können externe LLM-APIs nutzen. Die DSGVO-Konformität liegt in der Verantwortung der Nutzer:innen.',
    batchExecute: 'Batch-Ausführung',
    batchExecution: 'Batch-Ausführung',
    batchAbort: 'Batch abbrechen',
    abort: 'Abbrechen',
    cancel: 'Abbrechen',
    loading: 'Laden...',
    executingWorkflow: 'Workflow wird ausgeführt...',
    starting: 'Starte...',
    nodes: 'Knoten',
    batchRunCount: 'Anzahl Runs',
    batchUseSeed: 'Basis-Seed verwenden',
    batchBaseSeed: 'Basis-Seed',
    batchSeedHint: 'Jeder Run: Seed + Index',
    batchStart: 'Batch starten',
    stage: {
      configSelectPlaceholder: 'Auswählen...',
      evaluationCriteriaFallback: 'Bewertungskriterien...',
      feedbackInputTitle: 'Feedback-Eingang',
      deleteTitle: 'Löschen',
      selectLlmPlaceholder: 'LLM wählen...',
      resizeTitle: 'Größe ändern',
      input: {
        promptPlaceholder: 'Dein Prompt...'
      },
      imageInput: {
        uploadLabel: 'Bild hochladen'
      },
      interception: {
        contextPromptLabel: 'Context-Prompt',
        contextPromptPlaceholder: 'Transformations-Anweisungen...'
      },
      translation: {
        translationPromptLabel: 'Übersetzungs-Prompt',
        translationPromptPlaceholder: 'Übersetzungsanweisungen...'
      },
      modelAdaption: {
        targetModelLabel: 'Zielmodell',
        noAdaptionOption: 'Keine Adaption',
        videoModelsOption: 'Video-Modelle',
        audioModelsOption: 'Audio-Modelle'
      },
      comparisonEvaluator: {
        criteriaLabel: 'Vergleichs-Kriterien',
        criteriaPlaceholder: 'z.B. Vergleiche nach Originalität, Klarheit, Detailreichtum...',
        infoText: 'Verbinde bis zu 3 Text-Outputs'
      },
      seed: {
        modeLabel: 'Modus',
        modeFixed: 'Fest',
        modeRandom: 'Zufällig',
        valueLabel: 'Wert',
        baseLabel: 'Basis'
      },
      resolution: {
        customOption: 'Benutzerdefiniert',
        widthLabel: 'Breite',
        heightLabel: 'Höhe'
      },
      collector: {
        emptyText: 'Warte auf Ausführung...'
      },
      evaluation: {
        typeLabel: 'Bewertungstyp',
        typeCreativity: 'Kreativität',
        typeQuality: 'Qualität',
        typeCustom: 'Eigene',
        criteriaLabel: 'Bewertungskriterien',
        outputTypeLabel: 'Ausgabe-Typ',
        outputCommentary: 'Kommentar + Binary',
        outputScore: 'Kommentar + Score + Binary',
        outputAll: 'Alle',
        evalPassTitle: 'Bestanden (weiter)',
        evalFailTitle: 'Feedback (Rückkanal)',
        evalCommentaryTitle: 'Kommentar (weiter)'
      },
      imageEvaluation: {
        visionModelPlaceholder: 'Vision-Modell wählen...',
        frameworkLabel: 'Analyse-Framework',
        frameworkPanofsky: 'Kunsthistorisch (Panofsky)',
        frameworkEducational: 'Bildungstheoretisch',
        frameworkEthical: 'Ethisch',
        frameworkCritical: 'Kritisch/Dekolonial',
        frameworkCustom: 'Eigene Anweisung',
        customPromptLabel: 'Analyse-Prompt',
        customPromptPlaceholder: 'Beschreibe, wie das Bild analysiert werden soll...'
      },
      display: {
        imageAlt: 'Vorschau',
        emptyText: 'Vorschau (nach Ausführung)'
      }
    }
  },
  about: {
    title: 'Über das UCDCAE AI LAB',
    intro: 'Das UCDCAE AI LAB ist eine pädagogisch-künstlerische Experimentierplattform des UNESCO Chair in Digital Culture and Arts in Education für den explorativen Einsatz von generativer Künstlicher Intelligenz in der kulturell-ästhetischen Medienbildung. Es wurde im Rahmen der Projekte AI4ArtsEd und COMeARTS entwickelt.',
    project: {
      title: 'Das Projekt',
      description: 'KI verändert Gesellschaft und Arbeitswelt; sie wird zunehmend Thema der Bildung. Das Projekt sondiert Chancen, Bedingungen und Grenzen des pädagogischen Einsatzes künstlicher Intelligenz (KI) in kulturell diversitätssensiblen Settings der Kulturellen Bildung (KuBi).',
      paragraph2: 'In drei Teilprojekten – Allgemeinpädagogik (TPap), Informatik (TPinf) und Kunstpädagogik (TPkp) – greifen kreativitätsorientierte pädagogische KI-Praxisforschung und informatische KI-Konzeption und Programmierung in enger Kooperation ineinander. Das Projekt bezieht hierzu von Beginn an künstlerisch-pädagogische Praxisakteure in den Gestaltungsprozess systematisch ein; es agiert als Brücke zwischen der professionellen (qualitätsbezogenen, ästhetischen, ethischen und wertebezogenen) pädagogisch-praktischen Implementation einerseits und dem Umsetzungs- und Trainingsprozess des informatischen Teilprojekts andererseits.',
      paragraph3: 'Aus einem insgesamt ca. zweijährigen partizipativen Designprozess soll eine Opensource-KI-Technologie hervorgehen, die auslotet, inwieweit KI-Systeme unter günstigen Realbedingungen bereits auf ihrer Strukturebene künstlerisch-pädagogische Maßgaben einbeziehen können.',
      paragraph4: 'Dabei stehen a) die zukünftige Anwendbarkeit und der Mehrgewinn hochinnovativer Technologien für die Kulturelle Bildung im Zentrum, b) Reichweite und Grenzen der KI-Literacy von Lehrenden und Lernenden, sowie c) die übergreifende Frage nach der Bewertbarkeit und Bewertung der Transformation pädagogischer Settings durch komplexe nonhumane Akteure im Sinne einer pädagogischen Ethik und Technikfolgenabschätzung.',
      moreInfo: 'Weitere Informationen:'
    },
    subproject: {
      title: 'Teilprojekt "Allgemeine Pädagogik"',
      description: 'Das Teilprojekt "Allgemeine Pädagogik" beforscht im Rahmen der dem Verbundprojekt gemeinsamen Fragestellung Möglichkeiten und Grenzen eines auf partizipativer Praxisforschung aufsetzenden künstlerisch-pädagogischen KI-Designprozesses. Es führt zu diesem Zweck im ersten Projektjahr eine Serie von Recherchen, Analysen, Expert_innenworkshops und OpenSpaces durch. Die nachfolgende, in mehreren Zyklen als Feedback-Loop angelegte Projektphase erforscht den Einsatz eines Prototypen mit pädagogischen Prakter_innen und Artist-Educators v.a. der non-formalen kulturellen Bildung als relationalen und kollektiven transformativen Bildungsprozess.'
    },
    team: {
      title: 'Team',
      projectLead: 'Projektleitung',
      leadName: 'Prof. Dr. Benjamin Jörissen',
      leadInstitute: 'Institut für Pädagogik',
      leadChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
      leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
      researcher: 'Wissenschaftliche Mitarbeiterin',
      researcherName: 'Vanessa Baumann',
      researcherInstitute: 'Institut für Pädagogik',
      researcherChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
      researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
    },
    funding: {
      title: 'Gefördert vom'
    }
  },
  legal: {
    impressum: {
      title: 'Impressum',
      publisher: 'Herausgeber',
      represented: 'Vertreten durch den Präsidenten',
      responsible: 'Inhaltlich verantwortlich gem. § 18 Abs. 2 MStV',
      authority: 'Zuständige Aufsichtsbehörde',
      moreInfo: 'Weitere Informationen',
      moreInfoText: 'Das vollständige Impressum der FAU:',
      funding: 'Gefördert vom'
    },
    privacy: {
      title: 'Datenschutzerklärung',
      notice: 'Hinweis: Generierte Inhalte werden zu Forschungszwecken auf dem Server gespeichert. Es werden keine User- oder IP-Daten erfasst. Hochgeladene Bilder werden nicht gespeichert.',
      usage: 'Die Nutzung dieser Plattform ist ausschließlich eingetragenen Kooperationspartnern des UCDCAE AI LAB erlaubt. Es gelten die in diesem Rahmen vereinbarten datenschutzbezogenen Absprachen. Haben Sie hierzu Fragen, melden Sie sich bitte bei vanessa.baumann@fau.de.'
    }
  },
  docs: {
    title: 'Dokumentation & Anleitung',
    intro: {
      title: 'Willkommen',
      content: 'Kreative Experimente mit KI-Transformationen.'
    },
    gettingStarted: {
      title: 'Erste Schritte',
      step1: 'Eigenschaften aus Quadranten wählen',
      step2: 'Text oder Bild eingeben',
      step3: 'Transformation starten'
    },
    modes: {
      title: 'Modi',
      mode1: { name: 'Direkt', desc: 'Schnelle Experimente' },
      mode2: { name: 'Text', desc: 'Textbasierte Transformationen' },
      mode3: { name: 'Bild', desc: 'Bildbasierte Verfahren' }
    },
    support: {
      title: 'Unterstützung',
      content: 'Bei Fragen:'
    },
    wikipedia: {
      title: 'Wikipedia-Recherche',
      subtitle: 'Wissen über die Welt als Teil künstlerischer Prozesse',
      feature: 'Künstlerische Prozesse erfordern nicht nur ästhetisches Wissen, sondern auch Wissen über Sachverhalte in der Welt. Die KI recherchiert während der Transformation auf Wikipedia, um faktische Informationen zu finden.',
      languages: 'Über 70 Sprachen werden unterstützt',
      languagesDesc: 'Die KI wählt automatisch die passende sprachliche Wikipedia für das jeweilige Thema:',
      examples: {
        nigeria: 'Thema über Nigeria → Hausa, Yoruba, Igbo oder Englisch',
        india: 'Thema über Indien → Hindi, Tamil, Bengali oder andere regionale Sprachen',
        indigenous: 'Indigene Kulturen → Quechua, Māori, Inuktitut usw.'
      },
      why: 'Transparenz: Was weiß die KI?',
      whyDesc: 'Das System zeigt alle Recherche-Versuche an: Sowohl gefundene Artikel (als anklickbare Links) als auch Begriffe, zu denen nichts gefunden wurde. So wird sichtbar, was die KI zu wissen meint – und was nicht.',
      culturalRespect: 'Einladung zum Selbst-Recherchieren',
      culturalRespectDesc: 'Die angezeigten Wikipedia-Links sind eine Einladung, selbst mehr zu erfahren. Klicken Sie auf die Links, um die Quellen zu prüfen und Ihr eigenes Wissen zu erweitern.',
      limitations: 'Die KI-Recherche ist ein Hilfsmittel, kein Ersatz für eigene Auseinandersetzung mit dem Thema.'
    }
  },
  multiImage: {
    image1Label: 'Bild 1',
    image2Label: 'Bild 2 (optional)',
    image3Label: 'Bild 3 (optional)',
    contextLabel: 'Sage was Du mit den Bildern machen möchtest',
    contextPlaceholder: 'z.B. Füge das Haus aus Bild 2 und das Pferd aus Bild 3 in Bild 1 ein. Behalte Farben und Stil von Bild 1 bei.',
    modeTitle: 'Mehrere Bilder → Bild',
    selectConfig: 'Wähle dein Modell:',
    generating: 'Bilder werden fusioniert...'
  },
  imageTransform: {
    imageLabel: 'Dein Bild',
    contextLabel: 'Sage was Du an dem Bild verändern möchtest',
    contextPlaceholder: 'z.B. Verwandle es in ein Ölgemälde... Mache es bunter... Füge einen Sonnenuntergang hinzu...'
  },
  textTransform: {
    inputLabel: 'Deine Idee = WAS?',
    inputTooltip: 'Hier trägst Du ein, worum es gehen soll.',
    inputPlaceholder: 'z.B. Ein Fest in meiner Straße: ...',
    contextLabel: 'Deine Regeln = WIE?',
    contextTooltip: 'Hier trägst Du ein, wie Deine Idee dargestellt werden soll, oder klicke auf das Kreis-Symbol!',
    contextPlaceholder: 'z.B. Beschreibe alles so, wie es die Vögel auf den Bäumen wahrnehmen!',
    resultLabel: 'Idee + Regeln = Prompt',
    resultPlaceholder: 'Prompt erscheint nach Start-Klick (oder eigenen Text eingeben)',
    optimizedLabel: 'Modell-Optimierter Prompt',
    optimizedPlaceholder: 'Der optimierte Prompt erscheint nach Modellauswahl.'
  },
  training: {
    info: {
      title: 'Hinweis zum LoRA-Training',
      studioDescription: 'Trainiere eigene LoRA-Modelle für Stable Diffusion 3.5 Large mit deinen Bildern.',
      description: 'Dieses eingebaute Training ist für schnelle Tests gedacht.',
      limitations: 'Einschränkungen',
      limitationDuration: 'Training dauert 1-3 Stunden',
      limitationBlocking: 'Blockiert die Bildgenerierung während des Trainings',
      limitationConfig: 'Begrenzte Konfigurationsmöglichkeiten',
      showMore: 'Mehr erfahren',
      showLess: 'Weniger anzeigen'
    },
    placeholders: {
      projectName: 'z.B. Unser Schulgebäude',
      triggerWords: 'z.B. unser_schulgebaeude, schulhof, klassenzimmer'
    },
    labels: {
      projectName: 'Projektname',
      triggerWords: 'Trigger-Wörter',
      triggerHelp: 'Kommagetrennte Tags. Erstes = Haupt-Trigger, Rest = zusätzliche Tags pro Bild.',
      images: 'Trainingsbilder (10–50 empfohlen)',
      dropZone: 'Bilder hierher ziehen oder klicken',
      imagesSelected: '{count} Bilder ausgewählt',
      logs: 'Trainings-Log',
      waiting: 'Warte auf Trainingsstart...'
    },
    buttons: {
      start: 'Training starten',
      stop: 'Stopp',
      inProgress: 'Training läuft...',
      delete: 'Projektdaten löschen (DSGVO)',
      cancel: 'Abbrechen'
    },
    vram: {
      title: 'GPU VRAM Prüfung',
      checking: 'Prüfe VRAM...',
      used: 'belegt',
      free: 'frei',
      notEnough: 'Nicht genügend freier VRAM für das Training (benötigt {gb} GB).',
      clearQuestion: 'VRAM freigeben um fortzufahren?',
      enough: 'Genügend VRAM für das Training verfügbar.',
      clearing: 'Gebe VRAM frei...',
      newFree: 'Neu verfügbar',
      clearBtn: 'ComfyUI + Ollama VRAM freigeben'
    }
  },
  safetyBadges: {
    '§86a': '§86a',
    '86a_filter': '§86a',
    age_filter: 'Altersfilter',
    dsgvo_ner: 'DSGVO',
    dsgvo_llm: 'DSGVO',
    translation: '\u2192 EN',
    fast_filter: 'Inhalt',
    llm_context_check: 'Inhalt (LLM)',
    llm_safety_check: 'Jugendschutz',
    llm_check_failed: 'Pr\u00FCfung fehlgeschlagen',
    disabled: '\u2014'
  },
  safetyBlocked: {
    vlm: 'Dein Prompt war in Ordnung, aber das erzeugte Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Das kann passieren \u2014 die Bildgenerierung ist nicht immer vorhersagbar. Versuche es einfach nochmal, jede Generierung ist anders!',
    para86a: 'Dein Prompt wurde blockiert, weil er Symbole oder Begriffe enth\u00E4lt, die nach deutschem Recht (\u00A786a StGB) verboten sind. Diese Regel sch\u00FCtzt uns alle vor Hass und Gewalt. Versuche es mit einem anderen Thema!',
    dsgvo: 'Dein Prompt wurde blockiert, weil er etwas enth\u00E4lt, das wie ein Personenname aussieht. Das ist durch die Datenschutzgrundverordnung (DSGVO) gesch\u00FCtzt. Verwende stattdessen Beschreibungen wie \"ein M\u00E4dchen\" oder \"ein alter Mann\" statt Namen.',
    kids: 'Dein Prompt wurde vom Kinder-Schutzfilter blockiert. Manche Begriffe sind f\u00FCr Kinder nicht geeignet, weil sie erschreckend oder verst\u00F6rend sein k\u00F6nnen. Versuche, deine Idee mit freundlicheren Worten zu beschreiben!',
    youth: 'Dein Prompt wurde vom Jugendschutzfilter blockiert. Manche Inhalte sind auch f\u00FCr Jugendliche nicht geeignet. Versuche, deine Idee anders zu formulieren!',
    generic: 'Dein Prompt wurde vom Sicherheitssystem blockiert. Das System sch\u00FCtzt dich vor ungeeigneten Inhalten. Versuche es mit einer anderen Formulierung!',
    inputImage: 'Das hochgeladene Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Bitte verwende ein anderes Bild.',
    vlmSaw: 'Die Bild-KI sah',
    systemUnavailable: 'Das Sicherheitssystem (Ollama) reagiert nicht, daher kann keine weitere Verarbeitung erfolgen. Bitte den Systemadministrator kontaktieren.',
    suggestionLoading: 'Moment, ich habe eine Idee...',
    suggestionError: 'Ich konnte gerade keinen Vorschlag generieren. Versuch es einfach nochmal anders!'
  },
  splitCombine: {
    infoTitle: 'Split & Combine - Semantische Vektorfusion',
    infoDescription: 'Dieser Workflow fusioniert zwei Prompts auf der Ebene semantischer Vektoren. Das Ergebnis ist keine einfache Mischung, sondern eine tiefere mathematische Verbindung der Bedeutungsräume.',
    purposeTitle: 'Pädagogischer Zweck',
    purposeText: 'Erkunde, wie KI-Modelle Bedeutung als Zahlenräume repräsentieren. Was passiert, wenn wir verschiedene Konzepte mathematisch verschmelzen?',
    techTitle: 'Technische Details',
    techText: 'Modell: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
  },
  partialElimination: {
    infoTitle: 'Partial Elimination - Vektor-Dekonstruktion',
    infoDescription: 'Dieser Workflow manipuliert gezielt Teile des semantischen Vektors. Durch das Eliminieren bestimmter Dimensionen können wir beobachten, welche Aspekte der Bedeutung verloren gehen.',
    purposeTitle: 'Pädagogischer Zweck',
    purposeText: 'Verstehe, wie Bedeutung in verschiedenen Dimensionen des Vektorraums kodiert ist. Was bleibt übrig, wenn wir Teile "ausschalten"?',
    techTitle: 'Technische Details',
    techText: 'Modell: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
    encoderLabel: 'Text-Encoder',
    modeLabel: 'Eliminationsmodus',
    dimensionRange: 'Dimensions-Bereich',
    selected: 'Ausgewählt',
    dimensions: 'Dimensionen',
    emptyTitle: 'Warte auf Generierung...',
    emptySubtitle: 'Die Ergebnisse erscheinen hier',
    referenceLabel: 'Referenzbild',
    referenceDesc: 'Unmanipulierte Ausgabe (Original)',
    innerLabel: 'Innerer Bereich eliminiert',
    outerLabel: 'Äußerer Bereich eliminiert'
  },
  surrealizer: {
    infoTitle: 'Surrealisierer — Extrapolation jenseits des Bekannten',
    infoDescription: 'Zwei KI-"Gehirne" lesen deinen Text: CLIP-L versteht Sprache durch Bilder, T5 versteht sie rein sprachlich. Der Regler mischt nicht einfach zwischen beiden — er schiebt das Bild weit über das hinaus, was T5 allein erzeugen würde. Die KI muss dann Vektoren interpretieren, die sie im Training nie gesehen hat. Das Ergebnis: KI-Halluzinationen — Bilder, die kein Prompt direkt erzeugen könnte.',
    purposeTitle: 'Der Regler',
    purposeText: 'α < 0: CLIP-L wird verstärkt, T5 negiert — die oberen 3328 Dimensionen (wo CLIP-L nur Nullen hat) erhalten invertierte T5-Vektoren. Die Cross-Attention-Muster im Transformer kehren sich um: visuell getriebene Halluzinationen. ◆ α = 0: reines CLIP-L — normales Bild. ◆ α = 1: reines T5-XXL — noch normal, aber andere Qualität. ◆ α > 1: Extrapolation über T5 hinaus. Bei α = 20 schiebt die Formel das Embedding 19× über T5 hinweg in unerforschten Vektorraum — sprachlich getriebene Halluzinationen. ◆ Sweet Spot: α = 15–35.',
    techTitle: 'Wie es funktioniert',
    techText: 'Dein Prompt wird getrennt durch zwei Encoder geschickt: CLIP-L (visuell trainiert, 77 Tokens, 768 Dimensionen → aufgefüllt auf 4096) und T5-XXL (sprachlich trainiert, 512 Tokens, 4096 Dimensionen). Die ersten 77 Token-Positionen werden per Formel fusioniert: (1-α)·CLIP-L + α·T5. Die restlichen T5-Tokens (78–512) bleiben unverändert als semantischer Anker — sie halten das Bild an deinem Text fest, egal wie extrem α wird. Bei α > 1 entsteht keine Mischung, sondern Extrapolation: Vektoren, die kein Training je erzeugt hat. Bei α < 0 wird T5 negiert und CLIP-L verstärkt — qualitativ andere Halluzinationen, weil die Cross-Attention-Muster im Transformer invertiert werden.',
    sliderLabel: 'Extrapolation (α)',
    sliderNormal: 'normal',
    sliderWeird: 'weird',
    sliderCrazy: 'crazy',
    sliderExtremeWeird: 'super weird',
    sliderExtremeCrazy: 'super crazy',
    sliderHint: "α<0: über CLIP hinaus {'|'} α=0: reines CLIP {'|'} α=1: reines T5 {'|'} α>1: über T5 hinaus",
    expandLabel: 'Prompt für T5 erweitern',
    expandSuggest: 'Kurzer Prompt erkannt — T5-Erweiterung verbessert die Ergebnisse bei wenigen Wörtern deutlich.',
    expandHint: 'Dein Prompt hat wenige Wörter (~{count} CLIP-Tokens). Für optimale Halluzinationen kann die KI den T5-Kontext narrativ erweitern.',
    expandActive: 'Erweitere Prompt...',
    expandResultLabel: 'T5-Erweiterung (nur für T5-Encoder)',
    advancedLabel: 'Weitere Einstellungen',
    negativeLabel: 'Negativ-Prompt',
    negativeHint: 'Wird mit gleichem α extrapoliert. Bestimmt, woVON das Bild weg-extrapoliert wird — verschiedene Negativ-Prompts erzeugen grundlegend verschiedene Bildästhetiken.',
    cfgLabel: 'CFG Scale',
    cfgHint: 'Classifier-Free Guidance: Stärke des Prompt-Einflusses. Höher = stärkerer Effekt, weniger Variation.'
  },
  musicGeneration: {
    infoTitle: 'Musik-Generierung',
    infoDescription: 'Erstelle Musik aus Texten und Style-Tags. Die KI generiert Melodien, Rhythmen und Harmonien basierend auf deinen Lyrics und Genre-Angaben.',
    purposeTitle: 'Pädagogischer Zweck',
    purposeText: 'Erkunde wie KI musikalische Konzepte interpretiert. Wie beeinflusst die Wortwahl in den Lyrics die Melodie?',
    lyricsLabel: 'Lyrics (Text)',
    lyricsPlaceholder: '[Verse]\nDeine Lyrics hier...\n\n[Chorus]\nRefrain...',
    tagsLabel: 'Style Tags',
    tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
    selectModel: 'Wähle ein Musik-Modell:',
    generate: 'Musik generieren',
    generating: 'Musik wird generiert...'
  },
  musicGen: {
    simpleMode: 'Einfach',
    advancedMode: 'Erweitert',
    lyricsLabel: 'Lyrics',
    lyricsPlaceholder: 'Schreibe deine Song-Lyrics mit Strukturmarkern wie [Verse], [Chorus], [Bridge]...\n\nBeispiel:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
    tagsLabel: 'Style Tags',
    tagsPlaceholder: 'Genre, Stimmung, Instrumente...\n\nBeispiel: ska, aggressive, upbeat, high definition, bass and sax trio',
    refineButton: 'Lyrics & Tags verfeinern',
    refinedLyricsLabel: 'Verfeinerte Lyrics',
    refinedLyricsPlaceholder: 'Hier erscheinen deine verfeinerten Lyrics...',
    refiningLyricsMessage: 'Die KI verfeinert deine Lyrics...',
    refinedTagsLabel: 'Verfeinerte Tags',
    refinedTagsPlaceholder: 'Hier erscheinen die verfeinerten Style Tags...',
    refiningTagsMessage: 'Die KI generiert passende Style Tags...',
    selectModel: 'Wähle ein Musik-Modell',
    generateButton: 'Musik generieren',
    quality: 'Qualität'
  },
  musicGenV2: {
    lyricsWorkshop: 'Lyrics Workshop',
    lyricsInput: 'Dein Text',
    lyricsPlaceholder: 'Schreibe Lyrics, ein Thema, Stichworte oder eine Stimmung...',
    themeToLyrics: 'Stichworte zu Songtext',
    refineLyrics: 'Songtext strukturieren',
    resultLabel: 'Ergebnis',
    resultPlaceholder: 'Hier erscheinen deine Lyrics...',
    expandingTheme: 'Die KI schreibt einen Songtext aus deinen Stichworten...',
    refiningLyrics: 'Die KI strukturiert deinen Songtext...',
    soundExplorer: 'Sound Explorer',
    suggestFromLyrics: 'Aus Lyrics vorschlagen',
    suggestingTags: 'Die KI analysiert deine Lyrics...',
    mostImportant: 'wichtigste',
    dimGenre: 'Genre',
    dimTimbre: 'Klangfarbe',
    dimGender: 'Stimme',
    dimMood: 'Stimmung',
    dimInstrument: 'Instrumente',
    dimScene: 'Szene',
    dimRegion: 'Region (UNESCO)',
    dimTopic: 'Thema',
    audioLength: 'Audio-Länge',
    generateButton: 'Musik generieren',
    selectModel: 'Modell',
    customTags: 'Eigene Tags',
    customTagsPlaceholder: 'z.B. acoustic,dreamy,summer_vibes'
  },
  latentLab: {
    tabs: {
      image: 'Image Lab',
      textlab: 'Latent Text Lab',
      crossmodal: 'Crossmodal Lab'
    },
    imageLab: {
      headerTitle: 'Image Lab — Visuelle Vektorraumforschung',
      headerSubtitle: 'Fünf Werkzeuge zur Untersuchung, wie Diffusionsmodelle Bilder aus Text erzeugen: von der Entrauschung über Attention und Fusion bis zur Vektorarithmetik.',
      tabs: {
        archaeology: {
          label: 'Denoising Archaeology',
          short: 'Dem Modell beim Arbeiten zusehen'
        },
        attention: {
          label: 'Attention Cartography',
          short: 'Sehen, wohin das Modell schaut'
        },
        fusion: {
          label: 'Encoder Fusion',
          short: 'Surrealistische Verschmelzung'
        },
        probing: {
          label: 'Feature Probing',
          short: 'Dimensionsanalyse'
        },
        algebra: {
          label: 'Concept Algebra',
          short: 'Vektorarithmetik'
        }
      }
    },
    comingSoon: 'Dieses Tool wird in einer zukünftigen Version implementiert.',
    shared: {
      negativeHint: 'Begriffe, die das Modell aktiv vermeiden soll (z.B. "verschwommen, Text")',
      stepsHint: 'Mehr Schritte = höhere Qualität, aber längere Generierung',
      cfgHint: 'Classifier-Free Guidance: höher = stärker am Prompt orientiert, weniger Variation',
      seedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
      recordingActive: 'Aufzeichnung aktiv',
      recordingCount: '{count} Aufzeichnung | {count} Aufzeichnungen',
      recordingTooltip: 'Forschungsdaten werden automatisch gespeichert',
    },
    attention: {
      headerTitle: 'Attention Cartography — Welches Wort steuert welche Bildregion?',
      headerSubtitle: 'Für jedes Wort im Prompt zeigt eine Heatmap-Überlagerung auf dem generierten Bild, WO im Bild dieses Wort den größten Einfluss hatte. So wird sichtbar, wie das Modell semantische Konzepte räumlich verteilt.',
      explanationToggle: 'Ausführliche Erklärung anzeigen',
      explainWhatTitle: 'Was zeigt dieses Tool?',
      explainWhatText: 'Wenn ein Diffusionsmodell ein Bild erzeugt, liest es den Prompt nicht Wort für Wort ab wie eine Bauanleitung. Stattdessen verteilt ein Mechanismus namens „Attention" den Einfluss jedes Wortes auf verschiedene Bildregionen. Das Wort „Haus" beeinflusst hauptsächlich die Region, in der das Haus entsteht — aber auch benachbarte Bereiche, weil das Modell den Kontext der gesamten Szene versteht. Dieses Tool macht diese Verteilung sichtbar: Klicke auf ein Wort und sieh, welche Bildregionen aufleuchten.',
      explainHowTitle: 'Wie lese ich die Heatmap?',
      explainHowText: 'Helle, intensive Farbe = starker Einfluss des Wortes auf diese Region. Dunkle oder fehlende Farbe = wenig Einfluss. Wenn du mehrere Wörter auswählst, erscheinen sie in verschiedenen Farben. Beachte: Die Karten sind NICHT perfekt scharf begrenzt — das ist kein Fehler, sondern zeigt, dass das Modell Konzepte kontextuell und nicht isoliert verarbeitet. Ein „Haus" in einer Bauernhof-Szene hat auch etwas Einfluss auf Tiere und Felder, weil das Modell die Szene als Ganzes versteht.',
      explainReadTitle: 'Was verraten die zwei Regler?',
      explainReadText: 'Der Entrauschungsschritt-Regler zeigt, WANN im 25-schrittigen Erzeugungsprozess du die Attention betrachtest. Frühe Schritte zeigen die grobe Layoutplanung, späte die Detailzuordnung. Der Netzwerktiefe-Regler zeigt, WO im Transformer die Attention gemessen wird: Flache Schichten (nahe am Eingang) zeigen globale Kompositionsplanung, mittlere die semantische Zuordnung, tiefe die Feinabstimmung. Beide Achsen sind unabhängig — es lohnt sich, systematisch verschiedene Kombinationen zu erkunden.',
      techTitle: 'Technische Details',
      techText: 'SD3.5 verwendet einen MMDiT (Multimodal Diffusion Transformer) mit Joint Attention: Bild- und Text-Tokens bearbeiten sich gegenseitig in 24 Transformer-Blöcken. Wir ersetzen den Standard-SDPA-Prozessor durch einen manuellen Softmax(QK^T/√d)-Prozessor an 3 ausgewählten Blöcken, um die Text→Bild-Attention-Submatrix zu extrahieren. Die Maps haben 64x64 Auflösung (Patch-Grid) und werden per bilinearer Interpolation auf die Bildauflösung hochskaliert. SD3.5 nutzt zwei Text-Encoder: CLIP-L (BPE, 77 Tokens) und T5-XXL (SentencePiece, 512 Tokens). Beide können hier umgeschaltet werden, um zu sehen, wie unterschiedliche Tokenisierungen die Attention beeinflussen.',
      referencesTitle: 'Forschungsgrundlagen',
      promptLabel: 'Prompt',
      promptPlaceholder: 'z.B. Ein Haus steht in einer Landschaft, umgeben von landwirtschaftlichen Flächen, Natur und Tieren. Es sind einige Menschen zu sehen.',
      generate: 'Generieren + Analyse',
      generating: 'Bild wird generiert und Attention wird extrahiert...',
      emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um die Attention-Karten des Modells zu visualisieren.',
      advancedLabel: 'Erweiterte Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      tokensLabel: 'Tokens',
      tokensHint: 'Klicke auf ein oder mehrere Wörter. Subwort-Tokens (z.B. "Ku"+"gel") werden automatisch zusammengefasst. Mehrere Wörter erscheinen in verschiedenen Farben.',
      timestepLabel: 'Entrauschungsschritt',
      timestepHint: 'Diffusionsmodelle erzeugen Bilder in 25 Schritten vom Rauschen zum Bild. Frühe Schritte legen die grobe Struktur fest, späte verfeinern Details. Dieser Regler zeigt, worauf das Modell bei welchem Schritt achtet.',
      step: 'Schritt',
      layerLabel: 'Netzwerktiefe',
      layerHint: 'Bei jedem Entrauschungsschritt durchläuft das Signal alle 24 Schichten des Transformers. Flache Schichten (nahe am Eingang) erfassen globale Komposition, mittlere die semantische Zuordnung, tiefe (nahe am Ausgang) feine Details. Beide Regler sind unabhängig: Schritt = wann im Prozess, Tiefe = wo im Netzwerk.',
      layerEarly: 'Flach (Komposition)',
      layerMid: 'Mittel (Semantik)',
      layerLate: 'Tief (Detail)',
      opacityLabel: 'Heatmap',
      opacityHint: 'Stärke der farbigen Überlagerung auf dem Bild.',
      baseImageLabel: 'Basisbild',
      baseColor: 'Farbe',
      baseBW: 'S/W',
      baseOff: 'Aus',
      baseImageHint: 'Farbe zeigt das Originalbild. S/W entsättigt es, damit Heatmap-Farben klar erkennbar sind. Aus blendet das Bild aus und zeigt nur die Attention-Karte.',
      encoderLabel: 'Text-Encoder',
      encoderClipL: 'CLIP-L (77 Tokens)',
      encoderT5: 'T5-XXL (512 Tokens)',
      encoderHint: 'SD3.5 nutzt zwei Text-Encoder mit unterschiedlicher Tokenisierung. CLIP-L verwendet BPE (Byte-Pair-Encoding), T5-XXL SentencePiece. Vergleiche, wie beide Encoder denselben Prompt verarbeiten und welche Bildregionen sie jeweils steuern.',
      download: 'Bild herunterladen'
    },
    probing: {
      headerTitle: 'Feature Probing — Welche Dimensionen kodieren was?',
      headerSubtitle: 'Vergleiche zwei Prompts und finde heraus, welche Embedding-Dimensionen den semantischen Unterschied kodieren. Übertrage gezielt einzelne Dimensionen, um zu sehen, wie sie das Bild verändern.',
      explanationToggle: 'Ausführliche Erklärung anzeigen',
      explainWhatTitle: 'Was zeigt dieses Tool?',
      explainWhatText: 'Jedes Wort wird vom Text-Encoder in einen hochdimensionalen Vektor umgewandelt (z.B. 4096 Dimensionen bei T5). Wenn du ein Wort im Prompt änderst — z.B. „rotes" zu „blaues" — ändern sich bestimmte Dimensionen stärker als andere. Dieses Tool zeigt dir, WELCHE Dimensionen sich am meisten ändern und lässt dich gezielt einzelne Dimensionen von Prompt B in Prompt A übertragen.',
      explainHowTitle: 'Wie funktioniert die Übertragung?',
      explainHowText: 'Im Balkendiagramm siehst du alle Dimensionen sortiert nach Differenzgröße. Mit den Rang-Reglern (Von/Bis) wählst du einen Bereich aus — z.B. nur die Top-100 oder gezielt Rang 880–920. Beim Klick auf „Übertragen" wird das Bild mit denselben Einstellungen (gleicher Seed!) neu generiert — aber mit den ausgewählten Dimensionen aus Prompt B. So siehst du exakt, was diese Dimensionen „kodieren".',
      explainReadTitle: 'Wie lese ich das Balkendiagramm?',
      explainReadText: 'Jeder Balken repräsentiert eine Embedding-Dimension. Die Länge zeigt, wie stark sich diese Dimension zwischen Prompt A und B unterscheidet. Dimensionen mit großem Unterschied sind die wahrscheinlichsten Träger der semantischen Änderung. Aber: Embeddings sind verteilt — oft braucht es mehrere Dimensionen zusammen, um eine sichtbare Änderung zu bewirken.',
      techTitle: 'Technische Details',
      techText: 'SD3.5 verwendet drei Text-Encoder: CLIP-L (768d), CLIP-G (1280d) und T5-XXL (4096d). Du kannst jeden einzeln proben. Die Differenz wird als mittlere absolute Abweichung über alle Token-Positionen berechnet: mean(abs(B-A), dim=tokens). Die Übertragung ersetzt die ausgewählten Dimensionen in allen Token-Positionen gleichzeitig.',
      referencesTitle: 'Forschungsgrundlagen',
      promptALabel: 'Prompt A (Original)',
      promptBLabel: 'Prompt B (Vergleich)',
      promptAPlaceholder: 'z.B. Ein rotes Haus am See',
      promptBPlaceholder: 'z.B. Ein blaues Haus am See',
      encoderLabel: 'Encoder',
      encoderAll: 'Alle (empfohlen)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      analyzeBtn: 'Analysieren',
      analyzing: 'Prompts werden kodiert und verglichen...',
      transferBtn: 'Übertrage die ausgewählten Vektor-Dimensionen von Prompt B in das erzeugte Bild',
      transferring: 'Bild mit modifiziertem Embedding wird erzeugt...',
      rankFromLabel: 'Von Rang',
      rankToLabel: 'Bis Rang',
      sliderLabel: 'Dimensionen von Prompt B auswählen',
      range1Label: 'Bereich 1',
      range2Label: 'Bereich 2',
      addRange: 'Bereich hinzufügen',
      selectionDesc: '{count} Dimensionen von Prompt B ausgewählt (Rang {ranges} von {total})',
      listTitle: 'Die {count} Dimensionen von Prompt B mit der größten Differenz zu Prompt A',
      sortAsc: 'Aufsteigend sortiert',
      sortDesc: 'Absteigend sortiert',
      originalLabel: 'Original (Prompt A)',
      modifiedLabel: 'Modifiziert (Transfer von Prompt B)',
      modifiedHint: 'Wähle unten einen Rangbereich und klicke „Übertragen" — hier erscheint dann Prompt A mit den übertragenen Dimensionen aus B (gleicher Seed).',
      noDifference: 'Die Embeddings sind identisch — ändere Prompt B.',
      advancedLabel: 'Erweiterte Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      selectAll: 'Alle',
      selectNone: 'Keine',
      encoderHint: 'All = alle Encoder gleichzeitig. CLIP-L/CLIP-G/T5 = isoliert einen einzelnen Encoder für die Analyse.',
      sliderHint: 'Wähle einen Rangbereich der wichtigsten Embedding-Dimensionen (sortiert nach Unterschied zwischen A und B).',
      transferHint: 'Überträgt die ausgewählten Dimensionen von Prompt B auf Prompt A und generiert ein neues Bild.',
      downloadOriginal: 'Original herunterladen',
      downloadModified: 'Modifiziert herunterladen'
    },
    algebra: {
      headerTitle: 'Concept Algebra \u2014 Vektor-Arithmetik auf Bild-Embeddings',
      headerSubtitle: 'Wende die ber\u00fchmte Word2Vec-Analogie auf Bildgenerierung an: K\u00f6nig \u2212 Mann + Frau \u2248 K\u00f6nigin. Drei Prompts werden kodiert und algebraisch kombiniert.',
      explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
      explainWhatTitle: 'Was zeigt dieses Tool?',
      explainWhatText: 'Mikolov zeigte 2013, dass Word-Embeddings semantische Beziehungen als lineare Richtungen kodieren: Der Vektor von \u201eK\u00f6nig\u201c minus \u201eMann\u201c plus \u201eFrau\u201c ergibt einen Vektor nahe \u201eK\u00f6nigin\u201c. Dieses Tool \u00fcbertr\u00e4gt diese Idee auf die Text-Encoder von SD3.5: Statt einzelner W\u00f6rter manipulierst du ganze Prompt-Embeddings. Das Ergebnis ist ein Bild, das das Konzept A enth\u00e4lt, aber B durch C ersetzt hat.',
      explainHowTitle: 'Wie funktioniert die Algebra \u2014 und warum nicht einfach ein Negativ-Prompt?',
      explainHowText: 'Du gibst drei Prompts ein: A (Basis), B (subtrahieren) und C (addieren). Die Formel ist: Ergebnis = A \u2212 Skalierung\u2081\u00d7B + Skalierung\u2082\u00d7C. Mit den Skalierungsreglern steuerst du die Intensit\u00e4t: Bei 1.0 wird B vollst\u00e4ndig subtrahiert und C vollst\u00e4ndig addiert. Bei 0.5 nur zur H\u00e4lfte. Werte \u00fcber 1.0 verst\u00e4rken den Effekt. \u2014 Warum nicht einfach \u201eA + C\u201c als Prompt und \u201eB\u201c als Negativ-Prompt verwenden? Weil das etwas fundamental anderes tut: Ein Negativ-Prompt steuert den Entrauschungsprozess bei JEDEM der 25 Schritte weg von B \u2014 das Modell entscheidet Schritt f\u00fcr Schritt, wie es \u201enicht B\u201c interpretiert. Concept Algebra dagegen berechnet einen neuen Vektor VOR der Bildgenerierung: Die Subtraktion passiert im Embedding-Raum, nicht im Diffusionsprozess. Das Ergebnis ist ein einziger Vektor, der \u201eA ohne B-heit plus C-heit\u201c direkt kodiert. Der Negativ-Prompt sagt \u201emach das nicht\u201c. Die Algebra sagt \u201enimm dieses Konzept heraus und setze jenes ein\u201c \u2014 eine chirurgische Operation im Bedeutungsraum statt einer schrittweisen Vermeidungsstrategie.',
      explainReadTitle: 'Was bedeuten die Ergebnisse?',
      explainReadText: 'Links siehst du das Referenzbild (nur Prompt A, gleicher Seed). Rechts das Ergebnis der Algebra. Wenn die Analogie funktioniert, sollte das rechte Bild das Konzept von A zeigen, aber mit der semantischen Ver\u00e4nderung B\u2192C. Beispiel: \u201eSonnenuntergang am Strand\u201c \u2212 \u201eStrand\u201c + \u201eBerge\u201c \u2248 \u201eSonnenuntergang \u00fcber Bergen\u201c. Die L2-Distanz zeigt, wie weit sich das Ergebnis vom Original entfernt hat. \u2014 Ist die Operation kommutativ? Nein. Die Subtraktion von B und die Addition von C finden relativ zum Vektor A statt. Die Richtung B\u2192C ist nur im Kontext von A sinnvoll: \u201eK\u00f6nig \u2212 Mann\u201c entfernt die \u201em\u00e4nnlichen\u201c Richtungen aus dem K\u00f6nig-Vektor, \u201e+ Frau\u201c erg\u00e4nzt die \u201eweiblichen\u201c Richtungen \u2014 das Ergebnis liegt nahe \u201eK\u00f6nigin\u201c. C wird dabei nicht gezielt an die Stelle von B gesetzt, sondern einfach addiert. Dass das trotzdem funktioniert, zeigt, dass semantische Beziehungen im Vektorraum als konsistente lineare Richtungen kodiert sind.',
      techTitle: 'Technische Details',
      techText: 'Die Algebra wird auf den gew\u00e4hlten Encoder-Embeddings durchgef\u00fchrt: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d) oder alle zusammen (589 Tokens \u00d7 4096d). Dieselbe Operation wird auch auf die Pooled Embeddings (2048d) angewendet. Beide Bilder verwenden denselben Seed f\u00fcr faire Vergleichbarkeit.',
      referencesTitle: 'Forschungsgrundlagen',
      promptALabel: 'Prompt A (Basis)',
      promptAPlaceholder: 'z.B. Sonnenuntergang am Strand mit Palmen',
      promptBLabel: 'Prompt B (Subtrahieren)',
      promptBPlaceholder: 'z.B. Strand mit Palmen',
      promptCLabel: 'Prompt C (Addieren)',
      promptCPlaceholder: 'z.B. Schneebedeckte Berge',
      formulaLabel: 'A \u2212 B + C = ?',
      encoderLabel: 'Encoder',
      encoderAll: 'Alle (empfohlen)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      generateBtn: 'Berechnen',
      generating: 'Embeddings werden berechnet und Bilder erzeugt...',
      referenceLabel: 'Referenz (Prompt A)',
      resultLabel: 'Ergebnis (A \u2212 B + C)',
      l2Label: 'L2-Distanz zum Original',
      advancedLabel: 'Erweiterte Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      scaleSubLabel: 'Subtraktions-Skalierung',
      scaleAddLabel: 'Additions-Skalierung',
      encoderHint: 'All = alle Encoder gleichzeitig. CLIP-L/CLIP-G/T5 = isoliert einen einzelnen Encoder für die Arithmetik.',
      scaleSubHint: 'Gewicht der Subtraktion (B). Höher = stärkere Entfernung von Konzept B.',
      scaleAddHint: 'Gewicht der Addition (C). Höher = stärkere Hinzufügung von Konzept C.',
      l2Hint: 'Euklidischer Abstand im Embedding-Raum. Kleiner = ähnlicher, größer = unterschiedlicher.',
      downloadReference: 'Referenz herunterladen',
      downloadResult: 'Ergebnis herunterladen',
      resultHint: 'Gib drei Prompts ein und klicke auf Berechnen \u2014 hier erscheint das Ergebnis der Vektor-Arithmetik.'
    },
    archaeology: {
      headerTitle: 'Denoising Archaeology \u2014 Wie wird aus Rauschen ein Bild?',
      headerSubtitle: 'Beobachte jeden einzelnen Entrauschungsschritt. Diffusionsmodelle arbeiten nicht links-nach-rechts, sondern gleichzeitig \u00fcberall \u2014 von groben Formen zu feinen Details.',
      explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
      explainWhatTitle: 'Was zeigt dieses Tool?',
      explainWhatText: 'Ein Diffusionsmodell erzeugt ein Bild, indem es schrittweise Rauschen entfernt. Dabei entsteht das Bild nicht wie beim Zeichnen von links nach rechts \u2014 stattdessen arbeitet das Modell an ALLEN Bildregionen gleichzeitig. In den ersten Schritten entstehen grobe Strukturen: Wo ist oben, wo unten? Wo ist der Horizont? In den mittleren Schritten kommen semantische Inhalte: Objekte, Formen, Farben. Die letzten Schritte verfeinern Texturen und Details. Dieses Tool macht jeden einzelnen Schritt sichtbar.',
      explainHowTitle: 'Wie benutze ich das Tool?',
      explainHowText: 'Gib einen Prompt ein und klicke auf Generieren. Das Modell erzeugt 25 Zwischenbilder (eins pro Entrauschungsschritt). Diese erscheinen als Filmstreifen unten. Klicke auf ein Thumbnail oder benutze den Zeitregler, um jeden Schritt in voller Gr\u00f6\u00dfe zu betrachten. Vergleiche fr\u00fche und sp\u00e4te Schritte: Wann \u201ewei\u00df\u201c das Modell, was es zeichnet?',
      explainReadTitle: 'Was verraten die drei Phasen?',
      explainReadText: 'Fr\u00fche Schritte (1\u20138): Globale Komposition \u2014 Grundstruktur, Farbverteilung, Layoutplanung. Mittlere Schritte (9\u201317): Semantische Emergenz \u2014 Objekte werden erkennbar, Formen kristallisieren sich heraus. Sp\u00e4te Schritte (18\u201325): Detail-Verfeinerung \u2014 Texturen, Kanten, feine Muster. Die \u00dcberg\u00e4nge sind flie\u00dfend, aber die Phasen zeigen deutlich: Das Modell \u201eplant\u201c zuerst global und verfeinert dann lokal. Besonders aufschlussreich: Der allererste Schritt zeigt keine feink\u00f6rnigen Pixel, sondern farbige Flecken. Das liegt daran, dass das Rauschen im Latent-Raum (128\u00d7128 bei 16 Kan\u00e4len) erzeugt wird, nicht im Pixel-Raum. Der VAE \u00fcbersetzt jeden Latent-Pixel in einen ~8\u00d78-Pixel-Patch \u2014 selbst pures Gau\u00dfsches Rauschen wird dadurch zu zusammenh\u00e4ngenden Farbclustern. Das Modell \u201edenkt\u201c nie in einzelnen Pixeln, sondern immer in diesem komprimierten Raum.',
      techTitle: 'Technische Details',
      techText: 'SD3.5 Large verwendet Rectified Flow als Scheduler mit 25 Standardschritten. Bei jedem Schritt werden die aktuellen Latent-Vektoren durch den VAE dekodiert (1024\u00d71024 JPEG). Der VAE (Variational Autoencoder) \u00fcbersetzt den mathematischen Latent-Raum in Pixel. Die Latent-Darstellung ist 128\u00d7128 bei 16 Kan\u00e4len \u2014 jeder Latent-Pixel entspricht einem ~8\u00d78-Pixel-Patch im Bild. Deshalb zeigt schon der erste Schritt farbige Cluster statt feines Pixelrauschen: Der VAE interpretiert zuf\u00e4llige 16-dimensionale Vektoren als koh\u00e4rente Farbfl\u00e4chen.',
      referencesTitle: 'Forschungsgrundlagen',
      promptLabel: 'Prompt',
      promptPlaceholder: 'z.B. Ein Marktplatz in einer mittelalterlichen Stadt mit Menschen, Geb\u00e4uden und einem Brunnen',
      generate: 'Generieren',
      generating: 'Bild wird generiert \u2014 jeder Schritt wird aufgezeichnet...',
      emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um den Entrauschungsprozess zu visualisieren.',
      advancedLabel: 'Erweiterte Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      filmstripLabel: 'Entrauschungs-Filmstreifen',
      timelineLabel: 'Schritt',
      phaseEarly: 'Komposition',
      phaseMid: 'Semantik',
      phaseLate: 'Detail',
      phaseEarlyDesc: 'Globale Struktur und Farbverteilung entstehen',
      phaseMidDesc: 'Objekte und Formen werden erkennbar',
      phaseLateDesc: 'Texturen und feine Details werden gesch\u00e4rft',
      finalImageLabel: 'Finales Bild (volle Aufl\u00f6sung)',
      timelineHint: 'Scrubbt durch die Entrauschungsschritte — zeigt wie das Bild vom Rauschen zur fertigen Komposition entsteht.',
      download: 'Bild herunterladen'
    },
    textLab: {
      headerTitle: 'Latent Text Lab \u2014 Wissenschaftliche LLM-Dekonstruktion',
      headerSubtitle: 'Representation Engineering, vergleichende Modell-Arch\u00e4ologie und systematische Bias-Analyse: Drei forschungsbasierte Werkzeuge zur Untersuchung von Sprachmodellen.',
      explanationToggle: 'Erkl\u00e4rung anzeigen',
      modelPanel: {
        presetLabel: 'Preset',
        presetNone: 'Kein Preset (eigene ID)',
        customModelLabel: 'HuggingFace Modell-ID',
        customModelPlaceholder: 'z.B. meta-llama/Llama-3.2-1B',
        quantizationLabel: 'Quantisierung',
        quantAuto: 'Auto',
        quantizationHint: 'bf16 = volle Qualität, int8 = halb so viel VRAM, int4 = minimal VRAM aber niedrigste Qualität',
      },
      temperatureHint: 'Zufälligkeit der Textgenerierung. Niedrig = deterministisch, hoch = kreativer.',
      maxTokensHint: 'Maximale Anzahl generierter Tokens (Wortteile).',
      textSeedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
      tabs: {
        repeng: { label: 'Representation Engineering', short: 'Steuervektoren im LLM finden' },
        compare: { label: 'Modell-Vergleich', short: 'Zwei LLMs Schicht f\u00fcr Schicht vergleichen' },
        bias: { label: 'Bias-Arch\u00e4ologie', short: 'Verborgene Vorurteile im LLM aufdecken' },
      },
      repeng: {
        title: 'Representation Engineering',
        subtitle: 'Finde Konzept-Richtungen im Aktivierungsraum und steuere die Generierung',
        explainWhatTitle: 'Was zeigt dieses Experiment?',
        explainWhatText: 'Basiert auf Zou et al. (2023) \u201cRepresentation Engineering\u201d und Li et al. (2024) \u201cInference-Time Intervention\u201d. LLMs kodieren abstrakte Konzepte (Wahrheit, Stimmung, Ethik) als Richtungen im hochdimensionalen Aktivierungsraum. Durch Kontrastpaare (wahrer vs. falscher Satz) l\u00e4sst sich die Richtung extrahieren, die ein Konzept kodiert. Addiert man diese Richtung zur Laufzeit, \u00e4ndert sich das Modellverhalten gezielt \u2014 ohne Retraining. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
        explainHowTitle: 'Wie benutze ich es?',
        explainHowText: 'Dieses Experiment extrahiert eine \u201cWahrheits-Richtung\u201d aus dem Modell. Die vorausgef\u00fcllten Kontrastpaare enthalten jeweils einen wahren und einen falschen Satz. Aus den Unterschieden in den Aktivierungen berechnet das System eine Richtung im hochdimensionalen Raum. Wenn diese Richtung invertiert wird (\u03b1 = -1), sollte das Modell bei einem Fakten-Prompt eine falsche Antwort generieren \u2014 obwohl es die richtige \u201cwei\u00df\u201d. Empfehlung: Englische Prompts funktionieren deutlich besser, da die meisten Open-Source-LLMs prim\u00e4r auf englischen Daten trainiert wurden.',
        referencesTitle: 'Forschungsgrundlagen',
        expectedResults: 'Erwartete Ergebnisse: Bei \u03b1 = 0 (Baseline) generiert das Modell die korrekte Antwort. Bei \u03b1 = -1 (Invertierung) sollte eine falsche Antwort erscheinen \u2014 das ist der Kern des Experiments. Bei \u03b1 = +1 \u00e4ndert sich wenig, weil das Modell bereits korrekt antwortet. Ab |\u03b1| > 2 dominieren Artefakte (Wiederholungen, Unsinn). Der \"Sweet Spot\" liegt laut Zou et al. bei |\u03b1| zwischen 0.5 und 2.0. Erkl\u00e4rte Varianz > 50% bedeutet eine saubere Trennung \u2014 darunter sind die Kontrastpaare zu \u00e4hnlich oder zu wenige (mindestens 3 empfohlen).',
        pairsTitle: 'Kontrastpaare',
        pairsSubtitle: 'Mindestens 3 Paare empfohlen. Jedes Paar muss sich nur im Zielkonzept unterscheiden (wahr vs. falsch). Die Beispiele sind editierbar.',
        positiveLabel: 'Positiv (wahr)',
        negativeLabel: 'Negativ (falsch)',
        positivePlaceholder: 'z.B. The capital of France is Paris',
        negativePlaceholder: 'z.B. The capital of France is Berlin',
        addPair: 'Paar hinzuf\u00fcgen',
        removePair: 'Entfernen',
        targetLayerLabel: 'Ziel-Schicht',
        targetLayerHint: 'Welche Transformer-Schicht den Steuerungsvektor erhält. Verschiedene Schichten beeinflussen verschiedene Aspekte der Textgenerierung.',
        targetLayerAuto: 'Letzte Schicht',
        findDirection: 'Richtung finden',
        finding: 'Konzept-Richtung wird berechnet...',
        directionFound: 'Konzept-Richtung gefunden',
        varianceLabel: 'Erkl\u00e4rte Varianz',
        dimLabel: 'Dimensionen',
        projectionsTitle: 'Projektionen der Kontrastpaare',
        testTitle: 'Test + Manipulation',
        testSubtitle: 'Gib einen Satz ein und steuere die Generierung entlang der Konzept-Richtung',
        testPromptLabel: 'Test-Prompt',
        testPromptPlaceholder: 'z.B. The capital of Germany is',
        alphaLabel: 'Manipulationsst\u00e4rke (\u03b1)',
        alphaHint: 'Stärke des Steuerungsvektors. 0 = kein Effekt, höher = stärkerer Einfluss der Kontrastpaare.',
        temperatureLabel: 'Temperatur',
        maxTokensLabel: 'Max. Tokens',
        seedLabel: 'Seed (-1 = zuf\u00e4llig)',
        generateBtn: 'Generieren mit Manipulation',
        generating: 'Manipulierte Generierung l\u00e4uft...',
        baselineLabel: 'Baseline (ohne Manipulation)',
        manipulatedLabel: 'Manipuliert (\u03b1 = {alpha})',
        projectionLabel: 'Projektion auf Konzept-Richtung',
        interpretationTitle: 'Erl\u00e4uterung',
        interpreting: 'Ergebnisse werden analysiert...',
        interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
      },
      compare: {
        title: 'Vergleichende Modell-Arch\u00e4ologie',
        subtitle: 'Lade zwei Modelle und vergleiche ihre inneren Repr\u00e4sentationen systematisch',
        explainWhatTitle: 'Was zeigt dieses Experiment?',
        explainWhatText: 'Basiert auf Belinkov (2022) \u201cProbing Classifiers\u201d und Olsson et al. (2022) \u201cIn-Context Learning and Induction Heads\u201d. Die Heatmap zeigt CKA (Centered Kernel Alignment) zwischen Schichten beider Modelle. Hohe \u00c4hnlichkeit bedeutet: Diese Schichten repr\u00e4sentieren Information auf \u00e4hnliche Weise. Fr\u00fche Schichten (Syntax) sind oft \u00e4hnlich \u2014 sp\u00e4te Schichten (Semantik) divergieren st\u00e4rker. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
        explainHowTitle: 'Wie benutze ich es?',
        explainHowText: 'Modell A ist das aktive Preset. W\u00e4hle ein zweites Modell (Modell B) \u2014 per Preset oder eigener HuggingFace-ID \u2014 und lade es. Gib einen Text ein und klicke \u201cVergleichen\u201d. Die CKA-Heatmap zeigt, welche Schichten \u00e4hnlich repr\u00e4sentieren. Die Generierungen beider Modelle mit identischem Seed zeigen, wie unterschiedlich sie denselben Prompt vervollst\u00e4ndigen.',
        referencesTitle: 'Forschungsgrundlagen',
        modelATitle: 'Modell A (aus Preset-Auswahl)',
        modelAHint: 'Wechsel \u00fcber das Preset-Dropdown oben',
        modelBTitle: 'Modell B (zweites Modell)',
        modelBPresetLabel: 'Preset',
        modelBCustomLabel: 'HuggingFace Modell-ID',
        modelBCustomPlaceholder: 'z.B. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        modelBLoadBtn: 'Modell B laden',
        modelBLoaded: 'Modell B geladen',
        modelBNone: 'Modell B nicht geladen',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. Die Katze sa\u00df auf der Matte und beobachtete die V\u00f6gel',
        seedLabel: 'Seed',
        temperatureLabel: 'Temperatur',
        maxTokensLabel: 'Max. Tokens',
        compareBtn: 'Vergleichen',
        comparing: 'Modelle werden verglichen...',
        heatmapTitle: 'Schicht-Alignment (CKA)',
        heatmapAxisA: 'Modell A \u2014 Schichten',
        heatmapAxisB: 'Modell B \u2014 Schichten',
        heatmapExplain: 'Helle Zellen = hohe Repr\u00e4sentations\u00e4hnlichkeit. Diagonale Muster zeigen, dass die Modelle Information in \u00e4hnlicher Reihenfolge verarbeiten.',
        attentionTitle: 'Attention-Vergleich (letzte Schicht)',
        modelALabel: 'Modell A',
        modelBLabel: 'Modell B',
        generationTitle: 'Generierungs-Vergleich (gleicher Seed)',
        layerStatsTitle: 'Schicht-Statistiken',
        interpretationTitle: 'Erl\u00e4uterung',
        interpreting: 'Ergebnisse werden analysiert...',
        interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
      },
      bias: {
        title: 'Bias-Arch\u00e4ologie',
        subtitle: 'Systematische Bias-Experimente durch kontrollierte Token-Manipulation',
        explainWhatTitle: 'Was zeigt dieses Experiment?',
        explainWhatText: 'Basiert auf Zou et al. (2023) \u201cRepresentation Engineering\u201d und Bricken et al. (2023) \u201cTowards Monosemanticity\u201d. Statt freiem Manipulieren untersucht dieses Tool systematische Verzerrungen: Was passiert, wenn alle m\u00e4nnlichen Pronomen unterdr\u00fcckt werden? Welches Geschlecht w\u00e4hlt das Modell als Default? \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
        explainHowTitle: 'Wie benutze ich es?',
        explainHowText: 'W\u00e4hle einen Experiment-Typ (Geschlecht, Sentiment, Dom\u00e4ne oder eigene Tokens). Gib einen Prompt ein, der das Modell zur Fortsetzung einl\u00e4dt (z.B. \u201cThe doctor said to the patient\u201d). Die Ergebnisse zeigen Baseline vs. manipulierte Generierungen \u2014 die Unterschiede offenbaren die im Modell kodierten Biases.',
        referencesTitle: 'Forschungsgrundlagen',
        presetLabel: 'Experiment-Typ',
        presetGender: 'Geschlecht \u2014 Unterdr\u00fccke gendered Pronomen',
        presetSentiment: 'Sentiment \u2014 Verst\u00e4rke positiv/negativ',
        presetDomain: 'Dom\u00e4ne \u2014 Verst\u00e4rke wissenschaftlich/poetisch',
        presetCustom: 'Eigenes Experiment',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. The doctor said to the patient',
        customBoostLabel: 'Verst\u00e4rkte Tokens (kommagetrennt)',
        customBoostPlaceholder: 'z.B. dark,shadow,night',
        customSuppressLabel: 'Unterdr\u00fcckte Tokens (kommagetrennt)',
        customSuppressPlaceholder: 'z.B. light,sun,bright',
        numSamplesLabel: 'Stichproben pro Bedingung',
        temperatureLabel: 'Temperatur',
        maxTokensLabel: 'Max. Tokens',
        seedLabel: 'Basis-Seed',
        runBtn: 'Experiment starten',
        running: 'Bias-Experiment l\u00e4uft...',
        baselineTitle: 'Baseline (keine Manipulation)',
        groupTitle: 'Gruppe: {name}',
        modeSuppress: 'unterdr\u00fcckt',
        modeBoost: 'verst\u00e4rkt',
        tokensLabel: 'Tokens',
        sampleSeedLabel: 'Seed',
        genderDesc: 'Unterdr\u00fcckt alle geschlechtsspezifischen Pronomen und beobachtet, welche Defaults das Modell w\u00e4hlt.',
        sentimentDesc: 'Verst\u00e4rkt positive oder negative W\u00f6rter und misst, wie stark der gesamte Textfluss beeinflusst wird.',
        domainDesc: 'Verst\u00e4rkt wissenschaftliches oder poetisches Vokabular und beobachtet Register-Verschiebungen.',
        interpretationTitle: 'Erl\u00e4uterung',
        interpreting: 'Ergebnisse werden analysiert...',
        interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
      },
      error: {
        gpuUnreachable: 'GPU-Service nicht erreichbar. Ist er gestartet?',
        loadFailed: 'Modell konnte nicht geladen werden.',
        operationFailed: 'Operation fehlgeschlagen.'
      }
    },
    crossmodal: {
      headerTitle: 'Crossmodal Lab',
      headerSubtitle: 'Klang aus latenten Raeumen: T5-Embedding-Manipulation, bildgesteuerte Audiogenerierung, crossmodaler Transfer',
      explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
      generate: 'Generieren',
      generating: 'Generiere...',
      result: 'Ergebnis',
      seed: 'Seed',
      generationTime: 'Generierungszeit',
      tabs: {
        synth: {
          label: 'Latent Audio Synth',
          short: 'T5-Embedding-Manipulation',
          title: 'Latent Audio Synth',
          description: 'Direkte Manipulation des T5-Conditioning-Raums (768d) fuer Stable Audio. Interpoliere zwischen Prompts, extrapoliere ueber den Prompt hinaus, skaliere Embeddings und injiziere Rauschen. Ultra-kurze Loops, nahezu Echtzeit.'
        },
        mmaudio: {
          label: 'MMAudio',
          short: 'Bild/Text zu Audio (CVPR 2025)',
          title: 'MMAudio — Video/Image to Audio',
          description: 'Bild und Text fliessen als getrennte Signale in dasselbe Netzwerk ein — das Bild wird nicht in Sprache uebersetzt, sondern beide steuern die Klangerzeugung gleichzeitig. Das Modell wurde gemeinsam auf Video und Audio trainiert und lernt dadurch direkte Zusammenhaenge zwischen Sichtbarem und Hoerbarem. Bis 8s, 44.1kHz, ~1.2s Rechenzeit. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
        },
        guidance: {
          label: 'ImageBind Guidance',
          short: 'Gradientenbasierte Bildsteuerung',
          title: 'ImageBind Gradient Guidance',
          description: 'Gradient-basierte Steuerung waehrend des Stable Audio Denoising-Prozesses. ImageBind liefert einen gemeinsamen 1024d-Raum fuer Bild und Audio — der Gradient der Cosine-Similarity lenkt die Audiogenerierung in Richtung des Bild-Embeddings.'
        }
      },
      synth: {
        explainWhatTitle: 'Was macht der Latent Audio Synth?',
        explainWhatText: 'Stable Audio erzeugt Klang aus Text. Der Text wird dabei von einem T5-Encoder in einen Zahlenvektor mit 768 Dimensionen umgewandelt \u2014 diesen Vektor kannst du hier direkt manipulieren. Statt nur \u201ewas\u201c das Modell erzeugt zu \u00e4ndern (durch den Prompt), \u00e4nderst du \u201ewie\u201c das Modell den Text intern versteht. Zwei Prompts, die \u00e4hnlich klingen, k\u00f6nnen in diesem Raum weit auseinander liegen \u2014 und umgekehrt.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Gib in Prompt A einen Text ein \u2014 das bestimmt den Grundklang. Optional: Prompt B als Zielpunkt. Der Alpha-Regler steuert die Mischung: Bei 0 h\u00f6rst du nur A, bei 1 nur B, bei 0.5 eine Mischung. Werte \u00fcber 1 extrapolieren \u00fcber B hinaus (der Klang wird extremer), Werte unter 0 gehen in die entgegengesetzte Richtung. Magnitude skaliert das gesamte Embedding \u2014 h\u00f6here Werte erzeugen intensivere Kl\u00e4nge. Noise injiziert Zufall und erzeugt unvorhersehbare Variationen. Die Spectral Strip (unter dem Generate-Button) zeigt alle 768 Dimensionen als Balken. Du kannst einzelne Dimensionen per Mausklick verschieben und so gezielt den Klang manipulieren. Rechtsklick setzt eine Dimension zur\u00fcck.',
        promptA: 'Prompt A (Basis)',
        promptAPlaceholder: 'z.B. Meereswellen',
        promptB: 'Prompt B (Optional, fuer Interpolation)',
        promptBPlaceholder: 'z.B. Klaviermelodie',
        alpha: 'Alpha (Interpolation)',
        alphaHint: '0 = nur A, 1 = nur B, dazwischen = Mischung, >1 oder <0 = Extrapolation',
        magnitude: 'Magnitude (Skalierung)',
        magnitudeHint: 'Globale Skalierung des Embeddings (1.0 = unveraendert)',
        noise: 'Rauschen',
        noiseHint: 'Gauss-Rauschen auf dem Embedding (0 = kein Rauschen)',
        duration: 'Dauer (s)',
        steps: 'Schritte',
        cfg: 'CFG',
        durationHint: 'Länge des generierten Audio-Clips in Sekunden',
        stepsHint: 'Entrauschungsschritte. Mehr = höhere Qualität.',
        cfgHint: 'Classifier-Free Guidance für die Audiogenerierung',
        seedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
        loop: 'Loop-Wiedergabe',
        loopOn: 'Loop An',
        loopOff: 'Loop Aus',
        stop: 'Stop',
        looping: 'Loopt',
        playing: 'Spielt',
        stopped: 'Gestoppt',
        transpose: 'Transposition (Halbtoene)',
        midiSection: 'MIDI-Steuerung',
        midiUnsupported: 'Web MIDI wird von diesem Browser nicht unterstuetzt.',
        midiInput: 'MIDI-Eingang',
        midiNone: '(keiner)',
        midiMappings: 'CC-Zuordnungen',
        midiNoteC3: 'Note (C3 = Ref)',
        midiGenerate: 'Generieren + Transposition',
        midiPitch: 'Tonhoehe rel. C3',
        loopInterval: 'Loop-Intervall',
        loopOptimize: 'Auto-Optimierung',
        loopPingPong: 'Ping-Pong',
        loopIntervalHint: 'Start/Ende des Loop-Bereichs — verkuerze das Ende, um Stable Audios Fade-Out abzuschneiden',
        modeLoop: 'Loop',
        modePingPong: 'Ping-Pong',
        modeWavetable: 'Wavetable',
        modeRate: 'Tempo (schnell)',
        modePitch: 'Tonhoehe (OLA)',
        wavetableScan: 'Scan-Position',
        wavetableScanHint: 'Morpht zwischen Frames (0 = Anfang, 1 = Ende)',
        wavetableFrames: '{count} Frames',
        midiScan: 'Scan-Position',
        adsrTitle: 'ADSR-Huellkurve',
        adsrAttack: 'A',
        adsrDecay: 'D',
        adsrSustain: 'S',
        adsrRelease: 'R',
        adsrHint: 'Huellkurve fuer MIDI-Noten (Attack/Decay/Sustain/Release)',
        play: 'Abspielen',
        normalize: 'Lautheit normalisieren',
        peak: 'Peak',
        crossfade: 'Crossfade',
        transposeHint: 'Verschiebt die Tonhöhe in Halbtönen',
        crossfadeHint: 'Überblendzeit an der Loop-Grenze (ms)',
        normalizeHint: 'Normalisiert die Lautstärke auf maximale Amplitude',
        saveRaw: 'Raw speichern',
        saveLoop: 'Loop speichern',
        embeddingStats: 'Embedding-Statistiken',
        dimensions: {
          section: 'Dimensions-Explorer',
          hint: 'Auf Balken ziehen = Offset setzen. Horizontal malen = mehrere Dimensionen.',
          resetAll: 'Alle zuruecksetzen',
          hoverActivation: 'Aktivierung',
          hoverOffset: 'Offset',
          rightClickReset: 'Rechtsklick = zuruecksetzen',
          sortDiff: 'Sortiert nach Prompt-Differenz',
          sortMagnitude: 'Sortiert nach Aktivierung',
          activeOffsets: '{count} Offsets aktiv',
          applyAndGenerate: 'Anwenden und neu generieren',
          undo: 'Rueckgaengig',
          redo: 'Wiederholen'
        }
      },
      mmaudio: {
        explainWhatTitle: 'Was macht MMAudio?',
        explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) wurde gemeinsam auf Video und Audio trainiert. Es \u00fcbersetzt ein Bild nicht erst in Text und dann in Klang, sondern verarbeitet Bild und Text als parallele Signale im selben Netzwerk. Das Modell hat gelernt, welche Kl\u00e4nge zu welchen visuellen Szenen geh\u00f6ren \u2014 ein Wald erzeugt Vogelgezwitscher, eine Stra\u00dfe Verkehrsl\u00e4rm, eine Gitarre Zupfkl\u00e4nge.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Lade ein Bild hoch und/oder gib einen Text-Prompt ein \u2014 beides zusammen ergibt die reichsten Ergebnisse. Das Bild allein erzeugt Kl\u00e4nge passend zum visuellen Inhalt. Der Text-Prompt kann den Klang zus\u00e4tzlich lenken oder ohne Bild allein verwendet werden. Im Negativ-Prompt beschreibst du Kl\u00e4nge, die du NICHT h\u00f6ren willst (z.B. \u201eSprache, Musik\u201c). Duration bestimmt die L\u00e4nge (1\u20138 Sekunden). CFG Strength steuert, wie streng das Modell dem Prompt folgt \u2014 niedrige Werte (2\u20133) erzeugen vielf\u00e4ltigere, h\u00f6here Werte (6\u20138) prompt-treuere Ergebnisse.',
        imageUpload: 'Bild hochladen (optional)',
        prompt: 'Text-Prompt (optional)',
        promptPlaceholder: 'z.B. Knisterndes Lagerfeuer',
        negativePrompt: 'Negativ-Prompt',
        duration: 'Dauer (s)',
        maxDuration: 'Max 8s (Modell-Limit)',
        cfg: 'CFG',
        steps: 'Schritte',
        compareHint: 'Vergleiche: Nur Text vs. Bild + Text'
      },
      guidance: {
        explainWhatTitle: 'Was macht ImageBind Guidance?',
        explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) bringt sechs Sinne \u2014 Bild, Klang, Text, Tiefe, W\u00e4rme, Bewegung \u2014 in eine gemeinsame \u201eSprache\u201c. Dieses Tool nutzt diese Gemeinsamkeit: W\u00e4hrend der Klang Schritt f\u00fcr Schritt entsteht, vergleicht es st\u00e4ndig \u201eKlingt das schon nach dem Bild?\u201c und korrigiert die Richtung. Im Ergebnis zeigt die Cosine-Similarity, wie nah der erzeugte Klang dem Bild-Inhalt kam.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Lade ein Bild hoch \u2014 das ist die Zielrichtung f\u00fcr den Klang. Optional: Ein Text-Prompt als zus\u00e4tzliche Lenkung. Der Regler \u201e\u03bb Guidance Strength\u201c ist der wichtigste Parameter: Niedrige Werte (0.01\u20130.05) lassen dem Klang viel Freiheit, hohe Werte (0.3\u20131.0) binden ihn eng an das Bild. \u201eWarmup Steps\u201c bestimmt, ab welchem Schritt die Bildlenkung einsetzt \u2014 niedrige Werte starten sofort, h\u00f6here lassen die Grundstruktur erst ungesteuert entstehen. Total Steps und Duration steuern Qualit\u00e4t und L\u00e4nge.',
        referencesTitle: 'Forschungsgrundlagen',
        imageUpload: 'Bild hochladen',
        prompt: 'Basis-Prompt (optional)',
        promptPlaceholder: 'z.B. Umgebungsklanglandschaft',
        lambda: 'Guidance-Staerke',
        lambdaHint: 'Wie stark das Bild die Audiogenerierung steuert',
        warmupSteps: 'Warmup-Schritte',
        warmupHint: 'Gradient-Guidance nur in den ersten N Schritten',
        totalSteps: 'Gesamt-Schritte',
        duration: 'Dauer (s)',
        cfg: 'CFG',
        totalStepsHint: 'Gesamte Entrauschungsschritte. Mehr = höhere Qualität.',
        durationHint: 'Länge des generierten Audio-Clips in Sekunden',
        cosineSimilarity: 'Cosine-Similarity (Bild-Audio-Naehe)'
      }
    }
  },
  edutainment: {
    ui: {
      didYouKnow: '🤔 Wusstest du?',
      learnMore: '📚 Mehr erfahren',
      currentlyHappening: '⚡ Gerade passiert:',
      energyUsed: 'Verbrauchte Energie',
      co2Produced: 'CO₂ produziert'
    },
    energy: {
      kids_1: '💡 KI-Bilder brauchen Strom – so viel wie dein Handy 3 Stunden laden!',
      kids_2: '🔌 Die GPU ist wie ein Super-Taschenrechner der sehr viel Strom frisst!',
      kids_3: '⚡ Jedes Bild braucht so viel Energie wie eine LED-Lampe 10 Minuten an!',
      youth_1: '⚡ Eine GPU braucht beim Generieren {watts}W – wie ein kleiner Heizlüfter!',
      youth_2: '🔋 Ein Bild verbraucht etwa 0.01-0.02 kWh – klingt wenig, summiert sich aber!',
      youth_3: '🌡️ Die GPU wird gerade {temp}°C heiß – deshalb braucht sie Kühlung!',
      expert_1: '📊 Echtzeit: {watts}W bei {util}% Auslastung = {kwh} kWh bisher',
      expert_2: '🔥 TDP-Limit: {tdp}W | Aktuell: {watts}W ({percent}% des Limits)',
      expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – Modell + Aktivierungen'
    },
    data: {
      kids_1: '🧮 Die GPU rechnet gerade 10 Milliarden mal – schneller als du zählen kannst!',
      kids_2: '🎨 Das Bild entsteht in 50 kleinen Schritten – wie ein Puzzle das sich selbst löst!',
      kids_3: '🧩 Millionen von Zahlen fliegen gerade durch die GPU!',
      youth_1: '🔄 Jedes Bild durchläuft ~50 "Denoising Steps" – 50 Runden Rauschen entfernen!',
      youth_2: '📐 8 Milliarden Parameter werden gerade abgefragt – pro Bild!',
      youth_3: '🧠 Die KI "denkt" in Vektoren mit tausenden Dimensionen – wie Koordinaten in einem Raum.',
      expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – Text + Bild in gemeinsamen Attention-Layern',
      expert_2: '📈 Self-Attention: O(n²) Komplexität – jedes Token "sieht" alle anderen',
      expert_3: '⚙️ Classifier-Free Guidance: Prompt-Einfluss vs. Kreativität-Balance'
    },
    model: {
      kids_1: '🎓 Das KI-Modell hat sich Millionen Bilder angeschaut um malen zu lernen!',
      kids_2: '🤖 Die KI ist wie ein Künstler der nie vergisst was er gesehen hat!',
      kids_3: '✨ 8 Milliarden Verbindungen im Modell – mehr als Sterne am Himmel die du sehen kannst!',
      youth_1: '🧠 SD3.5 Large hat 8 Milliarden Parameter – wie 8 Milliarden Entscheidungsknoten.',
      youth_2: '📚 3 Text-Encoder arbeiten zusammen: CLIP-L, CLIP-G und T5-XXL',
      youth_3: '🔢 Das Modell braucht {vram} GB VRAM nur um geladen zu werden!',
      expert_1: '🏗️ Architektur: Rectified Flow + MMDiT mit 38 Transformer-Blöcken',
      expert_2: '📊 FP16/FP8 Quantisierung: Präzision vs. VRAM-Trade-off',
      expert_3: '🔗 LoRA: Low-Rank Adaptation – nur 0.1% der Parameter neu trainiert'
    },
    ethics: {
      kids_1: '🌍 KI lernt von Bildern im Internet – deshalb ist es wichtig, fair mit Kunst anderer zu sein!',
      kids_2: '⚖️ Nicht alle Künstler wurden gefragt ob die KI von ihnen lernen darf.',
      kids_3: '🤝 Gute KI respektiert die Arbeit von Menschen!',
      youth_1: '📜 Trainingsdaten stammen oft aus dem Internet. Künstler diskutieren: Fair Use oder Kopieren?',
      youth_2: '🏛️ Der EU AI Act fordert Transparenz: Woher kommen die Trainingsdaten?',
      youth_3: '💭 Frage: Wem gehört ein KI-generiertes Bild eigentlich?',
      expert_1: '⚠️ LAION-5B wurde teils ohne Urheber-Zustimmung erstellt – rechtliche Grauzone.',
      expert_2: '📋 EU AI Act Art. 52: Kennzeichnungspflicht für KI-generierte Inhalte',
      expert_3: '🔍 Model Cards & Datasheets: Best Practice für ML-Transparenz'
    },
    environment: {
      kids_1: '☁️ Jedes KI-Bild produziert ein bisschen CO₂ – wie Autofahren, nur weniger!',
      kids_2: '🌱 Überlege: Ist dieses Bild den Strom wert?',
      kids_3: '🌞 Die Energie für KI kommt oft aus Kraftwerken – manche sauber, manche nicht.',
      youth_1: '🏭 Deutscher Strommix: ~400g CO₂ pro kWh – das addiert sich!',
      youth_2: '📈 {co2}g CO₂ für dieses Bild – bei 1000 Bildern wären das {totalKg} kg!',
      youth_3: '💡 Tipp: Weniger Bilder generieren, dafür bewusster – spart Energie und CO₂.',
      expert_1: '📊 Berechnung: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
      expert_2: '🔬 Scope 2 Emissionen: Standort des Rechenzentrums entscheidend',
      expert_3: '⚡ PUE (Power Usage Effectiveness): Zusätzlicher Energie-Overhead für Kühlung'
    },
    iceberg: {
      drawPrompt: 'KI-Generierung verbraucht viel Energie. Zeichne Eisberge und schau was geschieht...',
      redraw: 'Neu zeichnen',
      startMelting: 'Schmelzen starten',
      melting: 'Eisberg schmilzt...',
      melted: 'Geschmolzen!',
      meltedMessage: '{co2}g CO₂ produziert',
      comparison: 'Diese CO₂-Menge lässt etwa {volume} cm³ Arktis-Eis schmelzen.',
      comparisonInfo: '(Jede Tonne CO₂ = ca. 6m³ Meereis-Verlust)',
      gpuPower: 'Stromverbrauch der Grafikkarte',
      gpuTemp: 'Temperatur der Grafikkarte',
      co2Info: 'CO₂-Emissionen durch Stromverbrauch (basierend auf deutschem Strommix)',
      drawAgain: 'Zeichne weitere Eisberge...'
    },
    pixel: {
      grafikkarte: 'Grafikkarte',
      energieverbrauch: 'Energieverbrauch',
      co2Menge: 'CO₂-Menge',
      smartphoneComparison: 'Du müsstest Dein Handy {minutes} Minuten ausgeschaltet lassen, um den CO₂-Verbrauch wieder auszugleichen!',
      clickToProcess: 'Klicke auf die Daten-Pixel um ein Minibild zu generieren!'
    },
    forest: {
      trees: 'Bäume',
      clickToPlant: 'Klicke um Bäume zu pflanzen! Wo Du einen Baum pflanzt, wird die Fabrik verschwinden.',
      gameOver: 'Der Wald ist verloren!',
      treesPlanted: 'Du hast {count} Bäume gepflanzt.',
      complete: 'Generation abgeschlossen',
      comparison: 'Ein durchschnittlicher Baum braucht {minutes} Minuten, um diese CO₂-Menge zu absorbieren.'
    },
    rareearth: {
      clickToClean: 'Klicke auf den See um Giftschlamm zu entfernen!',
      sludgeRemoved: 'Schlamm entfernt',
      environmentHealth: 'Umwelt',
      gameOverInactive: 'Du hast aufgegeben... der Abbau geht weiter',
      infoBanner: 'Seltene Erden für GPU-Chips: Der Abbau hinterlässt Giftschlamm und zerstört Ökosysteme. Deine Aufräum-Arbeit kann die Geschwindigkeit des Abbaus nicht aufhalten.',
      instructionsCooldown: '⏳ {seconds}s',
      statsGpu: 'GPU',
      statsHealth: 'Umwelt',
      statsSludge: 'Schlamm entfernt'
    }
  }
}
