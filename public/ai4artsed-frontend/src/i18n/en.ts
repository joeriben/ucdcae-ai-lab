export const en = {
  app: {
    title: 'UCDCAE AI LAB',
    subtitle: 'Creative AI Transformations'
  },
  form: {
    inputLabel: 'Your Text',
    inputPlaceholder: 'e.g. A flower in the meadow',
    schemaLabel: 'Transformation Style',
    executeModeLabel: 'Execution Mode',
    safetyLabel: 'Safety Level',
    generateButton: 'Generate'
  },
  schemas: {
    dada: 'Dada (Random & Absurd)',
    bauhaus: 'Bauhaus (Geometric)',
    stillepost: 'Stille Post (Iterative)'
  },
  safetyLevels: {
    kids: 'Kids',
    youth: 'Youth',
    adult: 'Adults',
    research: 'Research'
  },
  stages: {
    pipeline_starting: 'Pipeline Starting',
    translation_and_safety: 'Translation & Safety',
    interception: 'Transformation',
    pre_output_safety: 'Output Safety',
    media_generation: 'Image Generation',
    completed: 'Completed'
  },
  status: {
    idle: 'Ready',
    executing: 'Pipeline running...',
    connectionSlow: 'Connection slow, retrying...',
    completed: 'Pipeline completed!',
    error: 'Error occurred'
  },
  entities: {
    input: 'Input',
    translation: 'Translation',
    safety: 'Safety Check',
    interception: 'Transformation',
    safety_pre_output: 'Output Safety',
    media: 'Generated Image'
  },
  properties: {
    chill: 'chill',
    chaotic: 'wild',
    narrative: 'tell stories',
    algorithmic: 'follow rules',
    historical: 'history',
    contemporary: 'present',
    explore: 'test AI',
    create: 'make art',
    playful: 'playful',
    serious: 'serious'
  },
  phase2: {
    title: 'Prompt Input',
    userInput: 'Your Input',
    yourInput: 'Your Input',
    yourIdea: 'Your Idea: WHAT should this be about?',
    rules: 'Your Rules: HOW should your idea be implemented?',
    yourInstructions: 'Your Instructions',
    what: 'WHAT',
    how: 'HOW',
    userInputPlaceholder: 'e.g. A flower in the meadow',
    inputPlaceholder: 'Your text appears here...',
    metaPrompt: 'Artistic Instruction',
    instruction: 'Instruction',
    transformation: 'Artistic Transformation',
    metaPromptPlaceholder: 'Describe the transformation...',
    result: 'Result',
    expectedResult: 'Expected Result',
    execute: 'Execute Pipeline',
    executing: 'Running...',
    transforming: 'LLM transforming...',
    startTransformation: 'Start Transformation',
    letsGo: 'Ok, let\'s go!',
    modified: 'Modified',
    reset: 'Reset',
    loadingConfig: 'Loading configuration...',
    loadingMetaPrompt: 'Loading meta-prompt...',
    errorLoadingConfig: 'Error loading configuration',
    errorLoadingMetaPrompt: 'Error loading meta-prompt',
    threeForces: '3 Forces Working Together',
    twoForces: 'WHAT + HOW → LLM → Result',
    yourPrompt: 'Your Prompt:',
    writeYourText: 'Write your text...',
    examples: 'Examples',
    estimatedTime: '~12 seconds',
    stage12Time: '~5-10 seconds',
    willAppearAfterExecution: 'Will appear after execution...',
    back: 'Back',
    retry: 'Retry',
    transformedPrompt: 'Transformed Prompt',
    notYetTransformed: 'Not yet transformed...',
    transform: 'Transform',
    reTransform: 'Try again differently',
    startAI: 'AI, process my input',
    aiWorking: 'AI is working...',
    continueToMedia: 'Continue to Image Generation',
    readyForMedia: 'Ready for Image Generation',
    stage1: 'Stage 1: Translation + Safety...',
    stage2: 'Stage 2: Transformation...',
    selectMedia: 'Choose your medium:',
    mediaImage: 'Image',
    mediaAudio: 'Audio',
    mediaVideo: 'Video',
    media3D: '3D',
    comingSoon: 'Coming soon',
    generateMedia: 'Start!'
  },
  phase3: {
    generating: 'Image is being generated...',
    generatingHint: '~30 seconds'
  },
  common: {
    back: 'Back',
    loading: 'Loading...',
    error: 'Error',
    retry: 'Retry',
    cancel: 'Cancel',
    checkingSafety: 'Checking...'
  },
  gallery: {
    title: 'Favorites',  // Session 145: "My" redundant with switch
    empty: 'No favorites yet',
    favorite: 'Add to favorites',
    unfavorite: 'Remove from favorites',
    continue: 'Continue editing',
    restore: 'Restore session',
    viewMine: 'My favorites',  // Session 145
    viewAll: 'All favorites'  // Session 145
  },
  settings: {
    authRequired: 'Authentication Required',
    authPrompt: 'Please enter the password to access settings:',
    passwordPlaceholder: 'Enter password...',
    authenticate: 'Sign In',
    authenticating: 'Authenticating...',
    // Admin page
    title: 'Administration',
    tabs: {
      export: 'Research Data',
      config: 'Configuration',
      demos: 'Minigame Demo',
      matrix: 'Model Matrix',
      status: 'Backend Status'
    },
    loading: 'Loading settings...',
    presets: {
      title: 'Model Presets',
      help: 'Use the <strong>Model Matrix</strong> tab to see all available presets and apply them with one click.',
      openMatrix: 'Open Model Matrix'
    },
    testingTools: {
      title: 'Testing Tools for Educators',
      help: 'Test and explore the pedagogical minigames and animations before using them with learners.',
      openPreview: 'Open Minigame Preview',
      pixelEditor: 'Pixel Template Editor',
      includes: 'Includes: Pixel Animation, Iceberg Melting, Forest Game, Rare Earths'
    },
    general: {
      title: 'General Configuration',
      uiMode: 'UI Mode',
      uiModeHelp: 'Interface complexity level',
      kids: 'Kids (8–12)',
      youth: 'Youth (13–17)',
      expert: 'Expert',
      safetyLevel: 'Safety Level',
      defaultLanguage: 'Default Language',
      germanDe: 'German (de)',
      englishEn: 'English (en)',
      arabicAr: 'Arabic (ar)',
      hebrewHe: 'Hebrew (he)',
      turkishTr: 'Turkish (tr)',
      koreanKo: 'Korean (ko)',
      ukrainianUk: 'Ukrainian (uk)',
      frenchFr: 'French (fr)',
      spanishEs: 'Spanish (es)'
    },
    safety: {
      kidsTitle: 'Children (8–12)',
      kidsDesc: 'All filters active: §86a, DSGVO, Youth Protection (age-appropriate parameters), VLM image check',
      youthTitle: 'Youth (13–17)',
      youthDesc: 'All filters active: §86a, DSGVO, Youth Protection (youth parameters), VLM image check',
      adultTitle: 'Adults',
      adultDesc: '§86a + DSGVO active. No youth protection, no VLM image check.',
      researchTitle: 'Research Mode',
      researchDesc: 'NO safety filters active. Only permitted for use by research institutions in the context of scientific research projects.'
    },
    safetyModels: {
      title: 'Local Safety Models',
      help: 'Local via Ollama — person names and safety checks never leave the system',
      safetyModel: 'Safety Model',
      safetyModelHelp: 'Guard model for content safety (§86a, youth protection)',
      dsgvoModel: 'DSGVO-Verify Model',
      dsgvoModelHelp: 'General-purpose model for DSGVO NER verification (not a guard model)',
      vlmModel: 'VLM Safety Model',
      vlmModelHelp: 'Vision model for post-generation image safety check (kids/youth)',
      fast: 'fast, minimal',
      recommended: 'recommended'
    },
    dsgvo: {
      title: 'DSGVO Warning',
      notCompliant: 'The following models are <strong>NOT DSGVO-compliant</strong> (data processed outside EU):',
      compliantHint: 'DSGVO-compliant options:'
    },
    models: {
      title: 'Model Configuration',
      help: 'Model identifiers with provider prefix: local/, mistral/, anthropic/, openai/, openrouter/',
      matrixAdvised: 'Use of Model Matrix is advised. However, you may configure your settings here freely.',
      ollamaAvailable: '{count} Ollama models available (type or select from dropdown)',
      stage1Text: 'Stage 1 - Text Model',
      stage1Vision: 'Stage 1 - Vision Model',
      stage2Interception: 'Stage 2 - Interception Model',
      stage2Optimization: 'Stage 2 - Optimization Model',
      stage3: 'Stage 3 - Translation/Safety Model',
      stage4Legacy: 'Stage 4 - Legacy Model',
      chatHelper: 'Chat Helper Model',
      imageAnalysis: 'Image Analysis Model',
      coding: 'Code Generation (Tone.js, p5.js)'
    },
    api: {
      title: 'API Configuration',
      llmProvider: 'LLM Provider',
      localFramework: 'Local LLM framework',
      externalProvider: 'External LLM Provider',
      cloudProvider: 'Cloud LLM provider - requires API key',
      noneLocal: 'None (Local only, DSGVO)',
      mistralEu: 'Mistral AI (EU-based, DSGVO)',
      anthropicDirect: 'Anthropic Direct API (NOT DSGVO)',
      openaiDirect: 'OpenAI Direct API (NOT DSGVO)',
      openrouterDirect: 'OpenRouter (NOT DSGVO, EU routing available)',
      mistralInfo: 'Mistral AI (EU-based)',
      mistralDsgvo: 'DSGVO-compliant (EU infrastructure)',
      anthropicInfo: 'Anthropic Direct API',
      anthropicNotDsgvo: 'NOT DSGVO-compliant',
      anthropicWarning: 'Data processed outside EU. Use only for non-educational contexts.',
      openaiInfo: 'OpenAI Direct API',
      openaiNotDsgvo: 'NOT DSGVO-compliant (US-based)',
      openaiWarning: 'Data processed in United States. Use only for non-educational contexts.',
      openrouterInfo: 'OpenRouter',
      openrouterNotDsgvo: 'NOT DSGVO-compliant (US company)',
      openrouterWarning: 'EU server routing configurable in OpenRouter settings, but company is US-based.',
      storedIn: 'Stored in',
      currentKey: 'Current'
    },
    save: {
      saveApply: 'Save & Apply',
      saving: 'Saving...',
      applying: 'Applying...',
      success: 'Settings saved and applied',
      presetApplied: 'Applied preset: {preset}'
    },
    backendStatus: {
      loading: 'Checking backend status...',
      refresh: 'Refresh',
      refreshing: 'Refreshing...',
      localInfrastructure: 'Local Infrastructure',
      cloudApis: 'Cloud APIs',
      outputConfigs: 'Output Configs by Backend',
      reachable: 'Reachable',
      unreachable: 'Unreachable',
      available: 'Available',
      unavailable: 'Unavailable',
      configured: 'Configured',
      notConfigured: 'Not Configured',
      gpuService: 'GPU Service',
      subBackend: 'Sub-Backend',
      status: 'Status',
      comfyui: 'ComfyUI / SwarmUI',
      ollama: 'Ollama',
      gpuHardware: 'GPU Hardware',
      notDetected: 'Not detected',
      showModels: 'Show models',
      hideModels: 'Hide models',
      provider: 'Provider',
      keyStatus: 'API Key',
      dsgvoLabel: 'DSGVO',
      region: 'Region',
      dsgvoCompliant: 'Compliant',
      dsgvoNotCompliant: 'Not Compliant',
      configsAvailable: '{available} of {total} configs available',
      hidden: 'hidden'
    }
  },
  pipeline: {
    yourInput: 'Your input',
    result: 'Result',
    generatedMedia: 'Generated image'
  },
  landing: {
    subtitlePrefix: 'Pedagogical-artistic experimentation platform of the',
    subtitleSuffix: 'for the explorative use of generative AI in cultural-aesthetic media education',
    research: '',
    features: {
      textTransformation: {
        title: 'Text Transformation',
        description: 'Perspective shift through AI — your prompt is transformed through artistic-pedagogical lenses into image, video, and sound.'
      },
      imageTransformation: {
        title: 'Image Transformation',
        description: 'Transform images through different models and perspectives into new images and videos.'
      },
      multiImage: {
        title: 'Image Fusion',
        description: 'Combine multiple images and merge them into new image compositions through AI models.'
      },
      canvas: {
        title: 'Canvas Workflow',
        description: 'Visual workflow composition — connect modules via drag & drop into custom AI pipelines.'
      },
      music: {
        title: 'Music Generation',
        description: 'AI-powered music creation with lyrics, tags, and stylistic control.'
      },
      latentLab: {
        title: 'Latent Lab',
        description: 'Vector space research — surrealization, dimension elimination, embedding interpolation.'
      }
    }
  },
  research: {
    locked: 'Only available in research mode',
    lockedHint: 'Requires safety level "Adult" or "Research" (config.py)',
    complianceTitle: 'Research Mode Notice',
    complianceWarning: 'In research mode, no safety filters are active for prompts or generated images. Unexpected or inappropriate results may occur.',
    complianceAge: 'This mode is not recommended for persons under 16 years of age.',
    complianceConfirm: 'I confirm that I have understood the notices',
    complianceCancel: 'Cancel',
    complianceProceed: 'Proceed'
  },
  presetOverlay: {
    title: 'Choose Perspective',
    close: 'Close'
  },
  imageUpload: {
    clickHere: 'Click here',
    orDragImage: 'or drag an image here',
    formatHint: 'PNG, JPG, WEBP (max 10MB)',
    invalidFormat: 'Invalid file format. Only PNG, JPG, and WEBP allowed.',
    fileTooLarge: 'File too large. Maximum: {max}MB',
    uploadFailed: 'Upload failed',
    infoOriginal: 'Original:',
    infoSize: 'Size:'
  },
  sketchCanvas: {
    drawHere: 'Draw your sketch here',
    pen: 'Pen',
    eraser: 'Eraser',
    undo: 'Undo',
    clear: 'Clear',
    done: 'Done',
    brushSmall: 'Small',
    brushMedium: 'Medium',
    brushLarge: 'Large'
  },
  mediaInput: {
    choosePreset: 'Choose Perspective',
    translateToEnglish: 'Translate to English',
    copy: 'Copy',
    paste: 'Paste',
    delete: 'Delete',
    loading: 'Loading...',
    contentBlocked: 'Content blocked'
  },
  nav: {
    about: 'About',
    impressum: 'Imprint',
    privacy: 'Privacy',
    docs: 'Documentation',
    language: 'Switch language',
    settings: 'Settings',
    canvas: 'Canvas Workflow'
  },
  canvas: {
    title: 'Canvas Workflow',
    newWorkflow: 'New Workflow',
    importWorkflow: 'Import',
    exportWorkflow: 'Export',
    execute: 'Execute',
    ready: 'Ready',
    errors: 'errors',
    discardWorkflow: 'Discard current workflow?',
    importError: 'Failed to import file',
    selectTransformation: 'Select Transformation',
    selectOutput: 'Select Output Model',
    search: 'Search...',
    noResults: 'No results found',
    dragHint: 'Click or drag modules onto the canvas',
    editNameHint: '(double-click to edit)',
    modules: 'Modules',
    toggleSidebar: 'Toggle sidebar',
    dsgvoTooltip: 'Canvas workflows may use external LLM APIs. GDPR compliance is the responsibility of the user.',
    batchExecute: 'Batch Execute',
    batchExecution: 'Batch Execution',
    batchAbort: 'Abort Batch',
    abort: 'Abort',
    cancel: 'Cancel',
    loading: 'Loading...',
    executingWorkflow: 'Executing Workflow...',
    starting: 'Starting...',
    nodes: 'nodes',
    batchRunCount: 'Number of Runs',
    batchUseSeed: 'Use Base Seed',
    batchBaseSeed: 'Base Seed',
    batchSeedHint: 'Each run: seed + index',
    batchStart: 'Start Batch',
    stage: {
      configSelectPlaceholder: 'Select...',
      evaluationCriteriaFallback: 'Evaluation criteria...',
      feedbackInputTitle: 'Feedback Input',
      deleteTitle: 'Delete',
      selectLlmPlaceholder: 'Select LLM...',
      resizeTitle: 'Resize',
      input: {
        promptPlaceholder: 'Your prompt...'
      },
      imageInput: {
        uploadLabel: 'Upload Image'
      },
      interception: {
        contextPromptLabel: 'Context Prompt',
        contextPromptPlaceholder: 'Transformation instructions...'
      },
      translation: {
        translationPromptLabel: 'Translation Prompt',
        translationPromptPlaceholder: 'Translation instructions...'
      },
      modelAdaption: {
        targetModelLabel: 'Target Model',
        noAdaptionOption: 'No Adaption',
        videoModelsOption: 'Video Models',
        audioModelsOption: 'Audio Models'
      },
      comparisonEvaluator: {
        criteriaLabel: 'Comparison Criteria',
        criteriaPlaceholder: 'e.g. Compare by originality, clarity, detail...',
        infoText: 'Connect up to 3 text outputs'
      },
      seed: {
        modeLabel: 'Mode',
        modeFixed: 'Fixed',
        modeRandom: 'Random',
        valueLabel: 'Value',
        baseLabel: 'Base'
      },
      resolution: {
        customOption: 'Custom',
        widthLabel: 'Width',
        heightLabel: 'Height'
      },
      collector: {
        emptyText: 'Waiting for execution...'
      },
      evaluation: {
        typeLabel: 'Evaluation Type',
        typeCreativity: 'Creativity',
        typeQuality: 'Quality',
        typeCustom: 'Custom',
        criteriaLabel: 'Evaluation Criteria',
        outputTypeLabel: 'Output Type',
        outputCommentary: 'Commentary + Binary',
        outputScore: 'Commentary + Score + Binary',
        outputAll: 'All',
        evalPassTitle: 'Pass (forward)',
        evalFailTitle: 'Feedback (backward)',
        evalCommentaryTitle: 'Commentary (forward)'
      },
      imageEvaluation: {
        visionModelPlaceholder: 'Select Vision Model...',
        frameworkLabel: 'Analysis Framework',
        frameworkPanofsky: 'Art Historical (Panofsky)',
        frameworkEducational: 'Educational Theory',
        frameworkEthical: 'Ethical',
        frameworkCritical: 'Critical/Decolonial',
        frameworkCustom: 'Custom',
        customPromptLabel: 'Analysis Prompt',
        customPromptPlaceholder: 'Describe how the image should be analyzed...'
      },
      randomPrompt: {
        tokenLimit: 'Prompt Length'
      },
      display: {
        imageAlt: 'Preview',
        emptyText: 'Preview (after execution)'
      }
    }
  },
  about: {
    title: 'About the UCDCAE AI LAB',
    intro: 'The UCDCAE AI LAB is a pedagogical-artistic experimentation platform of the UNESCO Chair in Digital Culture and Arts in Education for the explorative use of generative artificial intelligence in cultural-aesthetic media education. It was developed within the AI4ArtsEd and COMeARTS projects.',
    project: {
      title: 'The Project',
      description: 'AI is transforming society and the world of work; it is increasingly becoming a subject of education. The project explores opportunities, conditions, and limits of the pedagogical use of artificial intelligence (AI) in culturally diversity-sensitive settings of cultural education.',
      paragraph2: 'In three sub-projects – General Pedagogy (TPap), Computer Science (TPinf), and Art Education (TPkp) – creativity-oriented pedagogical AI practice research and computer science AI conception and programming interlock in close cooperation. From the outset, the project systematically involves artistic-pedagogical practitioners in the design process; it acts as a bridge between professional (quality-related, aesthetic, ethical, and value-based) pedagogical-practical implementation on the one hand and the implementation and training process of the computer science sub-project on the other.',
      paragraph3: 'A participatory design process spanning approximately two years aims to produce an open-source AI technology that explores the extent to which AI systems can already incorporate artistic-pedagogical principles at their structural level under favorable real-world conditions.',
      paragraph4: 'The focus is on a) the future applicability and added value of highly innovative technologies for cultural education, b) the scope and limits of AI literacy among teachers and learners, and c) the overarching question of the assessability and evaluation of the transformation of pedagogical settings by complex non-human actors in terms of pedagogical ethics and technology assessment.',
      moreInfo: 'More information:'
    },
    subproject: {
      title: 'Sub-project "General Pedagogy"',
      description: 'The sub-project "General Pedagogy" researches possibilities and limits of an artistic-pedagogical AI design process based on participatory practice research within the framework of the joint research question of the collaborative project. For this purpose, it conducts a series of research, analyses, expert workshops, and open spaces in the first project year. The subsequent project phase, designed as a feedback loop in several cycles, explores the use of a prototype with pedagogical practitioners and artist-educators, particularly in non-formal cultural education, as a relational and collective transformative educational process.'
    },
    team: {
      title: 'Team',
      projectLead: 'Project Lead',
      leadName: 'Prof. Dr. Benjamin Jörissen',
      leadInstitute: 'Institute of Education',
      leadChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
      leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
      researcher: 'Research Associate',
      researcherName: 'Vanessa Baumann',
      researcherInstitute: 'Institute of Education',
      researcherChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
      researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
    },
    funding: {
      title: 'Funded by'
    }
  },
  legal: {
    impressum: {
      title: 'Imprint',
      publisher: 'Publisher',
      represented: 'Represented by the President',
      responsible: 'Responsible for content',
      authority: 'Supervisory Authority',
      moreInfo: 'Additional Information',
      moreInfoText: 'Complete imprint of FAU:',
      funding: 'Funded by'
    },
    privacy: {
      title: 'Privacy Policy',
      notice: 'Notice: Generated content is stored on the server for research purposes. No user or IP data is collected. Uploaded images are not stored.',
      usage: 'Use of this platform is exclusively permitted for registered cooperation partners of the UCDCAE AI LAB. The data protection agreements made in this context apply. If you have any questions, please contact vanessa.baumann@fau.de.'
    }
  },
  docs: {
    title: 'Documentation & Guide',
    intro: {
      title: 'Welcome',
      content: 'Creative experiments with AI transformations.'
    },
    gettingStarted: {
      title: 'Getting Started',
      step1: 'Select properties from quadrants',
      step2: 'Enter text or image',
      step3: 'Start transformation'
    },
    modes: {
      title: 'Modes',
      mode1: { name: 'Direct', desc: 'Quick experiments' },
      mode2: { name: 'Text', desc: 'Text-based transformations' },
      mode3: { name: 'Image', desc: 'Image-based procedures' }
    },
    support: {
      title: 'Support',
      content: 'For questions:'
    },
    wikipedia: {
      title: 'Wikipedia Research',
      subtitle: 'Knowledge about the world as part of artistic processes',
      feature: 'Artistic processes require not only aesthetic knowledge, but also knowledge about facts in the world. The AI researches Wikipedia during transformation to find factual information.',
      languages: 'Over 70 languages are supported',
      languagesDesc: 'The AI automatically chooses the appropriate language Wikipedia for each topic:',
      examples: {
        nigeria: 'Topic about Nigeria → Hausa, Yoruba, Igbo, or English',
        india: 'Topic about India → Hindi, Tamil, Bengali, or other regional languages',
        indigenous: 'Indigenous cultures → Quechua, Māori, Inuktitut, etc.'
      },
      why: 'Transparency: What does the AI know?',
      whyDesc: 'The system shows all research attempts: Both found articles (as clickable links) and terms for which nothing was found. This makes visible what the AI thinks it knows – and what it does not.',
      culturalRespect: 'Invitation to research yourself',
      culturalRespectDesc: 'The displayed Wikipedia links are an invitation to learn more yourself. Click on the links to check the sources and expand your own knowledge.',
      limitations: 'AI research is an aid, not a substitute for your own engagement with the topic.'
    }
  },
  multiImage: {
    image1Label: 'Image 1',
    image2Label: 'Image 2 (optional)',
    image3Label: 'Image 3 (optional)',
    contextLabel: 'Describe what you want to do with the images',
    contextPlaceholder: 'e.g. Insert the house from image 2 and the horse from image 3 into image 1. Preserve colors and style from image 1.',
    modeTitle: 'Multiple Images → Image',
    selectConfig: 'Choose your model:',
    generating: 'Images are being fused...'
  },
  imageTransform: {
    imageLabel: 'Your Image',
    contextLabel: 'Describe what you want to change in the image',
    contextPlaceholder: 'e.g. Transform it into an oil painting... Make it more colorful... Add a sunset...',
    uploadMode: 'Upload',
    sketchMode: 'Sketch'
  },
  textTransform: {
    inputLabel: 'Your Idea = WHAT?',
    inputTooltip: 'Enter what your creation should be about.',
    inputPlaceholder: 'e.g. A festival in my street: ...',
    contextLabel: 'Your Rules = HOW?',
    contextTooltip: 'Enter how your idea should be presented, or click the circle icon!',
    contextPlaceholder: 'e.g. Describe everything as the birds in the trees perceive it!',
    resultLabel: 'Idea + Rules = Prompt',
    resultPlaceholder: 'Prompt will appear after clicking start (or enter your own text)',
    optimizedLabel: 'Model-Optimized Prompt',
    optimizedPlaceholder: 'The optimized prompt will appear after model selection.'
  },
  training: {
    info: {
      title: 'About LoRA Training',
      studioDescription: 'Train custom LoRA models for Stable Diffusion 3.5 Large with your own images.',
      description: 'This built-in training is designed for quick tests.',
      limitations: 'Limitations',
      limitationDuration: 'Training takes 1-3 hours',
      limitationBlocking: 'Blocks image generation during training',
      limitationConfig: 'Limited configuration options',
      showMore: 'Learn more',
      showLess: 'Show less'
    },
    placeholders: {
      projectName: 'e.g. Our School Building',
      triggerWords: 'e.g. our_school_building, schoolyard, classroom'
    },
    labels: {
      projectName: 'Project Name',
      triggerWords: 'Trigger Words',
      triggerHelp: 'Comma-separated tags. First = primary trigger, rest = additional tags per image.',
      images: 'Training Images (10–50 recommended)',
      dropZone: 'Click or drop images here',
      imagesSelected: '{count} images selected',
      logs: 'Training Logs',
      waiting: 'Waiting for training to start...'
    },
    buttons: {
      start: 'Start Training',
      stop: 'Stop',
      inProgress: 'Training in Progress...',
      delete: 'Delete Project Files (GDPR)',
      cancel: 'Cancel'
    },
    vram: {
      title: 'GPU VRAM Check',
      checking: 'Checking VRAM...',
      used: 'used',
      free: 'free',
      notEnough: 'Not enough free VRAM for training (need {gb} GB).',
      clearQuestion: 'Clear VRAM to continue?',
      enough: 'Enough VRAM available for training.',
      clearing: 'Clearing VRAM...',
      newFree: 'New free',
      clearBtn: 'Clear ComfyUI + Ollama VRAM'
    }
  },
  safetyBadges: {
    '§86a': '§86a',
    '86a_filter': '§86a',
    age_filter: 'Age Filter',
    dsgvo_ner: 'GDPR',
    dsgvo_llm: 'GDPR',
    translation: '\u2192 EN',
    fast_filter: 'Content',
    llm_context_check: 'Content (LLM)',
    llm_safety_check: 'Youth Protection',
    llm_check_failed: 'Check Failed',
    disabled: '\u2014'
  },
  safetyBlocked: {
    vlm: 'Your prompt was fine, but the generated image was flagged as unsuitable by an image analysis AI. This can happen \u2014 image generation is not always predictable. Just try again, every generation is different!',
    para86a: 'Your prompt was blocked because it contains symbols or terms that are prohibited under German law (\u00A786a StGB). This rule protects us all from hate and violence. Try a different topic!',
    dsgvo: 'Your prompt was blocked because it contains something that looks like a person\'s name. This is protected by the General Data Protection Regulation (GDPR). Use descriptions like \"a girl\" or \"an old man\" instead of names.',
    kids: 'Your prompt was blocked by the child safety filter. Some terms are not suitable for children because they can be scary or disturbing. Try describing your idea with friendlier words!',
    youth: 'Your prompt was blocked by the youth protection filter. Some content is not suitable for teenagers either. Try rephrasing your idea!',
    generic: 'Your prompt was blocked by the safety system. The system protects you from unsuitable content. Try a different wording!',
    inputImage: 'The uploaded image was flagged as unsuitable by an image analysis AI. Please use a different image.',
    vlmSaw: 'The image AI saw',
    systemUnavailable: 'The safety system (Ollama) is not responding, so no further processing is possible. Please contact the system administrator.',
    suggestionLoading: 'Hang on, I have an idea...',
    suggestionError: 'I couldn\'t generate a suggestion right now. Just try again with different words!'
  },
  splitCombine: {
    infoTitle: 'Split & Combine - Semantic Vector Fusion',
    infoDescription: 'This workflow fuses two prompts at the semantic vector level. The result is not a simple blend, but a deeper mathematical connection of meaning spaces.',
    purposeTitle: 'Pedagogical Purpose',
    purposeText: 'Explore how AI models represent meaning as numerical spaces. What happens when we mathematically merge different concepts?',
    techTitle: 'Technical Details',
    techText: 'Model: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
  },
  partialElimination: {
    infoTitle: 'Partial Elimination - Vector Deconstruction',
    infoDescription: 'This workflow specifically manipulates parts of the semantic vector. By eliminating certain dimensions, we can observe which aspects of meaning are lost.',
    purposeTitle: 'Pedagogical Purpose',
    purposeText: 'Understand how meaning is encoded across different dimensions of the vector space. What remains when we "switch off" parts?',
    techTitle: 'Technical Details',
    techText: 'Model: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
    encoderLabel: 'Text Encoder',
    modeLabel: 'Elimination Mode',
    dimensionRange: 'Dimension Range',
    selected: 'Selected',
    dimensions: 'Dimensions',
    emptyTitle: 'Waiting for generation...',
    emptySubtitle: 'Results will appear here',
    referenceLabel: 'Reference Image',
    referenceDesc: 'Unmanipulated output (original)',
    innerLabel: 'Inner range eliminated',
    outerLabel: 'Outer range eliminated'
  },
  surrealizer: {
    infoTitle: 'Surrealizer — Extrapolation Beyond the Known',
    infoDescription: 'Two AI "brains" read your text: CLIP-L understands language through images (77 tokens, 768 dimensions), T5-XXL understands it purely linguistically (512 tokens, 4096 dimensions). The slider doesn\'t blend between them — it pushes the image far beyond what either encoder would produce alone. How the extrapolation is distributed across the token sequence is controlled by the Fusion Strategy.',
    purposeTitle: 'Fusion Strategy & Slider',
    purposeText: 'The Fusion Strategy determines WHERE extrapolation happens. Dual Alpha gently distorts the first 77 tokens (the visual core from CLIP-L) while fully extrapolating the extended T5 tokens — the image stays structurally recognizable but becomes aesthetically surprising. Normalized applies the same extrapolation everywhere but controls magnitude so no tokens dominate attention. Legacy extrapolates only the first 77 tokens and leaves the rest unchanged — the original approach, but less surreal with long prompts because unmodified T5 tokens dilute the effect. ◆ The α slider controls HOW FAR: α = 0 is pure CLIP-L (normal), α = 1 is pure T5 (still normal), α > 1 pushes past T5 into unexplored vector space. Sweet spot: 15–35. ◆ α < 0 inverts T5 and amplifies CLIP-L — qualitatively different hallucinations.',
    techTitle: 'How It Works',
    techText: 'CLIP-L encodes into 768 dimensions, padded to 4096 — 3328 dimensions are zeros. T5-XXL fills all 4096 dimensions. This asymmetry is the engine: extrapolation exploits the sparse CLIP-L space. The formula (1-α)·CLIP-L + α·T5 applies to the first 77 token positions where both encoders have data. Beyond position 77 only T5 exists. How these extended tokens are treated is what distinguishes the three strategies: Dual Alpha multiplies them by α (full extrapolation), Normalized does the same but rescales to typical T5 magnitude, Legacy leaves them at 1× (unchanged). At high α values, this difference is dramatic — a factor of 25× between Legacy and the other two.',
    sliderLabel: 'Extrapolation (α)',
    sliderNormal: 'normal',
    sliderWeird: 'weird',
    sliderCrazy: 'crazy',
    sliderExtremeWeird: 'super weird',
    sliderExtremeCrazy: 'super crazy',
    sliderHint: "α<0: past CLIP {'|'} α=0: pure CLIP {'|'} α=1: pure T5 {'|'} α>1: past T5",
    expandLabel: 'Expand prompt for T5',
    expandSuggest: 'Short prompt detected — T5 expansion significantly improves results with few words.',
    expandHint: 'Your prompt has few words (~{count} CLIP tokens). For optimal hallucinations, the AI can narratively expand the T5 context.',
    expandActive: 'Expanding prompt...',
    expandResultLabel: 'T5 expansion (T5 encoder only)',
    advancedLabel: 'Advanced Settings',
    fusionStrategyLabel: 'Fusion Strategy',
    fusion_dual_alpha: 'Dual Alpha',
    fusion_normalized: 'Normalized',
    fusion_legacy: 'Legacy',
    fusionHint_dual_alpha: 'Gentle α on core tokens (structural anchor via CLIP-L), full α on extended tokens (aesthetic surprise). Structurally recognizable, aesthetically surprising.',
    fusionHint_normalized: 'Uniform α on all tokens, then L2-normalized to typical T5 magnitude. Same extrapolation direction as Dual Alpha, but magnitude-controlled — no tokens dominate attention.',
    fusionHint_legacy: 'Original behavior: extrapolates first 77 tokens, appends T5 remainder unchanged (1×). Less surreal with long prompts — unmodified tokens dilute the effect.',
    negativeLabel: 'Negative Prompt',
    negativeHint: 'Extrapolated with the same α. Determines what the image extrapolates AWAY from — different negatives produce fundamentally different aesthetics.',
    cfgLabel: 'CFG Scale',
    cfgHint: 'Classifier-Free Guidance: strength of prompt influence. Higher = stronger effect, less variation.',
    seedLabel: 'Seed',
    seedHint: '-1 = random. Fixed seed enables A/B comparison across strategies.'
  },
  musicGeneration: {
    infoTitle: 'Music Generation',
    infoDescription: 'Create music from text and style tags. The AI generates melodies, rhythms, and harmonies based on your lyrics and genre specifications.',
    purposeTitle: 'Pedagogical Purpose',
    purposeText: 'Explore how AI interprets musical concepts. How does word choice in lyrics affect the melody?',
    lyricsLabel: 'Lyrics (Text)',
    lyricsPlaceholder: '[Verse]\nYour lyrics here...\n\n[Chorus]\nChorus...',
    tagsLabel: 'Style Tags',
    tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
    selectModel: 'Choose a music model:',
    generate: 'Generate Music',
    generating: 'Generating music...'
  },
  musicGen: {
    simpleMode: 'Simple',
    advancedMode: 'Advanced',
    lyricsLabel: 'Lyrics',
    lyricsPlaceholder: 'Write your song lyrics with structure markers like [Verse], [Chorus], [Bridge]...\n\nExample:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
    tagsLabel: 'Style Tags',
    tagsPlaceholder: 'Genre, mood, instruments...\n\nExample: ska, aggressive, upbeat, high definition, bass and sax trio',
    refineButton: 'Refine Lyrics & Tags',
    refinedLyricsLabel: 'Refined Lyrics',
    refinedLyricsPlaceholder: 'Your refined lyrics will appear here...',
    refiningLyricsMessage: 'AI is refining your lyrics...',
    refinedTagsLabel: 'Refined Tags',
    refinedTagsPlaceholder: 'Refined style tags will appear here...',
    refiningTagsMessage: 'AI is generating matching style tags...',
    selectModel: 'Choose a Music Model',
    generateButton: 'Generate Music',
    quality: 'Quality'
  },
  musicGenV2: {
    lyricsWorkshop: 'Lyrics Workshop',
    lyricsInput: 'Your Text',
    lyricsPlaceholder: 'Write lyrics, a theme, keywords, or a mood...',
    themeToLyrics: 'Keywords to Song Lyrics',
    refineLyrics: 'Structure Song Lyrics',
    resultLabel: 'Result',
    resultPlaceholder: 'Your lyrics will appear here...',
    expandingTheme: 'AI is writing song lyrics from your keywords...',
    refiningLyrics: 'AI is structuring your song lyrics...',
    soundExplorer: 'Sound Explorer',
    suggestFromLyrics: 'Suggest from Lyrics',
    suggestingTags: 'AI is analyzing your lyrics...',
    mostImportant: 'most important',
    dimGenre: 'Genre',
    dimTimbre: 'Timbre',
    dimGender: 'Voice',
    dimMood: 'Mood',
    dimInstrument: 'Instruments',
    dimScene: 'Scene',
    dimRegion: 'Region (UNESCO)',
    dimTopic: 'Topic',
    audioLength: 'Audio Length',
    generateButton: 'Generate Music',
    selectModel: 'Model',
    customTags: 'Custom Tags',
    customTagsPlaceholder: 'e.g. acoustic,dreamy,summer_vibes'
  },
  latentLab: {
    tabs: {
      image: 'Image Lab',
      textlab: 'Latent Text Lab',
      crossmodal: 'Crossmodal Lab'
    },
    imageLab: {
      headerTitle: 'Image Lab — Visual Vector Space Research',
      headerSubtitle: 'Five tools for investigating how diffusion models generate images from text: from denoising through attention and fusion to vector arithmetic.',
      tabs: {
        archaeology: {
          label: 'Denoising Archaeology',
          short: 'Watch the model work'
        },
        attention: {
          label: 'Attention Cartography',
          short: 'See where the model looks'
        },
        fusion: {
          label: 'Encoder Fusion',
          short: 'Surrealistic blending'
        },
        probing: {
          label: 'Feature Probing',
          short: 'Dimension-level analysis'
        },
        algebra: {
          label: 'Concept Algebra',
          short: 'Vector arithmetic'
        }
      }
    },
    comingSoon: 'This tool will be implemented in a future version.',
    shared: {
      negativeHint: 'Terms the model should actively avoid (e.g. "blurry, text")',
      stepsHint: 'More steps = higher quality but longer generation time',
      cfgHint: 'Classifier-Free Guidance: higher = stronger prompt adherence, less variation',
      seedHint: '-1 = random, fixed value = reproducible result',
      recordingActive: 'Recording active',
      recordingCount: '{count} recording | {count} recordings',
      recordingTooltip: 'Research data is saved automatically',
    },
    attention: {
      headerTitle: 'Attention Cartography — Which word steers which image region?',
      headerSubtitle: 'For each word in the prompt, a heatmap overlay on the generated image shows WHERE in the image that word had the most influence. This reveals how the model spatially distributes semantic concepts.',
      explanationToggle: 'Show detailed explanation',
      explainWhatTitle: 'What does this tool show?',
      explainWhatText: 'When a diffusion model generates an image, it does not read the prompt word by word like a set of instructions. Instead, a mechanism called "attention" distributes the influence of each word across different image regions. The word "house" mainly influences the region where the house appears — but also neighboring areas, because the model understands the context of the entire scene. This tool makes that distribution visible: click on a word and see which image regions light up.',
      explainHowTitle: 'How do I read the heatmap?',
      explainHowText: 'Bright, intense color = strong influence of the word on that region. Dark or absent color = little influence. If you select multiple words, they appear in different colors. Note: the maps are NOT perfectly sharp-edged — this is not a bug, but shows that the model processes concepts contextually, not in isolation. A "house" in a farm scene also has some influence on animals and fields, because the model understands the scene as a whole.',
      explainReadTitle: 'What do the two sliders reveal?',
      explainReadText: 'The denoising step slider shows WHEN in the 25-step generation process you are viewing attention. Early steps show rough layout planning, late steps show detail assignment. The network depth selector shows WHERE in the transformer attention is measured: shallow layers (near input) show global composition planning, middle layers semantic assignment, deep layers fine-tuning. Both axes are independent — it is worth systematically exploring different combinations.',
      techTitle: 'Technical details',
      techText: 'SD3.5 uses an MMDiT (Multimodal Diffusion Transformer) with joint attention: image and text tokens attend to each other across 24 transformer blocks. We replace the default SDPA processor with a manual softmax(QK^T/√d) processor at 3 selected blocks to extract the text→image attention submatrix. Maps are 64x64 resolution (patch grid), upscaled to image resolution via bilinear interpolation. SD3.5 uses two text encoders: CLIP-L (BPE, 77 tokens) and T5-XXL (SentencePiece, 512 tokens). Both can be toggled here to see how different tokenization strategies affect attention.',
      referencesTitle: 'Research References',
      promptLabel: 'Prompt',
      promptPlaceholder: 'e.g. A house stands in a landscape, surrounded by farmland, nature and animals. Some people can be seen.',
      generate: 'Generate + Analyze',
      generating: 'Generating image and extracting attention...',
      emptyHint: 'Enter a prompt and click Generate to visualize the model\'s attention maps.',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      tokensLabel: 'Tokens',
      tokensHint: 'Click one or more words. Subword tokens (e.g. "Ku"+"gel") are automatically combined. Multiple words appear in different colors.',
      timestepLabel: 'Denoising step',
      timestepHint: 'Diffusion models generate images in 25 steps from noise to image. Early steps establish rough structure, late steps refine details. This slider shows what the model attends to at each step.',
      step: 'Step',
      layerLabel: 'Network depth',
      layerHint: 'At every denoising step, the signal passes through all 24 transformer layers. Shallow layers (near input) capture global composition, middle layers semantic assignment, deep layers (near output) fine details. Both controls are independent: step = when in the process, depth = where in the network.',
      layerEarly: 'Shallow (Composition)',
      layerMid: 'Middle (Semantics)',
      layerLate: 'Deep (Detail)',
      opacityLabel: 'Heatmap',
      opacityHint: 'Strength of the colored overlay on the image.',
      baseImageLabel: 'Base image',
      baseColor: 'Color',
      baseBW: 'B/W',
      baseOff: 'Off',
      baseImageHint: 'Color shows the original image. B/W desaturates it so heatmap colors stand out. Off hides the image entirely and shows only the attention map.',
      encoderLabel: 'Text Encoder',
      encoderClipL: 'CLIP-L (77 Tokens)',
      encoderT5: 'T5-XXL (512 Tokens)',
      encoderHint: 'SD3.5 uses two text encoders with different tokenization. CLIP-L uses BPE (Byte-Pair Encoding), T5-XXL uses SentencePiece. Compare how both encoders process the same prompt and which image regions each one steers.',
      download: 'Download Image'
    },
    probing: {
      headerTitle: 'Feature Probing — Which dimensions encode what?',
      headerSubtitle: 'Compare two prompts and discover which embedding dimensions encode the semantic difference. Selectively transfer individual dimensions to see how they affect the image.',
      explanationToggle: 'Show detailed explanation',
      explainWhatTitle: 'What does this tool show?',
      explainWhatText: 'Every word is converted by the text encoder into a high-dimensional vector (e.g. 4096 dimensions for T5). When you change a word in the prompt — e.g. "red" to "blue" — certain dimensions change more than others. This tool shows you WHICH dimensions change most and lets you selectively transfer individual dimensions from prompt B into prompt A.',
      explainHowTitle: 'How does the transfer work?',
      explainHowText: 'The bar chart shows all dimensions sorted by difference magnitude. Use the rank range controls (From/To) to select a window — e.g. just the top 100 or specifically ranks 880–920. Clicking "Transfer" regenerates the image with the same settings (same seed!) — but with selected dimensions from prompt B. This lets you see exactly what those dimensions "encode".',
      explainReadTitle: 'How do I read the bar chart?',
      explainReadText: 'Each bar represents one embedding dimension. The length shows how much that dimension differs between prompt A and B. Dimensions with large differences are the most likely carriers of the semantic change. But note: embeddings are distributed — often multiple dimensions together are needed to produce a visible change.',
      techTitle: 'Technical details',
      techText: 'SD3.5 uses three text encoders: CLIP-L (768d), CLIP-G (1280d) and T5-XXL (4096d). You can probe each individually. The difference is computed as mean absolute deviation across all token positions: mean(abs(B-A), dim=tokens). The transfer replaces selected dimensions across all token positions simultaneously.',
      referencesTitle: 'Research References',
      promptALabel: 'Prompt A (Original)',
      promptBLabel: 'Prompt B (Comparison)',
      promptAPlaceholder: 'e.g. A red house by the lake',
      promptBPlaceholder: 'e.g. A blue house by the lake',
      encoderLabel: 'Encoder',
      encoderAll: 'All (recommended)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      analyzeBtn: 'Analyze',
      analyzing: 'Encoding and comparing prompts...',
      transferBtn: 'Transfer selected vector dimensions from Prompt B into the generated image',
      transferring: 'Generating image with modified embedding...',
      rankFromLabel: 'From rank',
      rankToLabel: 'To rank',
      sliderLabel: 'Select dimensions from Prompt B',
      range1Label: 'Range 1',
      range2Label: 'Range 2',
      addRange: 'Add range',
      selectionDesc: '{count} dimensions from Prompt B selected (rank {ranges} of {total})',
      listTitle: 'The {count} dimensions from Prompt B with the largest difference to Prompt A',
      sortAsc: 'Ascending',
      sortDesc: 'Descending',
      originalLabel: 'Original (Prompt A)',
      modifiedLabel: 'Modified (Transfer from Prompt B)',
      modifiedHint: 'Select a rank range below and click "Transfer" — this will show prompt A with the transferred dimensions from B (same seed).',
      noDifference: 'The embeddings are identical — change prompt B.',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      selectAll: 'All',
      selectNone: 'None',
      encoderHint: 'All = all encoders combined. CLIP-L/CLIP-G/T5 = isolates one encoder for the analysis.',
      sliderHint: 'Select a rank range of the most important embedding dimensions (sorted by difference between A and B).',
      transferHint: 'Transfers the selected dimensions from Prompt B onto Prompt A and generates a new image.',
      downloadOriginal: 'Download Original',
      downloadModified: 'Download Modified'
    },
    algebra: {
      headerTitle: 'Concept Algebra \u2014 Vector arithmetic on image embeddings',
      headerSubtitle: 'Apply the famous word2vec analogy to image generation: King \u2212 Man + Woman \u2248 Queen. Three prompts are encoded and algebraically combined.',
      explanationToggle: 'Show detailed explanation',
      explainWhatTitle: 'What does this tool show?',
      explainWhatText: 'In 2013, Mikolov showed that word embeddings encode semantic relationships as linear directions: the vector for "King" minus "Man" plus "Woman" yields a vector close to "Queen". This tool applies that idea to SD3.5\'s text encoders: instead of single words, you manipulate entire prompt embeddings. The result is an image that contains concept A but with B replaced by C.',
      explainHowTitle: 'How does the algebra work \u2014 and why not just use a negative prompt?',
      explainHowText: 'You enter three prompts: A (base), B (subtract), and C (add). The formula is: Result = A \u2212 Scale\u2081\u00d7B + Scale\u2082\u00d7C. The scale sliders control intensity: at 1.0, B is fully subtracted and C fully added. At 0.5, only half. Values above 1.0 amplify the effect. \u2014 Why not just use "A + C" as the prompt and "B" as the negative prompt? Because that does something fundamentally different: A negative prompt steers the denoising process away from B at EVERY one of the 25 steps \u2014 the model decides step by step how to interpret "not B". Concept Algebra instead computes a new vector BEFORE image generation: the subtraction happens in embedding space, not in the diffusion process. The result is a single vector that directly encodes "A without B-ness plus C-ness". The negative prompt says "don\'t do this". The algebra says "take this concept out and put that one in" \u2014 a surgical operation in meaning-space rather than a step-by-step avoidance strategy.',
      explainReadTitle: 'What do the results mean?',
      explainReadText: 'On the left you see the reference image (prompt A only, same seed). On the right, the algebra result. If the analogy works, the right image should show concept A but with the semantic change B\u2192C. Example: "Sunset at the beach" \u2212 "Beach" + "Mountains" \u2248 "Sunset over mountains". The L2 distance shows how far the result has moved from the original. \u2014 Is the operation commutative? No. Subtraction of B and addition of C happen relative to vector A. The direction B\u2192C only makes sense in the context of A: "King \u2212 Man" removes the "male" directions from the King vector, "+ Woman" adds the "female" directions \u2014 the result lands near "Queen". C is not surgically placed where B was removed; it is simply added. That this still works shows that semantic relationships are encoded as consistent linear directions in the vector space.',
      techTitle: 'Technical details',
      techText: 'The algebra is performed on the selected encoder embeddings: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), or all combined (589 tokens \u00d7 4096d). The same operation is also applied to pooled embeddings (2048d). Both images use the same seed for fair comparison.',
      referencesTitle: 'Research References',
      promptALabel: 'Prompt A (Base)',
      promptAPlaceholder: 'e.g. Sunset at the beach with palm trees',
      promptBLabel: 'Prompt B (Subtract)',
      promptBPlaceholder: 'e.g. Beach with palm trees',
      promptCLabel: 'Prompt C (Add)',
      promptCPlaceholder: 'e.g. Snow-covered mountains',
      formulaLabel: 'A \u2212 B + C = ?',
      encoderLabel: 'Encoder',
      encoderAll: 'All (recommended)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      generateBtn: 'Compute',
      generating: 'Computing embeddings and generating images...',
      referenceLabel: 'Reference (Prompt A)',
      resultLabel: 'Result (A \u2212 B + C)',
      l2Label: 'L2 distance from original',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      scaleSubLabel: 'Subtraction scale',
      scaleAddLabel: 'Addition scale',
      encoderHint: 'All = all encoders combined. CLIP-L/CLIP-G/T5 = isolates one encoder for the arithmetic.',
      scaleSubHint: 'Weight of subtraction (B). Higher = stronger removal of concept B.',
      scaleAddHint: 'Weight of addition (C). Higher = stronger injection of concept C.',
      l2Hint: 'Euclidean distance in embedding space. Smaller = more similar, larger = more different.',
      downloadReference: 'Download Reference',
      downloadResult: 'Download Result',
      resultHint: 'Enter three prompts and click Compute \u2014 the result of the vector arithmetic will appear here.'
    },
    archaeology: {
      headerTitle: 'Denoising Archaeology \u2014 How does noise become an image?',
      headerSubtitle: 'Observe every single denoising step. Diffusion models don\'t draw left-to-right \u2014 they work everywhere simultaneously, from rough shapes to fine detail.',
      explanationToggle: 'Show detailed explanation',
      explainWhatTitle: 'What does this tool show?',
      explainWhatText: 'A diffusion model creates an image by progressively removing noise. Unlike drawing from left to right, the model works on ALL image regions simultaneously. In the first steps, rough structures emerge: Where is up, where is down? Where is the horizon? In the middle steps, semantic content appears: objects, shapes, colors. The final steps refine textures and details. This tool makes every single step visible.',
      explainHowTitle: 'How do I use this tool?',
      explainHowText: 'Enter a prompt and click Generate. The model produces 25 intermediate images (one per denoising step). These appear as a filmstrip below. Click a thumbnail or use the timeline slider to view each step at full size. Compare early and late steps: When does the model "know" what it is drawing?',
      explainReadTitle: 'What do the three phases reveal?',
      explainReadText: 'Early steps (1\u20138): Global composition \u2014 basic structure, color distribution, layout planning. Middle steps (9\u201317): Semantic emergence \u2014 objects become recognizable, shapes crystallize. Late steps (18\u201325): Detail refinement \u2014 textures, edges, fine patterns. The transitions are gradual, but the phases clearly show: the model first "plans" globally, then refines locally. Particularly revealing: The very first step does not show fine-grained pixels, but colorful patches. This is because the noise is generated in latent space (128\u00d7128 at 16 channels), not in pixel space. The VAE translates each latent pixel into an ~8\u00d78 pixel patch \u2014 even pure Gaussian noise becomes coherent color clusters. The model never "thinks" in individual pixels, but always in this compressed space.',
      techTitle: 'Technical details',
      techText: 'SD3.5 Large uses Rectified Flow as scheduler with 25 default steps. At each step, the current latent vectors are decoded through the VAE (1024\u00d71024 JPEG). The VAE (Variational Autoencoder) translates the mathematical latent space into pixels. The latent representation is 128\u00d7128 at 16 channels \u2014 each latent pixel corresponds to an ~8\u00d78 pixel patch in the image. This is why even the first step shows colorful clusters instead of fine pixel noise: the VAE interprets random 16-dimensional vectors as coherent color patches.',
      referencesTitle: 'Research References',
      promptLabel: 'Prompt',
      promptPlaceholder: 'e.g. A marketplace in a medieval town with people, buildings and a fountain',
      generate: 'Generate',
      generating: 'Generating image \u2014 recording every step...',
      emptyHint: 'Enter a prompt and click Generate to visualize the denoising process.',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      stepsLabel: 'Steps',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      filmstripLabel: 'Denoising Filmstrip',
      timelineLabel: 'Step',
      phaseEarly: 'Composition',
      phaseMid: 'Semantics',
      phaseLate: 'Detail',
      phaseEarlyDesc: 'Global structure and color distribution emerge',
      phaseMidDesc: 'Objects and shapes become recognizable',
      phaseLateDesc: 'Textures and fine details are sharpened',
      finalImageLabel: 'Final image (full resolution)',
      timelineHint: 'Scrubs through denoising steps — shows how the image emerges from noise to final composition.',
      download: 'Download Image'
    },
    textLab: {
      headerTitle: 'Latent Text Lab \u2014 Scientific LLM Deconstruction',
      headerSubtitle: 'Representation Engineering, comparative model archaeology, and systematic bias analysis: Three research-based tools for investigating language models.',
      explanationToggle: 'Show explanation',
      modelPanel: {
        presetLabel: 'Preset',
        presetNone: 'No preset (custom ID)',
        customModelLabel: 'HuggingFace Model ID',
        customModelPlaceholder: 'e.g. meta-llama/Llama-3.2-1B',
        quantizationLabel: 'Quantization',
        quantAuto: 'Auto',
        quantizationHint: 'bf16 = full quality, int8 = half VRAM, int4 = minimal VRAM but lowest quality',
      },
      temperatureHint: 'Randomness of text generation. Low = deterministic, high = more creative.',
      maxTokensHint: 'Maximum number of generated tokens (word pieces).',
      textSeedHint: '-1 = random, fixed value = reproducible result',
      tabs: {
        repeng: { label: 'Representation Engineering', short: 'Find steering vectors in LLMs' },
        compare: { label: 'Model Comparison', short: 'Compare two LLMs layer by layer' },
        bias: { label: 'Bias Archaeology', short: 'Uncover hidden biases in LLMs' },
      },
      repeng: {
        title: 'Representation Engineering',
        subtitle: 'Find concept directions in activation space and steer generation',
        explainWhatTitle: 'What does this experiment show?',
        explainWhatText: 'Based on Zou et al. (2023) \u201cRepresentation Engineering\u201d and Li et al. (2024) \u201cInference-Time Intervention\u201d. LLMs encode abstract concepts (truth, sentiment, ethics) as directions in high-dimensional activation space. Through contrast pairs (true vs. false sentence), the direction encoding a concept can be extracted. Adding this direction at runtime changes model behavior in a targeted way \u2014 without retraining. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
        explainHowTitle: 'How do I use it?',
        explainHowText: 'This experiment extracts a \u201ctruth direction\u201d from the model. The pre-filled contrast pairs each contain a true and a false statement. From the differences in activations, the system computes a direction in high-dimensional space. When this direction is inverted (\u03b1 = -1), the model should generate a wrong answer for a factual prompt \u2014 even though it \u201cknows\u201d the correct one. Recommendation: English prompts work significantly better since most open-source LLMs were primarily trained on English data.',
        referencesTitle: 'Research References',
        expectedResults: 'Expected results: At \u03b1 = 0 (baseline), the model generates the correct answer. At \u03b1 = -1 (inversion), a wrong answer should appear \u2014 this is the core of the experiment. At \u03b1 = +1, little changes because the model already answers correctly. Beyond |\u03b1| > 2, artifacts dominate (repetitions, nonsense). The "sweet spot" according to Zou et al. is |\u03b1| between 0.5 and 2.0. Explained variance > 50% indicates clean separation \u2014 below that, contrast pairs are too similar or too few (at least 3 recommended).',
        pairsTitle: 'Contrast Pairs',
        pairsSubtitle: 'At least 3 pairs recommended. Each pair should differ only in the target concept (true vs. false). The examples are editable.',
        positiveLabel: 'Positive (true)',
        negativeLabel: 'Negative (false)',
        positivePlaceholder: 'e.g. The capital of France is Paris',
        negativePlaceholder: 'e.g. The capital of France is Berlin',
        addPair: 'Add pair',
        removePair: 'Remove',
        targetLayerLabel: 'Target layer',
        targetLayerHint: 'Which transformer layer receives the steering vector. Different layers influence different aspects of text generation.',
        targetLayerAuto: 'Last layer',
        findDirection: 'Find direction',
        finding: 'Computing concept direction...',
        directionFound: 'Concept direction found',
        varianceLabel: 'Explained variance',
        dimLabel: 'Dimensions',
        projectionsTitle: 'Contrast pair projections',
        testTitle: 'Test + Manipulation',
        testSubtitle: 'Enter a sentence and steer generation along the concept direction',
        testPromptLabel: 'Test prompt',
        testPromptPlaceholder: 'e.g. The capital of Germany is',
        alphaLabel: 'Manipulation strength (\u03b1)',
        alphaHint: 'Strength of the steering vector. 0 = no effect, higher = stronger influence of contrast pairs.',
        temperatureLabel: 'Temperature',
        maxTokensLabel: 'Max tokens',
        seedLabel: 'Seed (-1 = random)',
        generateBtn: 'Generate with manipulation',
        generating: 'Running manipulated generation...',
        baselineLabel: 'Baseline (no manipulation)',
        manipulatedLabel: 'Manipulated (\u03b1 = {alpha})',
        projectionLabel: 'Projection onto concept direction',
        interpretationTitle: 'Interpretation',
        interpreting: 'Analyzing results...',
        interpretationError: 'Could not generate interpretation'
      },
      compare: {
        title: 'Comparative Model Archaeology',
        subtitle: 'Load two models and systematically compare their internal representations',
        explainWhatTitle: 'What does this experiment show?',
        explainWhatText: 'Based on Belinkov (2022) \u201cProbing Classifiers\u201d and Olsson et al. (2022) \u201cIn-Context Learning and Induction Heads\u201d. The heatmap shows CKA (Centered Kernel Alignment) between layers of both models. High similarity means these layers represent information in a similar way. Early layers (syntax) are often similar \u2014 late layers (semantics) diverge more strongly. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
        explainHowTitle: 'How do I use it?',
        explainHowText: 'Model A is the active preset. Choose a second model (Model B) \u2014 via preset or custom HuggingFace ID \u2014 and load it. Enter text and click \u201cCompare\u201d. The CKA heatmap shows which layers represent information similarly. The generations from both models with the same seed show how differently they complete the same prompt.',
        referencesTitle: 'Research References',
        modelATitle: 'Model A (from preset selector)',
        modelAHint: 'Change via the preset dropdown above',
        modelBTitle: 'Model B (second model)',
        modelBPresetLabel: 'Preset',
        modelBCustomLabel: 'HuggingFace Model ID',
        modelBCustomPlaceholder: 'e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        modelBLoadBtn: 'Load Model B',
        modelBLoaded: 'Model B loaded',
        modelBNone: 'Model B not loaded',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. The cat sat on the mat and watched the birds',
        seedLabel: 'Seed',
        temperatureLabel: 'Temperature',
        maxTokensLabel: 'Max tokens',
        compareBtn: 'Compare',
        comparing: 'Comparing models...',
        heatmapTitle: 'Layer Alignment (CKA)',
        heatmapAxisA: 'Model A \u2014 Layers',
        heatmapAxisB: 'Model B \u2014 Layers',
        heatmapExplain: 'Bright cells = high representation similarity. Diagonal patterns show the models process information in a similar order.',
        attentionTitle: 'Attention Comparison (last layer)',
        modelALabel: 'Model A',
        modelBLabel: 'Model B',
        generationTitle: 'Generation Comparison (same seed)',
        layerStatsTitle: 'Layer Statistics',
        interpretationTitle: 'Interpretation',
        interpreting: 'Analyzing results...',
        interpretationError: 'Could not generate interpretation'
      },
      bias: {
        title: 'Bias Archaeology',
        subtitle: 'Systematic bias experiments through controlled token manipulation',
        explainWhatTitle: 'What does this experiment show?',
        explainWhatText: 'Based on Zou et al. (2023) \u201cRepresentation Engineering\u201d and Bricken et al. (2023) \u201cTowards Monosemanticity\u201d. Instead of free manipulation, this tool investigates systematic biases: What happens when all masculine pronouns are suppressed? Which gender does the model choose as default? \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
        explainHowTitle: 'How do I use it?',
        explainHowText: 'Choose an experiment type (gender, sentiment, domain, or custom tokens). Enter a prompt that invites the model to continue (e.g. \u201cThe doctor said to the patient\u201d). The results show baseline vs. manipulated generations \u2014 the differences reveal which biases are encoded in the model.',
        referencesTitle: 'Research References',
        presetLabel: 'Experiment type',
        presetGender: 'Gender \u2014 Suppress gendered pronouns',
        presetSentiment: 'Sentiment \u2014 Boost positive/negative',
        presetDomain: 'Domain \u2014 Boost scientific/poetic',
        presetCustom: 'Custom experiment',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. The doctor said to the patient',
        customBoostLabel: 'Boost tokens (comma-separated)',
        customBoostPlaceholder: 'e.g. dark,shadow,night',
        customSuppressLabel: 'Suppress tokens (comma-separated)',
        customSuppressPlaceholder: 'e.g. light,sun,bright',
        numSamplesLabel: 'Samples per condition',
        temperatureLabel: 'Temperature',
        maxTokensLabel: 'Max tokens',
        seedLabel: 'Base seed',
        runBtn: 'Run experiment',
        running: 'Running bias experiment...',
        baselineTitle: 'Baseline (no manipulation)',
        groupTitle: 'Group: {name}',
        modeSuppress: 'suppressed',
        modeBoost: 'boosted',
        tokensLabel: 'Tokens',
        sampleSeedLabel: 'Seed',
        genderDesc: 'Suppresses all gendered pronouns and observes which defaults the model chooses.',
        sentimentDesc: 'Boosts positive or negative words and measures how strongly the entire text flow is affected.',
        domainDesc: 'Boosts scientific or poetic vocabulary and observes register shifts.',
        interpretationTitle: 'Interpretation',
        interpreting: 'Analyzing results...',
        interpretationError: 'Could not generate interpretation'
      },
      error: {
        gpuUnreachable: 'GPU service unreachable. Is it running?',
        loadFailed: 'Failed to load model.',
        operationFailed: 'Operation failed.'
      }
    },
    crossmodal: {
      headerTitle: 'Crossmodal Lab',
      headerSubtitle: 'Sound from latent spaces: T5 embedding manipulation, image-guided audio generation, crossmodal transfer',
      explanationToggle: 'Show detailed explanation',
      generate: 'Generate',
      generating: 'Generating...',
      result: 'Result',
      seed: 'Seed',
      generationTime: 'Generation time',
      tabs: {
        synth: {
          label: 'Latent Audio Synth',
          short: 'T5 embedding manipulation',
          title: 'Latent Audio Synth',
          description: 'Direct manipulation of Stable Audio\'s T5 conditioning space (768d). Interpolate between prompts, extrapolate beyond the prompt, scale embeddings and inject noise. Ultra-short loops, near real-time.'
        },
        mmaudio: {
          label: 'MMAudio',
          short: 'Image/text to audio (CVPR 2025)',
          title: 'MMAudio — Video/Image to Audio',
          description: 'Image and text enter the same network as separate signals — the image is not translated into language, both guide sound generation simultaneously. The model was jointly trained on video and audio, learning direct associations between what is seen and what is heard. Up to 8s, 44.1kHz, ~1.2s compute time. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
        },
        guidance: {
          label: 'ImageBind Guidance',
          short: 'Gradient-based image guidance',
          title: 'ImageBind Gradient Guidance',
          description: 'Gradient-based steering during the Stable Audio denoising process. ImageBind provides a shared 1024d space for image and audio — the gradient of the cosine similarity guides audio generation towards the image embedding.'
        }
      },
      synth: {
        explainWhatTitle: 'What does the Latent Audio Synth do?',
        explainWhatText: 'Stable Audio generates sound from text. The text is converted by a T5 encoder into a numerical vector with 768 dimensions \u2014 this vector is what you manipulate here. Instead of just changing \u201cwhat\u201d the model generates (via the prompt), you change \u201chow\u201d the model internally understands the text. Two prompts that sound similar can be far apart in this space \u2014 and vice versa.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'Enter text in Prompt A \u2014 this determines the base sound. Optional: Prompt B as a target point. The Alpha slider controls the mix: at 0 you hear only A, at 1 only B, at 0.5 a blend. Values above 1 extrapolate beyond B (the sound becomes more extreme), values below 0 go in the opposite direction. Magnitude scales the entire embedding \u2014 higher values produce more intense sounds. Noise injects randomness and creates unpredictable variations. The Spectral Strip (below the Generate button) shows all 768 dimensions as bars. You can shift individual dimensions by clicking and dragging, directly manipulating the sound. Right-click resets a dimension.',
        promptA: 'Prompt A (Base)',
        promptAPlaceholder: 'e.g. ocean waves',
        promptB: 'Prompt B (Optional, for interpolation)',
        promptBPlaceholder: 'e.g. piano melody',
        alpha: 'Alpha (Interpolation)',
        alphaHint: '0 = A only, 1 = B only, between = blend, >1 or <0 = extrapolation',
        magnitude: 'Magnitude (Scaling)',
        magnitudeHint: 'Global embedding scaling (1.0 = unchanged)',
        noise: 'Noise',
        noiseHint: 'Gaussian noise on embedding (0 = no noise)',
        duration: 'Duration (s)',
        steps: 'Steps',
        cfg: 'CFG',
        durationHint: 'Length of the generated audio clip in seconds',
        stepsHint: 'Denoising steps. More = higher quality.',
        cfgHint: 'Classifier-Free Guidance for audio generation',
        seedHint: '-1 = random, fixed value = reproducible result',
        loop: 'Loop playback',
        loopOn: 'Loop On',
        loopOff: 'Loop Off',
        stop: 'Stop',
        looping: 'Looping',
        playing: 'Playing',
        stopped: 'Stopped',
        transpose: 'Transpose (semitones)',
        midiSection: 'MIDI Control',
        midiUnsupported: 'Web MIDI is not supported by this browser.',
        midiInput: 'MIDI Input',
        midiNone: '(none)',
        midiMappings: 'CC Mappings',
        midiNoteC3: 'Note (C3 = Ref)',
        midiGenerate: 'Generate + Transpose',
        midiPitch: 'Pitch rel. C3',
        loopInterval: 'Loop interval',
        loopOptimize: 'Auto-optimize',
        loopPingPong: 'Ping-pong',
        loopIntervalHint: 'Start/end of loop region — shorten end to trim Stable Audio fade-out',
        modeLoop: 'Loop',
        modePingPong: 'Ping-Pong',
        modeWavetable: 'Wavetable',
        modeRate: 'Tempo (fast)',
        modePitch: 'Pitch (OLA)',
        wavetableScan: 'Scan Position',
        wavetableScanHint: 'Morph between frames (0 = start, 1 = end)',
        wavetableFrames: '{count} frames',
        midiScan: 'Scan Position',
        adsrTitle: 'ADSR Envelope',
        adsrAttack: 'A',
        adsrDecay: 'D',
        adsrSustain: 'S',
        adsrRelease: 'R',
        adsrHint: 'Envelope for MIDI notes (Attack/Decay/Sustain/Release)',
        play: 'Play',
        normalize: 'Normalize loudness',
        peak: 'Peak',
        crossfade: 'Crossfade',
        transposeHint: 'Shifts pitch in semitones',
        crossfadeHint: 'Crossfade time at loop boundary (ms)',
        normalizeHint: 'Normalizes volume to maximum amplitude',
        saveRaw: 'Save raw',
        saveLoop: 'Save loop',
        embeddingStats: 'Embedding statistics',
        dimensions: {
          section: 'Dimension Explorer',
          hint: 'Drag on bars = set offset. Paint horizontally = multiple dimensions.',
          resetAll: 'Reset all',
          hoverActivation: 'Activation',
          hoverOffset: 'Offset',
          rightClickReset: 'Right-click = reset',
          sortDiff: 'Sorted by prompt difference',
          sortMagnitude: 'Sorted by activation',
          activeOffsets: '{count} offsets active',
          applyAndGenerate: 'Apply and regenerate',
          undo: 'Undo',
          redo: 'Redo'
        }
      },
      mmaudio: {
        explainWhatTitle: 'What does MMAudio do?',
        explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) was jointly trained on video and audio. It doesn\'t translate an image into text and then into sound, but processes image and text as parallel signals in the same network. The model learned which sounds belong to which visual scenes \u2014 a forest produces birdsong, a street produces traffic noise, a guitar produces plucking sounds.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'Upload an image and/or enter a text prompt \u2014 using both together yields the richest results. The image alone generates sounds matching the visual content. The text prompt can additionally steer the sound or be used alone without an image. In the Negative Prompt, describe sounds you do NOT want to hear (e.g. \u201cspeech, music\u201d). Duration sets the length (1\u20138 seconds). CFG Strength controls how strictly the model follows the prompt \u2014 low values (2\u20133) produce more varied results, higher values (6\u20138) are more prompt-faithful.',
        imageUpload: 'Upload image (optional)',
        prompt: 'Text prompt (optional)',
        promptPlaceholder: 'e.g. crackling campfire',
        negativePrompt: 'Negative prompt',
        duration: 'Duration (s)',
        maxDuration: 'Max 8s (model limit)',
        cfg: 'CFG',
        steps: 'Steps',
        compareHint: 'Compare: Text only vs. Image + Text'
      },
      guidance: {
        explainWhatTitle: 'What does ImageBind Guidance do?',
        explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) brings six senses \u2014 image, sound, text, depth, heat, motion \u2014 into a shared \u201clanguage\u201d. This tool uses that common ground: as the sound is generated step by step, it constantly asks \u201cDoes this sound like the image yet?\u201d and corrects the direction. The cosine similarity in the result shows how close the generated sound came to the image content.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'Upload an image \u2014 this is the target direction for the sound. Optional: a text prompt for additional steering. The \u201c\u03bb Guidance Strength\u201d slider is the most important parameter: low values (0.01\u20130.05) give the sound a lot of freedom, high values (0.3\u20131.0) tie it closely to the image. \u201cWarmup Steps\u201d determines from which step the image guidance kicks in \u2014 low values start immediately, higher values let the basic structure emerge unguided first. Total Steps and Duration control quality and length.',
        referencesTitle: 'Research References',
        imageUpload: 'Upload image',
        prompt: 'Base prompt (optional)',
        promptPlaceholder: 'e.g. ambient soundscape',
        lambda: 'Guidance strength',
        lambdaHint: 'How strongly the image steers audio generation',
        warmupSteps: 'Warmup steps',
        warmupHint: 'Gradient guidance only during first N steps',
        totalSteps: 'Total steps',
        duration: 'Duration (s)',
        cfg: 'CFG',
        totalStepsHint: 'Total denoising steps. More = higher quality.',
        durationHint: 'Length of the generated audio clip in seconds',
        cosineSimilarity: 'Cosine similarity (image-audio proximity)'
      }
    }
  },
  edutainment: {
    ui: {
      didYouKnow: '🤔 Did you know?',
      learnMore: '📚 Learn more',
      currentlyHappening: '⚡ Currently happening:',
      energyUsed: 'Energy used',
      co2Produced: 'CO₂ produced'
    },
    denoising: {
      modelLoading: 'Loading model into GPU memory...',
      modelCard: 'Model Profile',
      publisher: 'Publisher',
      architecture: 'Architecture',
      parameters: 'Parameters',
      textEncoders: 'Text Encoders',
      quantization: 'Quantization',
      vramRequired: 'VRAM Required',
      resolution: 'Resolution',
      license: 'License',
      fairCulture: 'Fair Culture',
      safetyByDesign: 'Safety by Design',
      denoisingActive: 'Denoising in progress...',
    },
    energy: {
      kids_1: '💡 AI images need electricity – as much as charging your phone for 3 hours!',
      kids_2: '🔌 The GPU is like a super calculator that eats lots of power!',
      kids_3: '⚡ Each image needs as much energy as running an LED light for 10 minutes!',
      youth_1: '⚡ A GPU uses {watts}W while generating – like a small space heater!',
      youth_2: '🔋 One image uses about 0.01-0.02 kWh – sounds little, but adds up!',
      youth_3: '🌡️ The GPU is getting {temp}°C hot right now – that\'s why it needs cooling!',
      expert_1: '📊 Realtime: {watts}W at {util}% utilization = {kwh} kWh so far',
      expert_2: '🔥 TDP limit: {tdp}W | Current: {watts}W ({percent}% of limit)',
      expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – model + activations'
    },
    data: {
      kids_1: '🧮 The GPU is calculating 10 billion times right now – faster than you can count!',
      kids_2: '🎨 The image is created in 50 small steps – like a puzzle solving itself!',
      kids_3: '🧩 Millions of numbers are flying through the GPU right now!',
      youth_1: '🔄 Each image goes through ~50 "denoising steps" – 50 rounds of removing noise!',
      youth_2: '📐 8 billion parameters are being queried – per image!',
      youth_3: '🧠 The AI "thinks" in vectors with thousands of dimensions – like coordinates in a space.',
      expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – text + image in joint attention layers',
      expert_2: '📈 Self-Attention: O(n²) complexity – every token "sees" all others',
      expert_3: '⚙️ Classifier-Free Guidance: prompt influence vs. creativity balance'
    },
    model: {
      kids_1: '🎓 The AI model looked at millions of images to learn how to paint!',
      kids_2: '🤖 The AI is like an artist who never forgets what they\'ve seen!',
      kids_3: '✨ 8 billion connections in the model – more than stars you can see in the sky!',
      youth_1: '🧠 SD3.5 Large has 8 billion parameters – like 8 billion decision nodes.',
      youth_2: '📚 3 text encoders work together: CLIP-L, CLIP-G, and T5-XXL',
      youth_3: '🔢 The model needs {vram} GB VRAM just to be loaded!',
      expert_1: '🏗️ Architecture: Rectified Flow + MMDiT with 38 transformer blocks',
      expert_2: '📊 FP16/FP8 quantization: precision vs. VRAM trade-off',
      expert_3: '🔗 LoRA: Low-Rank Adaptation – only 0.1% of parameters retrained'
    },
    ethics: {
      kids_1: '🌍 AI learns from images on the internet – that\'s why it\'s important to be fair with other people\'s art!',
      kids_2: '⚖️ Not all artists were asked if the AI could learn from them.',
      kids_3: '🤝 Good AI respects people\'s work!',
      youth_1: '📜 Training data often comes from the internet. Artists debate: Fair Use or copying?',
      youth_2: '🏛️ The EU AI Act demands transparency: Where does the training data come from?',
      youth_3: '💭 Question: Who actually owns an AI-generated image?',
      expert_1: '⚠️ LAION-5B was partly created without creator consent – legal gray area.',
      expert_2: '📋 EU AI Act Art. 52: Labeling requirement for AI-generated content',
      expert_3: '🔍 Model Cards & Datasheets: Best practice for ML transparency'
    },
    environment: {
      kids_1: '☁️ Each AI image produces a bit of CO₂ – like driving a car, but less!',
      kids_2: '🌱 Think: Is this image worth the electricity?',
      kids_3: '🌞 Energy for AI often comes from power plants – some clean, some not.',
      youth_1: '🏭 German power grid: ~400g CO₂ per kWh – that adds up!',
      youth_2: '📈 {co2}g CO₂ for this image – with 1000 images that would be {totalKg} kg!',
      youth_3: '💡 Tip: Generate fewer images, but more thoughtfully – saves energy and CO₂.',
      expert_1: '📊 Calculation: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
      expert_2: '🔬 Scope 2 emissions: data center location is decisive',
      expert_3: '⚡ PUE (Power Usage Effectiveness): Additional energy overhead for cooling'
    },
    iceberg: {
      drawPrompt: 'AI generation uses a lot of energy. Draw icebergs and see what happens...',
      redraw: 'Redraw',
      startMelting: 'Start melting',
      melting: 'Iceberg melting...',
      melted: 'Melted!',
      meltedMessage: '{co2}g CO₂ produced',
      comparison: 'This CO₂ amount melts about {volume} cm³ of Arctic ice.',
      comparisonInfo: '(Each ton of CO₂ = approx. 6m³ sea ice loss)',
      gpuPower: 'Graphics card power consumption',
      gpuTemp: 'Graphics card temperature',
      co2Info: 'CO₂ emissions from power consumption (based on German energy mix)',
      drawAgain: 'Draw more icebergs...'
    },
    pixel: {
      grafikkarte: 'Graphics Card',
      energieverbrauch: 'Energy Usage',
      co2Menge: 'CO₂ Amount',
      smartphoneComparison: 'You would need to keep your phone off for {minutes} minutes to offset this CO₂ usage!',
      clickToProcess: 'Click on the data pixels to generate a mini image!'
    },
    forest: {
      trees: 'Trees',
      clickToPlant: 'Click to plant trees! Where you plant a tree, the factory will disappear.',
      gameOver: 'The forest is lost!',
      treesPlanted: 'You planted {count} trees.',
      complete: 'Generation complete',
      comparison: 'An average tree needs {minutes} minutes to absorb this amount of CO₂.'
    },
    rareearth: {
      clickToClean: 'Click the lake to remove toxic sludge!',
      sludgeRemoved: 'Sludge removed',
      environmentHealth: 'Environment',
      gameOverInactive: 'You gave up... mining continues',
      infoBanner: 'Rare earth mining for GPU chips leaves toxic sludge and destroys ecosystems. Your cleanup efforts cannot match the speed of extraction.',
      instructionsCooldown: '⏳ {seconds}s',
      statsGpu: 'GPU',
      statsHealth: 'Environment',
      statsSludge: 'Sludge removed'
    }
  }
}
