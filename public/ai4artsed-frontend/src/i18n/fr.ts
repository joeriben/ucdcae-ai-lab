export const fr = {
  app: {
    title: 'UCDCAE AI LAB',
    subtitle: 'Transformations créatives par IA'
  },
  form: {
    inputLabel: 'Votre texte',
    inputPlaceholder: 'p. ex. Une fleur dans la prairie',
    schemaLabel: 'Style de transformation',
    executeModeLabel: 'Mode d\'exécution',
    safetyLabel: 'Niveau de sécurité',
    generateButton: 'Générer'
  },
  schemas: {
    dada: 'Dada (Aléatoire & Absurde)',
    bauhaus: 'Bauhaus (Géométrique)',
    stillepost: 'Téléphone arabe (Itératif)'
  },
  safetyLevels: {
    kids: 'Enfants',
    youth: 'Jeunes',
    adult: 'Adultes',
    research: 'Recherche'
  },
  stages: {
    pipeline_starting: 'Démarrage du pipeline',
    translation_and_safety: 'Traduction & Sécurité',
    interception: 'Transformation',
    pre_output_safety: 'Sécurité de sortie',
    media_generation: 'Génération d\'image',
    completed: 'Terminé'
  },
  status: {
    idle: 'Prêt',
    executing: 'Pipeline en cours...',
    connectionSlow: 'Connexion lente, nouvelle tentative...',
    completed: 'Pipeline terminé !',
    error: 'Erreur survenue'
  },
  entities: {
    input: 'Entrée',
    translation: 'Traduction',
    safety: 'Vérification de sécurité',
    interception: 'Transformation',
    safety_pre_output: 'Sécurité de sortie',
    media: 'Image générée'
  },
  properties: {
    chill: 'tranquille',
    chaotic: 'sauvage',
    narrative: 'raconter des histoires',
    algorithmic: 'suivre des règles',
    historical: 'histoire',
    contemporary: 'présent',
    explore: 'tester l\'IA',
    create: 'créer de l\'art',
    playful: 'ludique',
    serious: 'sérieux'
  },
  phase2: {
    title: 'Saisie du prompt',
    userInput: 'Votre saisie',
    yourInput: 'Votre saisie',
    yourIdea: 'Votre idée : DE QUOI cela devrait-il parler ?',
    rules: 'Vos règles : COMMENT votre idée doit-elle être réalisée ?',
    yourInstructions: 'Vos instructions',
    what: 'QUOI',
    how: 'COMMENT',
    userInputPlaceholder: 'p. ex. Une fleur dans la prairie',
    inputPlaceholder: 'Votre texte apparaît ici...',
    metaPrompt: 'Instruction artistique',
    instruction: 'Instruction',
    transformation: 'Transformation artistique',
    metaPromptPlaceholder: 'Décrivez la transformation...',
    result: 'Résultat',
    expectedResult: 'Résultat attendu',
    execute: 'Exécuter le pipeline',
    executing: 'En cours...',
    transforming: 'Transformation LLM en cours...',
    startTransformation: 'Lancer la transformation',
    letsGo: 'C\'est parti !',
    modified: 'Modifié',
    reset: 'Réinitialiser',
    loadingConfig: 'Chargement de la configuration...',
    loadingMetaPrompt: 'Chargement du méta-prompt...',
    errorLoadingConfig: 'Erreur lors du chargement de la configuration',
    errorLoadingMetaPrompt: 'Erreur lors du chargement du méta-prompt',
    threeForces: '3 forces en action',
    twoForces: 'QUOI + COMMENT → LLM → Résultat',
    yourPrompt: 'Votre prompt :',
    writeYourText: 'Écrivez votre texte...',
    examples: 'Exemples',
    estimatedTime: '~12 secondes',
    stage12Time: '~5-10 secondes',
    willAppearAfterExecution: 'Apparaîtra après l\'exécution...',
    back: 'Retour',
    retry: 'Réessayer',
    transformedPrompt: 'Prompt transformé',
    notYetTransformed: 'Pas encore transformé...',
    transform: 'Transformer',
    reTransform: 'Réessayer autrement',
    startAI: 'IA, traite ma saisie',
    aiWorking: 'L\'IA travaille...',
    continueToMedia: 'Continuer vers la génération d\'image',
    readyForMedia: 'Prêt pour la génération d\'image',
    stage1: 'Étape 1 : Traduction + Sécurité...',
    stage2: 'Étape 2 : Transformation...',
    selectMedia: 'Choisissez votre média :',
    mediaImage: 'Image',
    mediaAudio: 'Audio',
    mediaVideo: 'Vidéo',
    media3D: '3D',
    comingSoon: 'Bientôt disponible',
    generateMedia: 'Démarrer !'
  },
  phase3: {
    generating: 'L\'image est en cours de génération...',
    generatingHint: '~30 secondes'
  },
  common: {
    back: 'Retour',
    loading: 'Chargement...',
    error: 'Erreur',
    retry: 'Réessayer',
    cancel: 'Annuler',
    checkingSafety: 'Vérification...'
  },
  gallery: {
    title: 'Favoris',
    empty: 'Pas encore de favoris',
    favorite: 'Ajouter aux favoris',
    unfavorite: 'Retirer des favoris',
    continue: 'Continuer l\'édition',
    restore: 'Restaurer la session',
    viewMine: 'Mes favoris',
    viewAll: 'Tous les favoris'
  },
  settings: {
    authRequired: 'Authentification requise',
    authPrompt: 'Veuillez entrer le mot de passe pour accéder aux paramètres :',
    passwordPlaceholder: 'Entrer le mot de passe...',
    authenticate: 'Se connecter',
    authenticating: 'Authentification...',
    title: 'Administration',
    tabs: {
      export: 'Données de recherche',
      config: 'Configuration',
      demos: 'Démo minijeu',
      matrix: 'Matrice de modèles'
    },
    loading: 'Chargement des paramètres...',
    presets: {
      title: 'Préréglages de modèles',
      help: 'Utilisez l\'onglet <strong>Matrice de modèles</strong> pour voir tous les préréglages disponibles et les appliquer en un clic.',
      openMatrix: 'Ouvrir la matrice de modèles'
    },
    testingTools: {
      title: 'Outils de test pour les enseignants',
      help: 'Testez et explorez les mini-jeux et animations pédagogiques avant de les utiliser avec les apprenants.',
      openPreview: 'Ouvrir l\'aperçu des mini-jeux',
      pixelEditor: 'Éditeur de modèles pixel',
      includes: 'Inclut : Animation pixel, Fonte des icebergs, Jeu de la forêt, Terres rares'
    },
    general: {
      title: 'Configuration générale',
      uiMode: 'Mode d\'interface',
      uiModeHelp: 'Niveau de complexité de l\'interface',
      kids: 'Enfants (8–12)',
      youth: 'Jeunes (13–17)',
      expert: 'Expert',
      safetyLevel: 'Niveau de sécurité',
      defaultLanguage: 'Langue par défaut',
      germanDe: 'Allemand (de)',
      englishEn: 'Anglais (en)',
      turkishTr: 'Turc (tr)',
      koreanKo: '한국어 (ko)',
      ukrainianUk: 'Українська (uk)',
      frenchFr: 'Français (fr)'
    },
    safety: {
      kidsTitle: 'Enfants (8–12)',
      kidsDesc: 'Tous les filtres actifs : §86a, RGPD, Protection de la jeunesse (paramètres adaptés à l\'âge), Vérification VLM des images',
      youthTitle: 'Jeunes (13–17)',
      youthDesc: 'Tous les filtres actifs : §86a, RGPD, Protection de la jeunesse (paramètres jeunesse), Vérification VLM des images',
      adultTitle: 'Adultes',
      adultDesc: '§86a + RGPD actifs. Pas de protection de la jeunesse, pas de vérification VLM des images.',
      researchTitle: 'Mode recherche',
      researchDesc: 'AUCUN filtre de sécurité actif. Uniquement autorisé pour les institutions de recherche dans le cadre de projets de recherche scientifique.'
    },
    safetyModels: {
      title: 'Modèles de sécurité locaux',
      help: 'Locaux via Ollama — les noms de personnes et vérifications de sécurité ne quittent jamais le système',
      safetyModel: 'Modèle de sécurité',
      safetyModelHelp: 'Modèle garde pour la sécurité du contenu (§86a, protection de la jeunesse)',
      dsgvoModel: 'Modèle de vérification RGPD',
      dsgvoModelHelp: 'Modèle généraliste pour la vérification NER RGPD (pas un modèle garde)',
      vlmModel: 'Modèle VLM de sécurité',
      vlmModelHelp: 'Modèle de vision pour la vérification de sécurité des images après génération (enfants/jeunes)',
      fast: 'rapide, minimal',
      recommended: 'recommandé'
    },
    dsgvo: {
      title: 'Avertissement RGPD',
      notCompliant: 'Les modèles suivants ne sont <strong>PAS conformes au RGPD</strong> (données traitées hors UE) :',
      compliantHint: 'Options conformes au RGPD :'
    },
    models: {
      title: 'Configuration des modèles',
      help: 'Identifiants de modèles avec préfixe de fournisseur : local/, mistral/, anthropic/, openai/, openrouter/',
      matrixAdvised: 'L\'utilisation de la matrice de modèles est recommandée. Vous pouvez toutefois configurer vos paramètres librement ici.',
      ollamaAvailable: '{count} modèles Ollama disponibles (saisissez ou sélectionnez dans la liste)',
      stage1Text: 'Étape 1 - Modèle texte',
      stage1Vision: 'Étape 1 - Modèle vision',
      stage2Interception: 'Étape 2 - Modèle d\'interception',
      stage2Optimization: 'Étape 2 - Modèle d\'optimisation',
      stage3: 'Étape 3 - Modèle traduction/sécurité',
      stage4Legacy: 'Étape 4 - Modèle hérité',
      chatHelper: 'Modèle assistant de chat',
      imageAnalysis: 'Modèle d\'analyse d\'image',
      coding: 'Génération de code (Tone.js, p5.js)'
    },
    api: {
      title: 'Configuration API',
      llmProvider: 'Fournisseur LLM',
      localFramework: 'Framework LLM local',
      externalProvider: 'Fournisseur LLM externe',
      cloudProvider: 'Fournisseur LLM cloud — nécessite une clé API',
      noneLocal: 'Aucun (local uniquement, RGPD)',
      mistralEu: 'Mistral AI (basé en UE, RGPD)',
      anthropicDirect: 'API Anthropic directe (NON conforme RGPD)',
      openaiDirect: 'API OpenAI directe (NON conforme RGPD)',
      openrouterDirect: 'OpenRouter (NON conforme RGPD, routage UE disponible)',
      mistralInfo: 'Mistral AI (basé en UE)',
      mistralDsgvo: 'Conforme au RGPD (infrastructure UE)',
      anthropicInfo: 'API Anthropic directe',
      anthropicNotDsgvo: 'NON conforme au RGPD',
      anthropicWarning: 'Données traitées hors UE. À utiliser uniquement dans des contextes non éducatifs.',
      openaiInfo: 'API OpenAI directe',
      openaiNotDsgvo: 'NON conforme au RGPD (basé aux États-Unis)',
      openaiWarning: 'Données traitées aux États-Unis. À utiliser uniquement dans des contextes non éducatifs.',
      openrouterInfo: 'OpenRouter',
      openrouterNotDsgvo: 'NON conforme au RGPD (entreprise américaine)',
      openrouterWarning: 'Routage serveur UE configurable dans les paramètres OpenRouter, mais l\'entreprise est basée aux États-Unis.',
      storedIn: 'Stocké dans',
      currentKey: 'Actuelle'
    },
    save: {
      saveApply: 'Enregistrer et appliquer',
      saving: 'Enregistrement...',
      applying: 'Application...',
      success: 'Paramètres enregistrés et appliqués',
      presetApplied: 'Préréglage appliqué : {preset}'
    }
  },
  pipeline: {
    yourInput: 'Votre saisie',
    result: 'Résultat',
    generatedMedia: 'Image générée'
  },
  landing: {
    subtitlePrefix: 'Plateforme d\'expérimentation pédagogique-artistique de la',
    subtitleSuffix: 'pour l\'utilisation exploratoire de l\'IA générative dans l\'éducation médiatique culturelle-esthétique',
    research: '',
    features: {
      textTransformation: {
        title: 'Transformation de texte',
        description: 'Changement de perspective par l\'IA — votre prompt est transformé à travers des prismes artistiques-pédagogiques en image, vidéo et son.'
      },
      imageTransformation: {
        title: 'Transformation d\'image',
        description: 'Transformez des images à travers différents modèles et perspectives en nouvelles images et vidéos.'
      },
      multiImage: {
        title: 'Fusion d\'images',
        description: 'Combinez plusieurs images et fusionnez-les en nouvelles compositions visuelles grâce aux modèles d\'IA.'
      },
      canvas: {
        title: 'Canvas Workflow',
        description: 'Composition visuelle de workflows — connectez des modules par glisser-déposer en pipelines IA personnalisés.'
      },
      music: {
        title: 'Génération musicale',
        description: 'Création musicale par IA avec paroles, tags et contrôle stylistique.'
      },
      latentLab: {
        title: 'Latent Lab',
        description: 'Recherche en espace vectoriel — surréalisation, élimination de dimensions, interpolation d\'embeddings.'
      }
    }
  },
  research: {
    locked: 'Uniquement disponible en mode recherche',
    lockedHint: 'Nécessite le niveau de sécurité « Adulte » ou « Recherche » (config.py)',
    complianceTitle: 'Avis mode recherche',
    complianceWarning: 'En mode recherche, aucun filtre de sécurité n\'est actif pour les prompts ou les images générées. Des résultats inattendus ou inappropriés peuvent survenir.',
    complianceAge: 'Ce mode n\'est pas recommandé pour les personnes de moins de 16 ans.',
    complianceConfirm: 'Je confirme avoir compris les avis',
    complianceCancel: 'Annuler',
    complianceProceed: 'Continuer'
  },
  presetOverlay: {
    title: 'Choisir une perspective',
    close: 'Fermer'
  },
  imageUpload: {
    clickHere: 'Cliquez ici',
    orDragImage: 'ou glissez une image ici',
    formatHint: 'PNG, JPG, WEBP (max 10 Mo)',
    invalidFormat: 'Format de fichier invalide. Seuls PNG, JPG et WEBP sont acceptés.',
    fileTooLarge: 'Fichier trop volumineux. Maximum : {max} Mo',
    uploadFailed: 'Échec du téléchargement',
    infoOriginal: 'Original :',
    infoSize: 'Taille :'
  },
  mediaInput: {
    choosePreset: 'Choisir une perspective',
    translateToEnglish: 'Traduire en anglais',
    copy: 'Copier',
    paste: 'Coller',
    delete: 'Supprimer',
    loading: 'Chargement...',
    contentBlocked: 'Contenu bloqué'
  },
  nav: {
    about: 'À propos',
    impressum: 'Mentions légales',
    privacy: 'Confidentialité',
    docs: 'Documentation',
    language: 'Changer de langue',
    settings: 'Paramètres',
    canvas: 'Canvas Workflow'
  },
  canvas: {
    title: 'Canvas Workflow',
    newWorkflow: 'Nouveau workflow',
    importWorkflow: 'Importer',
    exportWorkflow: 'Exporter',
    execute: 'Exécuter',
    ready: 'Prêt',
    errors: 'erreurs',
    discardWorkflow: 'Abandonner le workflow actuel ?',
    importError: 'Échec de l\'importation du fichier',
    selectTransformation: 'Sélectionner une transformation',
    selectOutput: 'Sélectionner un modèle de sortie',
    search: 'Rechercher...',
    noResults: 'Aucun résultat trouvé',
    dragHint: 'Cliquez ou glissez des modules sur le canvas',
    editNameHint: '(double-cliquez pour modifier)',
    modules: 'Modules',
    toggleSidebar: 'Basculer la barre latérale',
    dsgvoTooltip: 'Les workflows Canvas peuvent utiliser des API LLM externes. La conformité au RGPD relève de la responsabilité de l\'utilisateur.',
    batchExecute: 'Exécution par lot',
    batchExecution: 'Exécution par lot',
    batchAbort: 'Interrompre le lot',
    abort: 'Interrompre',
    cancel: 'Annuler',
    loading: 'Chargement...',
    executingWorkflow: 'Exécution du workflow...',
    starting: 'Démarrage...',
    nodes: 'nœuds',
    batchRunCount: 'Nombre d\'exécutions',
    batchUseSeed: 'Utiliser un seed de base',
    batchBaseSeed: 'Seed de base',
    batchSeedHint: 'Chaque exécution : seed + index',
    batchStart: 'Lancer le lot',
    stage: {
      configSelectPlaceholder: 'Sélectionner...',
      evaluationCriteriaFallback: 'Critères d\'évaluation...',
      feedbackInputTitle: 'Entrée de retour',
      deleteTitle: 'Supprimer',
      selectLlmPlaceholder: 'Sélectionner un LLM...',
      resizeTitle: 'Redimensionner',
      input: {
        promptPlaceholder: 'Votre prompt...'
      },
      imageInput: {
        uploadLabel: 'Télécharger une image'
      },
      interception: {
        contextPromptLabel: 'Prompt de contexte',
        contextPromptPlaceholder: 'Instructions de transformation...'
      },
      translation: {
        translationPromptLabel: 'Prompt de traduction',
        translationPromptPlaceholder: 'Instructions de traduction...'
      },
      modelAdaption: {
        targetModelLabel: 'Modèle cible',
        noAdaptionOption: 'Pas d\'adaptation',
        videoModelsOption: 'Modèles vidéo',
        audioModelsOption: 'Modèles audio'
      },
      comparisonEvaluator: {
        criteriaLabel: 'Critères de comparaison',
        criteriaPlaceholder: 'p. ex. Comparer par originalité, clarté, détail...',
        infoText: 'Connectez jusqu\'à 3 sorties texte'
      },
      seed: {
        modeLabel: 'Mode',
        modeFixed: 'Fixe',
        modeRandom: 'Aléatoire',
        valueLabel: 'Valeur',
        baseLabel: 'Base'
      },
      resolution: {
        customOption: 'Personnalisé',
        widthLabel: 'Largeur',
        heightLabel: 'Hauteur'
      },
      collector: {
        emptyText: 'En attente de l\'exécution...'
      },
      evaluation: {
        typeLabel: 'Type d\'évaluation',
        typeCreativity: 'Créativité',
        typeQuality: 'Qualité',
        typeCustom: 'Personnalisé',
        criteriaLabel: 'Critères d\'évaluation',
        outputTypeLabel: 'Type de sortie',
        outputCommentary: 'Commentaire + Binaire',
        outputScore: 'Commentaire + Score + Binaire',
        outputAll: 'Tout',
        evalPassTitle: 'Réussi (en avant)',
        evalFailTitle: 'Retour (en arrière)',
        evalCommentaryTitle: 'Commentaire (en avant)'
      },
      imageEvaluation: {
        visionModelPlaceholder: 'Sélectionner un modèle de vision...',
        frameworkLabel: 'Cadre d\'analyse',
        frameworkPanofsky: 'Histoire de l\'art (Panofsky)',
        frameworkEducational: 'Théorie éducative',
        frameworkEthical: 'Éthique',
        frameworkCritical: 'Critique/Décolonial',
        frameworkCustom: 'Personnalisé',
        customPromptLabel: 'Prompt d\'analyse',
        customPromptPlaceholder: 'Décrivez comment l\'image doit être analysée...'
      },
      display: {
        imageAlt: 'Aperçu',
        emptyText: 'Aperçu (après exécution)'
      }
    }
  },
  about: {
    title: 'À propos du UCDCAE AI LAB',
    intro: 'Le UCDCAE AI LAB est une plateforme d\'expérimentation pédagogique-artistique de la Chaire UNESCO en Culture numérique et Arts dans l\'Éducation pour l\'utilisation exploratoire de l\'intelligence artificielle générative dans l\'éducation médiatique culturelle-esthétique. Elle a été développée dans le cadre des projets AI4ArtsEd et COMeARTS.',
    project: {
      title: 'Le projet',
      description: 'L\'IA transforme la société et le monde du travail ; elle devient de plus en plus un sujet d\'éducation. Le projet explore les opportunités, les conditions et les limites de l\'utilisation pédagogique de l\'intelligence artificielle (IA) dans des contextes d\'éducation culturelle sensibles à la diversité.',
      paragraph2: 'Dans trois sous-projets — Pédagogie générale (TPap), Informatique (TPinf) et Éducation artistique (TPkp) — la recherche pratique pédagogique orientée créativité et la conception et programmation informatique de l\'IA s\'entrelacent en étroite coopération. Dès le départ, le projet implique systématiquement des praticiens artistiques-pédagogiques dans le processus de conception ; il fait le pont entre la mise en œuvre pédagogique-pratique professionnelle (liée à la qualité, esthétique, éthique et aux valeurs) d\'une part et le processus de mise en œuvre et de formation du sous-projet informatique d\'autre part.',
      paragraph3: 'Un processus de conception participatif d\'environ deux ans vise à produire une technologie IA open source qui explore dans quelle mesure les systèmes d\'IA peuvent déjà intégrer des principes artistiques-pédagogiques à leur niveau structurel dans des conditions réelles favorables.',
      paragraph4: 'L\'accent est mis sur a) l\'applicabilité future et la valeur ajoutée des technologies hautement innovantes pour l\'éducation culturelle, b) la portée et les limites de la littératie IA chez les enseignants et les apprenants, et c) la question transversale de l\'évaluabilité et de l\'évaluation de la transformation des cadres pédagogiques par des acteurs non humains complexes en termes d\'éthique pédagogique et d\'évaluation technologique.',
      moreInfo: 'Plus d\'informations :'
    },
    subproject: {
      title: 'Sous-projet « Pédagogie générale »',
      description: 'Le sous-projet « Pédagogie générale » étudie les possibilités et les limites d\'un processus de conception IA artistique-pédagogique basé sur la recherche-action participative dans le cadre de la question de recherche conjointe du projet collaboratif. À cette fin, il mène une série de recherches, analyses, ateliers d\'experts et espaces ouverts au cours de la première année de projet. La phase suivante du projet, conçue comme une boucle de rétroaction en plusieurs cycles, explore l\'utilisation d\'un prototype avec des praticiens pédagogiques et des artistes-éducateurs, en particulier dans l\'éducation culturelle non formelle, comme processus éducatif transformatif relationnel et collectif.'
    },
    team: {
      title: 'Équipe',
      projectLead: 'Responsable du projet',
      leadName: 'Prof. Dr. Benjamin Jörissen',
      leadInstitute: 'Institut d\'Éducation',
      leadChair: 'Chaire d\'Éducation avec spécialisation en Culture et Éducation esthétique',
      leadUnesco: 'Chaire UNESCO en Culture numérique et Arts dans l\'Éducation',
      researcher: 'Collaborateur/trice de recherche',
      researcherName: 'Vanessa Baumann',
      researcherInstitute: 'Institut d\'Éducation',
      researcherChair: 'Chaire d\'Éducation avec spécialisation en Culture et Éducation esthétique',
      researcherUnesco: 'Chaire UNESCO en Culture numérique et Arts dans l\'Éducation'
    },
    funding: {
      title: 'Financé par'
    }
  },
  legal: {
    impressum: {
      title: 'Mentions légales',
      publisher: 'Éditeur',
      represented: 'Représenté par le Président',
      responsible: 'Responsable du contenu',
      authority: 'Autorité de tutelle',
      moreInfo: 'Informations complémentaires',
      moreInfoText: 'Mentions légales complètes de la FAU :',
      funding: 'Financé par'
    },
    privacy: {
      title: 'Politique de confidentialité',
      notice: 'Avis : Le contenu généré est stocké sur le serveur à des fins de recherche. Aucune donnée utilisateur ou IP n\'est collectée. Les images téléchargées ne sont pas stockées.',
      usage: 'L\'utilisation de cette plateforme est exclusivement réservée aux partenaires de coopération enregistrés du UCDCAE AI LAB. Les accords de protection des données conclus dans ce cadre s\'appliquent. Pour toute question, veuillez contacter vanessa.baumann@fau.de.'
    }
  },
  docs: {
    title: 'Documentation & Guide',
    intro: {
      title: 'Bienvenue',
      content: 'Expériences créatives avec les transformations IA.'
    },
    gettingStarted: {
      title: 'Premiers pas',
      step1: 'Sélectionnez des propriétés dans les quadrants',
      step2: 'Entrez du texte ou une image',
      step3: 'Lancez la transformation'
    },
    modes: {
      title: 'Modes',
      mode1: { name: 'Direct', desc: 'Expériences rapides' },
      mode2: { name: 'Texte', desc: 'Transformations textuelles' },
      mode3: { name: 'Image', desc: 'Procédés basés sur l\'image' }
    },
    support: {
      title: 'Support',
      content: 'Pour toute question :'
    },
    wikipedia: {
      title: 'Recherche Wikipédia',
      subtitle: 'Le savoir sur le monde comme partie des processus artistiques',
      feature: 'Les processus artistiques nécessitent non seulement un savoir esthétique, mais aussi des connaissances factuelles sur le monde. L\'IA recherche sur Wikipédia pendant la transformation pour trouver des informations factuelles.',
      languages: 'Plus de 70 langues sont prises en charge',
      languagesDesc: 'L\'IA choisit automatiquement la Wikipédia dans la langue appropriée pour chaque sujet :',
      examples: {
        nigeria: 'Sujet sur le Nigeria → Haoussa, Yorouba, Igbo ou anglais',
        india: 'Sujet sur l\'Inde → Hindi, tamoul, bengali ou autres langues régionales',
        indigenous: 'Cultures autochtones → Quechua, Māori, Inuktitut, etc.'
      },
      why: 'Transparence : Que sait l\'IA ?',
      whyDesc: 'Le système affiche toutes les tentatives de recherche : les articles trouvés (sous forme de liens cliquables) et les termes pour lesquels rien n\'a été trouvé. Cela rend visible ce que l\'IA pense savoir — et ce qu\'elle ne sait pas.',
      culturalRespect: 'Invitation à faire vos propres recherches',
      culturalRespectDesc: 'Les liens Wikipédia affichés sont une invitation à en apprendre davantage par vous-même. Cliquez sur les liens pour vérifier les sources et enrichir vos propres connaissances.',
      limitations: 'La recherche par IA est une aide, pas un substitut à votre propre engagement avec le sujet.'
    }
  },
  multiImage: {
    image1Label: 'Image 1',
    image2Label: 'Image 2 (optionnelle)',
    image3Label: 'Image 3 (optionnelle)',
    contextLabel: 'Décrivez ce que vous voulez faire avec les images',
    contextPlaceholder: 'p. ex. Insérer la maison de l\'image 2 et le cheval de l\'image 3 dans l\'image 1. Conserver les couleurs et le style de l\'image 1.',
    modeTitle: 'Plusieurs images → Image',
    selectConfig: 'Choisissez votre modèle :',
    generating: 'Les images sont en cours de fusion...'
  },
  imageTransform: {
    imageLabel: 'Votre image',
    contextLabel: 'Décrivez ce que vous voulez changer dans l\'image',
    contextPlaceholder: 'p. ex. Transformer en peinture à l\'huile... Rendre plus coloré... Ajouter un coucher de soleil...'
  },
  textTransform: {
    inputLabel: 'Votre idée = QUOI ?',
    inputTooltip: 'Entrez le sujet de votre création.',
    inputPlaceholder: 'p. ex. Une fête dans ma rue : ...',
    contextLabel: 'Vos règles = COMMENT ?',
    contextTooltip: 'Entrez comment votre idée doit être présentée, ou cliquez sur l\'icône cercle !',
    contextPlaceholder: 'p. ex. Décrivez tout tel que les oiseaux dans les arbres le perçoivent !',
    resultLabel: 'Idée + Règles = Prompt',
    resultPlaceholder: 'Le prompt apparaîtra après avoir cliqué sur démarrer (ou entrez votre propre texte)',
    optimizedLabel: 'Prompt optimisé pour le modèle',
    optimizedPlaceholder: 'Le prompt optimisé apparaîtra après la sélection du modèle.'
  },
  training: {
    info: {
      title: 'À propos de l\'entraînement LoRA',
      studioDescription: 'Entraînez des modèles LoRA personnalisés pour Stable Diffusion 3.5 Large avec vos propres images.',
      description: 'Cet entraînement intégré est conçu pour des tests rapides.',
      limitations: 'Limitations',
      limitationDuration: 'L\'entraînement dure 1 à 3 heures',
      limitationBlocking: 'Bloque la génération d\'images pendant l\'entraînement',
      limitationConfig: 'Options de configuration limitées',
      showMore: 'En savoir plus',
      showLess: 'Afficher moins'
    },
    placeholders: {
      projectName: 'p. ex. Notre bâtiment scolaire',
      triggerWords: 'p. ex. notre_ecole, cour_de_recreation, salle_de_classe'
    },
    labels: {
      projectName: 'Nom du projet',
      triggerWords: 'Mots déclencheurs',
      triggerHelp: 'Tags séparés par des virgules. Le premier = déclencheur principal, le reste = tags supplémentaires par image.',
      images: 'Images d\'entraînement (10–50 recommandées)',
      dropZone: 'Cliquez ou déposez des images ici',
      imagesSelected: '{count} images sélectionnées',
      logs: 'Journaux d\'entraînement',
      waiting: 'En attente du démarrage de l\'entraînement...'
    },
    buttons: {
      start: 'Lancer l\'entraînement',
      stop: 'Arrêter',
      inProgress: 'Entraînement en cours...',
      delete: 'Supprimer les fichiers du projet (RGPD)',
      cancel: 'Annuler'
    },
    vram: {
      title: 'Vérification VRAM GPU',
      checking: 'Vérification de la VRAM...',
      used: 'utilisée',
      free: 'libre',
      notEnough: 'Pas assez de VRAM libre pour l\'entraînement (besoin de {gb} Go).',
      clearQuestion: 'Libérer la VRAM pour continuer ?',
      enough: 'Assez de VRAM disponible pour l\'entraînement.',
      clearing: 'Libération de la VRAM...',
      newFree: 'Nouvelle VRAM libre',
      clearBtn: 'Libérer la VRAM ComfyUI + Ollama'
    }
  },
  safetyBadges: {
    '§86a': '§86a',
    '86a_filter': '§86a',
    age_filter: 'Filtre d\'âge',
    dsgvo_ner: 'RGPD',
    dsgvo_llm: 'RGPD',
    translation: '\u2192 EN',
    fast_filter: 'Contenu',
    llm_context_check: 'Contenu (LLM)',
    llm_safety_check: 'Protection de la jeunesse',
    llm_check_failed: 'Vérification échouée',
    disabled: '\u2014'
  },
  safetyBlocked: {
    vlm: 'Votre prompt était correct, mais l\'image générée a été signalée comme inappropriée par une IA d\'analyse d\'image. Cela peut arriver — la génération d\'images n\'est pas toujours prévisible. Réessayez, chaque génération est différente !',
    para86a: 'Votre prompt a été bloqué car il contient des symboles ou termes interdits par la loi allemande (§86a StGB). Cette règle nous protège tous de la haine et de la violence. Essayez un autre sujet !',
    dsgvo: 'Votre prompt a été bloqué car il contient ce qui ressemble à un nom de personne. Ceci est protégé par le Règlement général sur la protection des données (RGPD). Utilisez des descriptions comme « une fille » ou « un vieil homme » au lieu de noms.',
    kids: 'Votre prompt a été bloqué par le filtre de sécurité enfants. Certains termes ne conviennent pas aux enfants car ils peuvent être effrayants ou perturbants. Essayez de décrire votre idée avec des mots plus amicaux !',
    youth: 'Votre prompt a été bloqué par le filtre de protection de la jeunesse. Certains contenus ne conviennent pas non plus aux adolescents. Essayez de reformuler votre idée !',
    generic: 'Votre prompt a été bloqué par le système de sécurité. Le système vous protège des contenus inappropriés. Essayez une formulation différente !',
    inputImage: 'L\'image téléchargée a été signalée comme inappropriée par une IA d\'analyse d\'image. Veuillez utiliser une autre image.',
    vlmSaw: 'L\'IA d\'image a vu',
    systemUnavailable: 'Le système de sécurité (Ollama) ne répond pas, aucun traitement supplémentaire n\'est possible. Veuillez contacter l\'administrateur système.',
    suggestionLoading: 'Attendez, j\'ai une idée...',
    suggestionError: 'Je n\'ai pas pu générer de suggestion pour le moment. Réessayez avec d\'autres mots !'
  },
  splitCombine: {
    infoTitle: 'Split & Combine — Fusion sémantique de vecteurs',
    infoDescription: 'Ce workflow fusionne deux prompts au niveau des vecteurs sémantiques. Le résultat n\'est pas un simple mélange, mais une connexion mathématique plus profonde des espaces de sens.',
    purposeTitle: 'Objectif pédagogique',
    purposeText: 'Explorez comment les modèles d\'IA représentent le sens sous forme d\'espaces numériques. Que se passe-t-il lorsque nous fusionnons mathématiquement des concepts différents ?',
    techTitle: 'Détails techniques',
    techText: 'Modèle : SD3.5 Large | Encodeur : DualCLIP (CLIP-G + T5-XXL)'
  },
  partialElimination: {
    infoTitle: 'Élimination partielle — Déconstruction vectorielle',
    infoDescription: 'Ce workflow manipule spécifiquement des parties du vecteur sémantique. En éliminant certaines dimensions, nous pouvons observer quels aspects du sens sont perdus.',
    purposeTitle: 'Objectif pédagogique',
    purposeText: 'Comprenez comment le sens est encodé à travers différentes dimensions de l\'espace vectoriel. Que reste-t-il quand on « éteint » des parties ?',
    techTitle: 'Détails techniques',
    techText: 'Modèle : SD3.5 Large | Encodeur : TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
    encoderLabel: 'Encodeur de texte',
    modeLabel: 'Mode d\'élimination',
    dimensionRange: 'Plage de dimensions',
    selected: 'Sélectionné',
    dimensions: 'Dimensions',
    emptyTitle: 'En attente de la génération...',
    emptySubtitle: 'Les résultats apparaîtront ici',
    referenceLabel: 'Image de référence',
    referenceDesc: 'Sortie non manipulée (original)',
    innerLabel: 'Plage intérieure éliminée',
    outerLabel: 'Plage extérieure éliminée'
  },
  surrealizer: {
    infoTitle: 'Surréalisateur — Extrapolation au-delà du connu',
    infoDescription: 'Deux « cerveaux » d\'IA lisent votre texte : CLIP-L comprend le langage à travers les images, T5 le comprend de manière purement linguistique. Le curseur ne fait pas simplement un mélange — il pousse l\'image bien au-delà de ce que T5 seul produirait. L\'IA doit alors interpréter des vecteurs qu\'elle n\'a jamais rencontrés pendant l\'entraînement. Le résultat : des hallucinations d\'IA — des images qu\'aucun prompt ne pourrait produire directement.',
    purposeTitle: 'Le curseur',
    purposeText: 'α < 0 : CLIP-L amplifié, T5 inversé — les 3328 dimensions supérieures (où CLIP-L est rempli de zéros) reçoivent des vecteurs T5 inversés. Les patterns d\'attention croisée dans le transformer s\'inversent : hallucinations pilotées visuellement. ◆ α = 0 : CLIP-L pur — image normale. ◆ α = 1 : T5-XXL pur — encore normal, mais qualité différente. ◆ α > 1 : extrapolation au-delà de T5. À α = 20, la formule pousse l\'embedding 19× au-delà de T5 dans un espace vectoriel inexploré — hallucinations pilotées linguistiquement. ◆ Sweet spot : α = 15–35.',
    techTitle: 'Comment ça fonctionne',
    techText: 'Votre prompt est envoyé séparément à travers deux encodeurs : CLIP-L (entraîné visuellement, 77 tokens, 768 dims → rempli à 4096) et T5-XXL (entraîné linguistiquement, 512 tokens, 4096 dims). Les 77 premières positions de tokens sont fusionnées : (1-α)·CLIP-L + α·T5. Les tokens T5 restants (78–512) restent inchangés comme ancre sémantique — ils maintiennent l\'image liée à votre texte quel que soit l\'extrémité de α. À α > 1, ce n\'est pas un mélange mais une extrapolation : des vecteurs qu\'aucun entraînement n\'a jamais produits. À α < 0, T5 est inversé et CLIP-L amplifié — des hallucinations qualitativement différentes car les patterns d\'attention croisée dans le transformer sont inversés.',
    sliderLabel: 'Extrapolation (α)',
    sliderNormal: 'normal',
    sliderWeird: 'étrange',
    sliderCrazy: 'fou',
    sliderExtremeWeird: 'super étrange',
    sliderExtremeCrazy: 'super fou',
    sliderHint: "α<0 : au-delà de CLIP {'|'} α=0 : CLIP pur {'|'} α=1 : T5 pur {'|'} α>1 : au-delà de T5",
    expandLabel: 'Enrichir le prompt pour T5',
    expandSuggest: 'Prompt court détecté — l\'expansion T5 améliore significativement les résultats avec peu de mots.',
    expandHint: 'Votre prompt contient peu de mots (~{count} tokens CLIP). Pour des hallucinations optimales, l\'IA peut enrichir narrativement le contexte T5.',
    expandActive: 'Enrichissement du prompt...',
    expandResultLabel: 'Expansion T5 (encodeur T5 uniquement)',
    advancedLabel: 'Paramètres avancés',
    negativeLabel: 'Prompt négatif',
    negativeHint: 'Extrapolé avec le même α. Détermine de quoi l\'image s\'éloigne par extrapolation — des négatifs différents produisent des esthétiques fondamentalement différentes.',
    cfgLabel: 'Échelle CFG',
    cfgHint: 'Classifier-Free Guidance : force d\'influence du prompt. Plus élevé = effet plus fort, moins de variation.'
  },
  musicGeneration: {
    infoTitle: 'Génération musicale',
    infoDescription: 'Créez de la musique à partir de texte et de tags de style. L\'IA génère des mélodies, des rythmes et des harmonies à partir de vos paroles et spécifications de genre.',
    purposeTitle: 'Objectif pédagogique',
    purposeText: 'Explorez comment l\'IA interprète les concepts musicaux. Comment le choix des mots dans les paroles affecte-t-il la mélodie ?',
    lyricsLabel: 'Paroles (Texte)',
    lyricsPlaceholder: '[Couplet]\nVos paroles ici...\n\n[Refrain]\nRefrain...',
    tagsLabel: 'Tags de style',
    tagsPlaceholder: 'pop, piano, entraînant, voix féminine, 120bpm',
    selectModel: 'Choisissez un modèle musical :',
    generate: 'Générer de la musique',
    generating: 'Génération de la musique...'
  },
  musicGen: {
    simpleMode: 'Simple',
    advancedMode: 'Avancé',
    lyricsLabel: 'Paroles',
    lyricsPlaceholder: 'Écrivez les paroles de votre chanson avec des marqueurs de structure comme [Couplet], [Refrain], [Pont]...\n\nExemple :\n[Couplet]\ndo ré mi fa sol\nla si do ré mi\n\n[Refrain]\nc\'est tout ce que je veux te chanter',
    tagsLabel: 'Tags de style',
    tagsPlaceholder: 'Genre, ambiance, instruments...\n\nExemple : ska, agressif, entraînant, haute définition, trio basse et saxophone',
    refineButton: 'Affiner paroles et tags',
    refinedLyricsLabel: 'Paroles affinées',
    refinedLyricsPlaceholder: 'Vos paroles affinées apparaîtront ici...',
    refiningLyricsMessage: 'L\'IA affine vos paroles...',
    refinedTagsLabel: 'Tags affinés',
    refinedTagsPlaceholder: 'Les tags de style affinés apparaîtront ici...',
    refiningTagsMessage: 'L\'IA génère des tags de style correspondants...',
    selectModel: 'Choisir un modèle musical',
    generateButton: 'Générer de la musique',
    quality: 'Qualité'
  },
  musicGenV2: {
    lyricsWorkshop: 'Atelier paroles',
    lyricsInput: 'Votre texte',
    lyricsPlaceholder: 'Écrivez des paroles, un thème, des mots-clés ou une ambiance...',
    themeToLyrics: 'Mots-clés vers paroles de chanson',
    refineLyrics: 'Structurer les paroles',
    resultLabel: 'Résultat',
    resultPlaceholder: 'Vos paroles apparaîtront ici...',
    expandingTheme: 'L\'IA écrit des paroles à partir de vos mots-clés...',
    refiningLyrics: 'L\'IA structure vos paroles...',
    soundExplorer: 'Explorateur sonore',
    suggestFromLyrics: 'Suggérer à partir des paroles',
    suggestingTags: 'L\'IA analyse vos paroles...',
    mostImportant: 'le plus important',
    dimGenre: 'Genre',
    dimTimbre: 'Timbre',
    dimGender: 'Voix',
    dimMood: 'Ambiance',
    dimInstrument: 'Instruments',
    dimScene: 'Scène',
    dimRegion: 'Région (UNESCO)',
    dimTopic: 'Thème',
    audioLength: 'Durée audio',
    generateButton: 'Générer de la musique',
    selectModel: 'Modèle',
    customTags: 'Tags personnalisés',
    customTagsPlaceholder: 'p. ex. acoustic,dreamy,summer_vibes'
  },
  latentLab: {
    tabs: {
      image: 'Laboratoire d\'images',
      textlab: 'Latent Text Lab',
      crossmodal: 'Laboratoire crossmodal'
    },
    imageLab: {
      headerTitle: 'Laboratoire d\'images — Recherche visuelle en espace vectoriel',
      headerSubtitle: 'Cinq outils pour explorer comment les modèles de diffusion génèrent des images à partir de texte : du débruitage à l\'attention et la fusion, jusqu\'à l\'arithmétique vectorielle.',
      tabs: {
        archaeology: {
          label: 'Archéologie du débruitage',
          short: 'Observer le modèle travailler'
        },
        attention: {
          label: 'Cartographie d\'attention',
          short: 'Voir où le modèle regarde'
        },
        fusion: {
          label: 'Fusion d\'encodeurs',
          short: 'Mélange surréaliste'
        },
        probing: {
          label: 'Sondage de caractéristiques',
          short: 'Analyse dimensionnelle'
        },
        algebra: {
          label: 'Algèbre de concepts',
          short: 'Arithmétique vectorielle'
        }
      }
    },
    comingSoon: 'Cet outil sera implémenté dans une version future.',
    shared: {
      negativeHint: 'Termes que le modèle doit activement éviter (ex. "flou, texte")',
      stepsHint: 'Plus d\'étapes = meilleure qualité mais temps de génération plus long',
      cfgHint: 'Classifier-Free Guidance : plus élevé = plus fidèle au prompt, moins de variation',
      seedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
      recordingActive: 'Enregistrement actif',
      recordingCount: '{count} enregistrement | {count} enregistrements',
      recordingTooltip: 'Les données de recherche sont sauvegardées automatiquement',
    },
    attention: {
      headerTitle: 'Cartographie d\'attention — Quel mot dirige quelle région de l\'image ?',
      headerSubtitle: 'Pour chaque mot du prompt, une heatmap superposée à l\'image générée montre OÙ dans l\'image ce mot a eu le plus d\'influence. Cela révèle comment le modèle distribue spatialement les concepts sémantiques.',
      explanationToggle: 'Afficher l\'explication détaillée',
      explainWhatTitle: 'Que montre cet outil ?',
      explainWhatText: 'Lorsqu\'un modèle de diffusion génère une image, il ne lit pas le prompt mot à mot comme un ensemble d\'instructions. Au lieu de cela, un mécanisme appelé « attention » distribue l\'influence de chaque mot à travers différentes régions de l\'image. Le mot « maison » influence principalement la région où apparaît la maison — mais aussi les zones voisines, car le modèle comprend le contexte de toute la scène. Cet outil rend cette distribution visible : cliquez sur un mot et voyez quelles régions de l\'image s\'illuminent.',
      explainHowTitle: 'Comment lire la heatmap ?',
      explainHowText: 'Couleur vive et intense = forte influence du mot sur cette région. Couleur sombre ou absente = peu d\'influence. Si vous sélectionnez plusieurs mots, ils apparaissent en couleurs différentes. Note : les cartes ne sont PAS parfaitement délimitées — ce n\'est pas un bug, mais cela montre que le modèle traite les concepts contextuellement, pas de manière isolée. Une « maison » dans une scène de ferme a aussi une certaine influence sur les animaux et les champs, car le modèle comprend la scène dans son ensemble.',
      explainReadTitle: 'Que révèlent les deux curseurs ?',
      explainReadText: 'Le curseur d\'étape de débruitage montre QUAND dans le processus de génération en 25 étapes vous observez l\'attention. Les premières étapes montrent la planification grossière de la mise en page, les dernières étapes l\'attribution des détails. Le sélecteur de profondeur réseau montre OÙ dans le transformer l\'attention est mesurée : les couches superficielles (près de l\'entrée) montrent la planification de la composition globale, les couches médianes l\'attribution sémantique, les couches profondes le peaufinage. Les deux axes sont indépendants — il vaut la peine d\'explorer systématiquement différentes combinaisons.',
      techTitle: 'Détails techniques',
      techText: 'SD3.5 utilise un MMDiT (Multimodal Diffusion Transformer) avec attention jointe : les tokens image et texte s\'attendent mutuellement à travers 24 blocs transformer. Nous remplaçons le processeur SDPA par défaut par un processeur softmax(QK^T/√d) manuel à 3 blocs sélectionnés pour extraire la sous-matrice d\'attention texte→image. Les cartes sont en résolution 64x64 (grille de patches), mises à l\'échelle par interpolation bilinéaire. SD3.5 utilise deux encodeurs de texte : CLIP-L (BPE, 77 tokens) et T5-XXL (SentencePiece, 512 tokens). Les deux peuvent être activés ici pour voir comment différentes stratégies de tokenisation affectent l\'attention.',
      referencesTitle: 'Références scientifiques',
      promptLabel: 'Prompt',
      promptPlaceholder: 'p. ex. Une maison se dresse dans un paysage, entourée de terres agricoles, de nature et d\'animaux. Quelques personnes sont visibles.',
      generate: 'Générer + Analyser',
      generating: 'Génération de l\'image et extraction de l\'attention...',
      emptyHint: 'Entrez un prompt et cliquez sur Générer pour visualiser les cartes d\'attention du modèle.',
      advancedLabel: 'Paramètres avancés',
      negativeLabel: 'Prompt négatif',
      stepsLabel: 'Étapes',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      tokensLabel: 'Tokens',
      tokensHint: 'Cliquez sur un ou plusieurs mots. Les sous-tokens (p. ex. « Mai » + « son ») sont automatiquement combinés. Plusieurs mots apparaissent en couleurs différentes.',
      timestepLabel: 'Étape de débruitage',
      timestepHint: 'Les modèles de diffusion génèrent des images en 25 étapes, du bruit à l\'image. Les premières étapes établissent la structure grossière, les dernières affinent les détails. Ce curseur montre ce à quoi le modèle prête attention à chaque étape.',
      step: 'Étape',
      layerLabel: 'Profondeur réseau',
      layerHint: 'À chaque étape de débruitage, le signal traverse les 24 couches transformer. Les couches superficielles (près de l\'entrée) capturent la composition globale, les couches médianes l\'attribution sémantique, les couches profondes (près de la sortie) les détails fins. Les deux contrôles sont indépendants : étape = quand dans le processus, profondeur = où dans le réseau.',
      layerEarly: 'Superficiel (Composition)',
      layerMid: 'Médian (Sémantique)',
      layerLate: 'Profond (Détail)',
      opacityLabel: 'Heatmap',
      opacityHint: 'Intensité de la superposition colorée sur l\'image.',
      baseImageLabel: 'Image de base',
      baseColor: 'Couleur',
      baseBW: 'N/B',
      baseOff: 'Désactivé',
      baseImageHint: 'Couleur affiche l\'image originale. N/B la désature pour que les couleurs de la heatmap ressortent. Désactivé masque entièrement l\'image et n\'affiche que la carte d\'attention.',
      encoderLabel: 'Encodeur de texte',
      encoderClipL: 'CLIP-L (77 Tokens)',
      encoderT5: 'T5-XXL (512 Tokens)',
      encoderHint: 'SD3.5 utilise deux encodeurs de texte avec une tokenisation différente. CLIP-L utilise BPE (Byte-Pair Encoding), T5-XXL utilise SentencePiece. Comparez comment les deux encodeurs traitent le même prompt et quelles régions de l\'image chacun dirige.',
      download: 'Télécharger l\'image'
    },
    probing: {
      headerTitle: 'Sondage de caractéristiques — Quelles dimensions encodent quoi ?',
      headerSubtitle: 'Comparez deux prompts et découvrez quelles dimensions d\'embedding encodent la différence sémantique. Transférez sélectivement des dimensions individuelles pour voir comment elles affectent l\'image.',
      explanationToggle: 'Afficher l\'explication détaillée',
      explainWhatTitle: 'Que montre cet outil ?',
      explainWhatText: 'Chaque mot est converti par l\'encodeur de texte en un vecteur de haute dimension (p. ex. 4096 dimensions pour T5). Lorsque vous changez un mot dans le prompt — p. ex. « rouge » en « bleu » — certaines dimensions changent plus que d\'autres. Cet outil vous montre QUELLES dimensions changent le plus et vous permet de transférer sélectivement des dimensions individuelles du prompt B vers le prompt A.',
      explainHowTitle: 'Comment fonctionne le transfert ?',
      explainHowText: 'Le diagramme à barres montre toutes les dimensions triées par magnitude de différence. Utilisez les contrôles de plage de rang (De/À) pour sélectionner une fenêtre — p. ex. juste les 100 premières ou spécifiquement les rangs 880–920. Cliquer sur « Transférer » régénère l\'image avec les mêmes paramètres (même seed !) — mais avec les dimensions sélectionnées du prompt B. Cela vous permet de voir exactement ce que ces dimensions « encodent ».',
      explainReadTitle: 'Comment lire le diagramme à barres ?',
      explainReadText: 'Chaque barre représente une dimension d\'embedding. La longueur montre combien cette dimension diffère entre le prompt A et B. Les dimensions avec de grandes différences sont les porteurs les plus probables du changement sémantique. Mais attention : les embeddings sont distribués — souvent plusieurs dimensions ensemble sont nécessaires pour produire un changement visible.',
      techTitle: 'Détails techniques',
      techText: 'SD3.5 utilise trois encodeurs de texte : CLIP-L (768d), CLIP-G (1280d) et T5-XXL (4096d). Vous pouvez sonder chacun individuellement. La différence est calculée comme déviation absolue moyenne sur toutes les positions de tokens : mean(abs(B-A), dim=tokens). Le transfert remplace les dimensions sélectionnées sur toutes les positions de tokens simultanément.',
      referencesTitle: 'Références scientifiques',
      promptALabel: 'Prompt A (Original)',
      promptBLabel: 'Prompt B (Comparaison)',
      promptAPlaceholder: 'p. ex. Une maison rouge au bord du lac',
      promptBPlaceholder: 'p. ex. Une maison bleue au bord du lac',
      encoderLabel: 'Encodeur',
      encoderAll: 'Tous (recommandé)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      analyzeBtn: 'Analyser',
      analyzing: 'Encodage et comparaison des prompts...',
      transferBtn: 'Transférer les dimensions vectorielles sélectionnées du Prompt B dans l\'image générée',
      transferring: 'Génération de l\'image avec l\'embedding modifié...',
      rankFromLabel: 'Du rang',
      rankToLabel: 'Au rang',
      sliderLabel: 'Sélectionner des dimensions du Prompt B',
      range1Label: 'Plage 1',
      range2Label: 'Plage 2',
      addRange: 'Ajouter une plage',
      selectionDesc: '{count} dimensions du Prompt B sélectionnées (rang {ranges} sur {total})',
      listTitle: 'Les {count} dimensions du Prompt B avec la plus grande différence par rapport au Prompt A',
      sortAsc: 'Croissant',
      sortDesc: 'Décroissant',
      originalLabel: 'Original (Prompt A)',
      modifiedLabel: 'Modifié (Transfert du Prompt B)',
      modifiedHint: 'Sélectionnez une plage de rangs ci-dessous et cliquez sur « Transférer » — ceci affichera le prompt A avec les dimensions transférées de B (même seed).',
      noDifference: 'Les embeddings sont identiques — modifiez le prompt B.',
      advancedLabel: 'Paramètres avancés',
      negativeLabel: 'Prompt négatif',
      stepsLabel: 'Étapes',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      selectAll: 'Tout',
      selectNone: 'Aucun',
      encoderHint: 'Tous = tous les encodeurs combinés. CLIP-L/CLIP-G/T5 = isole un seul encodeur pour l\'analyse.',
      sliderHint: 'Sélectionnez une plage de rangs des dimensions d\'embedding les plus importantes (triées par différence entre A et B).',
      transferHint: 'Transfère les dimensions sélectionnées du Prompt B vers le Prompt A et génère une nouvelle image.',
      downloadOriginal: 'Télécharger l\'original',
      downloadModified: 'Télécharger le modifié'
    },
    algebra: {
      headerTitle: 'Algèbre de concepts \u2014 Arithmétique vectorielle sur les embeddings d\'image',
      headerSubtitle: 'Appliquez la célèbre analogie word2vec à la génération d\'images : Roi \u2212 Homme + Femme \u2248 Reine. Trois prompts sont encodés et combinés algébriquement.',
      explanationToggle: 'Afficher l\'explication détaillée',
      explainWhatTitle: 'Que montre cet outil ?',
      explainWhatText: 'En 2013, Mikolov a montré que les embeddings de mots encodent les relations sémantiques comme des directions linéaires : le vecteur de « Roi » moins « Homme » plus « Femme » donne un vecteur proche de « Reine ». Cet outil applique cette idée aux encodeurs de texte de SD3.5 : au lieu de mots isolés, vous manipulez des embeddings de prompts entiers. Le résultat est une image contenant le concept A mais avec B remplacé par C.',
      explainHowTitle: 'Comment fonctionne l\'algèbre \u2014 et pourquoi ne pas simplement utiliser un prompt négatif ?',
      explainHowText: 'Vous entrez trois prompts : A (base), B (soustraire) et C (ajouter). La formule est : Résultat = A \u2212 Échelle\u2081\u00d7B + Échelle\u2082\u00d7C. Les curseurs d\'échelle contrôlent l\'intensité : à 1.0, B est entièrement soustrait et C entièrement ajouté. À 0.5, seulement la moitié. Les valeurs au-dessus de 1.0 amplifient l\'effet. \u2014 Pourquoi ne pas simplement utiliser « A + C » comme prompt et « B » comme prompt négatif ? Parce que cela fait quelque chose de fondamentalement différent : un prompt négatif dirige le processus de débruitage loin de B à CHACUNE des 25 étapes \u2014 le modèle décide étape par étape comment interpréter « pas B ». L\'algèbre de concepts calcule plutôt un nouveau vecteur AVANT la génération d\'image : la soustraction se fait dans l\'espace d\'embedding, pas dans le processus de diffusion. Le résultat est un vecteur unique qui encode directement « A sans la B-ité plus la C-ité ». Le prompt négatif dit « ne fais pas ça ». L\'algèbre dit « retire ce concept et mets celui-ci » \u2014 une opération chirurgicale dans l\'espace des sens plutôt qu\'une stratégie d\'évitement étape par étape.',
      explainReadTitle: 'Que signifient les résultats ?',
      explainReadText: 'À gauche vous voyez l\'image de référence (prompt A uniquement, même seed). À droite, le résultat de l\'algèbre. Si l\'analogie fonctionne, l\'image de droite devrait montrer le concept A mais avec le changement sémantique B\u2192C. Exemple : « Coucher de soleil sur la plage » \u2212 « Plage » + « Montagnes » \u2248 « Coucher de soleil sur les montagnes ». La distance L2 montre à quel point le résultat s\'est éloigné de l\'original. \u2014 L\'opération est-elle commutative ? Non. La soustraction de B et l\'addition de C se font par rapport au vecteur A. La direction B\u2192C n\'a de sens que dans le contexte de A : « Roi \u2212 Homme » supprime les directions « masculines » du vecteur Roi, « + Femme » ajoute les directions « féminines » \u2014 le résultat atterrit près de « Reine ». C n\'est pas placé chirurgicalement là où B a été supprimé ; il est simplement ajouté. Que cela fonctionne quand même montre que les relations sémantiques sont encodées comme des directions linéaires cohérentes dans l\'espace vectoriel.',
      techTitle: 'Détails techniques',
      techText: 'L\'algèbre est effectuée sur les embeddings de l\'encodeur sélectionné : CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), ou tous combinés (589 tokens \u00d7 4096d). La même opération est aussi appliquée aux embeddings pooled (2048d). Les deux images utilisent le même seed pour une comparaison équitable.',
      referencesTitle: 'Références scientifiques',
      promptALabel: 'Prompt A (Base)',
      promptAPlaceholder: 'p. ex. Coucher de soleil sur la plage avec des palmiers',
      promptBLabel: 'Prompt B (Soustraire)',
      promptBPlaceholder: 'p. ex. Plage avec des palmiers',
      promptCLabel: 'Prompt C (Ajouter)',
      promptCPlaceholder: 'p. ex. Montagnes enneigées',
      formulaLabel: 'A \u2212 B + C = ?',
      encoderLabel: 'Encodeur',
      encoderAll: 'Tous (recommandé)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      generateBtn: 'Calculer',
      generating: 'Calcul des embeddings et génération des images...',
      referenceLabel: 'Référence (Prompt A)',
      resultLabel: 'Résultat (A \u2212 B + C)',
      l2Label: 'Distance L2 par rapport à l\'original',
      advancedLabel: 'Paramètres avancés',
      negativeLabel: 'Prompt négatif',
      stepsLabel: 'Étapes',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      scaleSubLabel: 'Échelle de soustraction',
      scaleAddLabel: 'Échelle d\'addition',
      encoderHint: 'Tous = tous les encodeurs combinés. CLIP-L/CLIP-G/T5 = isole un seul encodeur pour l\'arithmétique.',
      scaleSubHint: 'Poids de la soustraction (B). Plus élevé = suppression plus forte du concept B.',
      scaleAddHint: 'Poids de l\'addition (C). Plus élevé = injection plus forte du concept C.',
      l2Hint: 'Distance euclidienne dans l\'espace d\'embedding. Plus petit = plus similaire, plus grand = plus différent.',
      downloadReference: 'Télécharger la référence',
      downloadResult: 'Télécharger le résultat',
      resultHint: 'Entrez trois prompts et cliquez sur Calculer \u2014 le résultat de l\'arithmétique vectorielle apparaîtra ici.'
    },
    archaeology: {
      headerTitle: 'Archéologie du débruitage \u2014 Comment le bruit devient-il une image ?',
      headerSubtitle: 'Observez chaque étape de débruitage. Les modèles de diffusion ne dessinent pas de gauche à droite \u2014 ils travaillent partout simultanément, des formes grossières aux détails fins.',
      explanationToggle: 'Afficher l\'explication détaillée',
      explainWhatTitle: 'Que montre cet outil ?',
      explainWhatText: 'Un modèle de diffusion crée une image en supprimant progressivement le bruit. Contrairement au dessin de gauche à droite, le modèle travaille sur TOUTES les régions de l\'image simultanément. Dans les premières étapes, des structures grossières émergent : où est le haut, où est le bas ? Où est l\'horizon ? Dans les étapes intermédiaires, le contenu sémantique apparaît : objets, formes, couleurs. Les étapes finales affinent les textures et les détails. Cet outil rend chaque étape visible.',
      explainHowTitle: 'Comment utiliser cet outil ?',
      explainHowText: 'Entrez un prompt et cliquez sur Générer. Le modèle produit 25 images intermédiaires (une par étape de débruitage). Elles apparaissent sous forme de pellicule en dessous. Cliquez sur une miniature ou utilisez le curseur de timeline pour voir chaque étape en taille réelle. Comparez les premières et dernières étapes : quand le modèle « sait-il » ce qu\'il dessine ?',
      explainReadTitle: 'Que révèlent les trois phases ?',
      explainReadText: 'Premières étapes (1\u20138) : Composition globale \u2014 structure de base, distribution des couleurs, planification de la mise en page. Étapes intermédiaires (9\u201317) : Émergence sémantique \u2014 les objets deviennent reconnaissables, les formes se cristallisent. Dernières étapes (18\u201325) : Affinement des détails \u2014 textures, bords, motifs fins. Les transitions sont graduelles, mais les phases montrent clairement : le modèle « planifie » d\'abord globalement, puis affine localement. Particulièrement révélateur : la toute première étape ne montre pas des pixels fins, mais des taches colorées. C\'est parce que le bruit est généré dans l\'espace latent (128\u00d7128 à 16 canaux), pas dans l\'espace pixel. Le VAE traduit chaque pixel latent en un patch d\'environ 8\u00d78 pixels \u2014 même du bruit gaussien pur devient des groupes de couleurs cohérents. Le modèle ne « pense » jamais en pixels individuels, mais toujours dans cet espace compressé.',
      techTitle: 'Détails techniques',
      techText: 'SD3.5 Large utilise Rectified Flow comme planificateur avec 25 étapes par défaut. À chaque étape, les vecteurs latents actuels sont décodés par le VAE (1024\u00d71024 JPEG). Le VAE (Variational Autoencoder) traduit l\'espace latent mathématique en pixels. La représentation latente est 128\u00d7128 à 16 canaux \u2014 chaque pixel latent correspond à un patch d\'environ 8\u00d78 pixels dans l\'image. C\'est pourquoi même la première étape montre des groupes colorés plutôt que du bruit pixel fin : le VAE interprète des vecteurs aléatoires à 16 dimensions comme des patches de couleur cohérents.',
      referencesTitle: 'Références scientifiques',
      promptLabel: 'Prompt',
      promptPlaceholder: 'p. ex. Une place de marché dans une ville médiévale avec des gens, des bâtiments et une fontaine',
      generate: 'Générer',
      generating: 'Génération de l\'image \u2014 enregistrement de chaque étape...',
      emptyHint: 'Entrez un prompt et cliquez sur Générer pour visualiser le processus de débruitage.',
      advancedLabel: 'Paramètres avancés',
      negativeLabel: 'Prompt négatif',
      stepsLabel: 'Étapes',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      filmstripLabel: 'Pellicule de débruitage',
      timelineLabel: 'Étape',
      phaseEarly: 'Composition',
      phaseMid: 'Sémantique',
      phaseLate: 'Détail',
      phaseEarlyDesc: 'La structure globale et la distribution des couleurs émergent',
      phaseMidDesc: 'Les objets et les formes deviennent reconnaissables',
      phaseLateDesc: 'Les textures et les détails fins sont affinés',
      finalImageLabel: 'Image finale (pleine résolution)',
      timelineHint: 'Parcourt les étapes de débruitage — montre comment l\'image émerge du bruit vers la composition finale.',
      download: 'Télécharger l\'image'
    },
    textLab: {
      headerTitle: 'Latent Text Lab \u2014 Déconstruction scientifique de LLM',
      headerSubtitle: 'Ingénierie de représentation, archéologie comparative de modèles et analyse systématique de biais : trois outils basés sur la recherche pour étudier les modèles de langage.',
      explanationToggle: 'Afficher l\'explication',
      modelPanel: {
        presetLabel: 'Préréglage',
        presetNone: 'Pas de préréglage (ID personnalisé)',
        customModelLabel: 'ID modèle HuggingFace',
        customModelPlaceholder: 'p. ex. meta-llama/Llama-3.2-1B',
        quantizationLabel: 'Quantification',
        quantAuto: 'Auto',
        quantizationHint: 'bf16 = qualité maximale, int8 = moitié de VRAM, int4 = VRAM minimal mais qualité la plus basse',
      },
      temperatureHint: 'Aléatoire de la génération de texte. Bas = déterministe, haut = plus créatif.',
      maxTokensHint: 'Nombre maximum de tokens générés (morceaux de mots).',
      textSeedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
      tabs: {
        repeng: { label: 'Ing\u00e9nierie des repr\u00e9sentations', short: 'Trouver des vecteurs de guidage dans les LLM' },
        compare: { label: 'Comparaison de mod\u00e8les', short: 'Comparer deux LLM couche par couche' },
        bias: { label: 'Arch\u00e9ologie des biais', short: 'R\u00e9v\u00e9ler les biais cach\u00e9s dans les LLM' },
      },
      repeng: {
        title: 'Ingénierie de représentation',
        subtitle: 'Trouver des directions de concepts dans l\'espace d\'activation et guider la génération',
        explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
        explainWhatText: 'Bas\u00e9 sur Zou et al. (2023) \u00ab Representation Engineering \u00bb et Li et al. (2024) \u00ab Inference-Time Intervention \u00bb. Les LLM encodent des concepts abstraits comme des directions dans l\'espace d\'activation. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
        explainHowTitle: 'Comment l\'utiliser ?',
        explainHowText: 'Cette exp\u00e9rience extrait une \u00ab direction de v\u00e9rit\u00e9 \u00bb du mod\u00e8le. Les paires de contraste contiennent chacune une d\u00e9claration vraie et une fausse. Recommandation : les prompts en anglais fonctionnent mieux.',
        referencesTitle: 'Références scientifiques',
        expectedResults: 'Résultats attendus : À α = 0 (baseline), le modèle génère la bonne réponse. À α = -1 (inversion), une mauvaise réponse devrait apparaître — c\'est le cœur de l\'expérience. À α = +1, peu de choses changent car le modèle répond déjà correctement. Au-delà de |α| > 2, les artefacts dominent (répétitions, non-sens). Le « sweet spot » selon Zou et al. est |α| entre 0,5 et 2,0. La variance expliquée > 50% indique une séparation nette — en dessous, les paires de contraste sont trop similaires ou trop peu nombreuses (au moins 3 recommandées).',
        pairsTitle: 'Paires de contraste',
        pairsSubtitle: 'Au moins 3 paires recommandées. Chaque paire ne doit différer que par le concept cible (vrai vs. faux). Les exemples sont modifiables.',
        positiveLabel: 'Positif (vrai)',
        negativeLabel: 'Négatif (faux)',
        positivePlaceholder: 'p. ex. La capitale de la France est Paris',
        negativePlaceholder: 'p. ex. La capitale de la France est Berlin',
        addPair: 'Ajouter une paire',
        removePair: 'Supprimer',
        targetLayerLabel: 'Couche cible',
        targetLayerHint: 'Quelle couche du transformer reçoit le vecteur de direction. Différentes couches influencent différents aspects de la génération de texte.',
        targetLayerAuto: 'Dernière couche',
        findDirection: 'Trouver la direction',
        finding: 'Calcul de la direction du concept...',
        directionFound: 'Direction du concept trouvée',
        varianceLabel: 'Variance expliquée',
        dimLabel: 'Dimensions',
        projectionsTitle: 'Projections des paires de contraste',
        testTitle: 'Test + Manipulation',
        testSubtitle: 'Entrez une phrase et guidez la génération le long de la direction du concept',
        testPromptLabel: 'Prompt de test',
        testPromptPlaceholder: 'p. ex. La capitale de l\'Allemagne est',
        alphaLabel: 'Force de manipulation (α)',
        alphaHint: 'Force du vecteur de direction. 0 = aucun effet, plus élevé = influence plus forte des paires de contraste.',
        temperatureLabel: 'Température',
        maxTokensLabel: 'Tokens max',
        seedLabel: 'Seed (-1 = aléatoire)',
        generateBtn: 'Générer avec manipulation',
        generating: 'Exécution de la génération manipulée...',
        baselineLabel: 'Baseline (sans manipulation)',
        manipulatedLabel: 'Manipulé (α = {alpha})',
        projectionLabel: 'Projection sur la direction du concept',
        interpretationTitle: 'Interprétation',
        interpreting: 'Analyse des résultats...',
        interpretationError: 'Impossible de générer l\'interprétation'
      },
      compare: {
        title: 'Archéologie comparative de modèles',
        subtitle: 'Chargez deux modèles et comparez systématiquement leurs représentations internes',
        explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
        explainWhatText: 'Bas\u00e9 sur Belinkov (2022) et Olsson et al. (2022). La heatmap montre le CKA entre les couches des deux mod\u00e8les. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
        explainHowTitle: 'Comment l\'utiliser ?',
        explainHowText: 'Le mod\u00e8le A est le pr\u00e9r\u00e9glage actif. Choisissez un second mod\u00e8le (B) et chargez-le. Entrez du texte et cliquez \u00ab Comparer \u00bb.',
        referencesTitle: 'Références scientifiques',
        modelATitle: 'Modèle A (du sélecteur de préréglage)',
        modelAHint: 'Changez via le menu déroulant ci-dessus',
        modelBTitle: 'Modèle B (second modèle)',
        modelBPresetLabel: 'Préréglage',
        modelBCustomLabel: 'ID modèle HuggingFace',
        modelBCustomPlaceholder: 'p. ex. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        modelBLoadBtn: 'Charger le modèle B',
        modelBLoaded: 'Modèle B chargé',
        modelBNone: 'Modèle B non chargé',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ex. Le chat était assis sur le tapis et regardait les oiseaux',
        seedLabel: 'Seed',
        temperatureLabel: 'Température',
        maxTokensLabel: 'Tokens max',
        compareBtn: 'Comparer',
        comparing: 'Comparaison des modèles...',
        heatmapTitle: 'Alignement des couches (CKA)',
        heatmapAxisA: 'Modèle A \u2014 Couches',
        heatmapAxisB: 'Modèle B \u2014 Couches',
        heatmapExplain: 'Cellules claires = forte similarité de représentation. Les motifs diagonaux montrent que les modèles traitent l\'information dans un ordre similaire.',
        attentionTitle: 'Comparaison d\'attention (dernière couche)',
        modelALabel: 'Modèle A',
        modelBLabel: 'Modèle B',
        generationTitle: 'Comparaison de génération (même seed)',
        layerStatsTitle: 'Statistiques des couches',
        interpretationTitle: 'Interprétation',
        interpreting: 'Analyse des résultats...',
        interpretationError: 'Impossible de générer l\'interprétation'
      },
      bias: {
        title: 'Archéologie de biais',
        subtitle: 'Expériences systématiques de biais par manipulation contrôlée de tokens',
        explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
        explainWhatText: 'Bas\u00e9 sur Zou et al. (2023) et Bricken et al. (2023). Cet outil \u00e9tudie les biais syst\u00e9matiques. \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
        explainHowTitle: 'Comment l\'utiliser ?',
        explainHowText: 'Choisissez un type d\'exp\u00e9rience. Entrez un prompt qui invite le mod\u00e8le \u00e0 continuer. Les r\u00e9sultats montrent les g\u00e9n\u00e9rations de base vs. manipul\u00e9es.',
        referencesTitle: 'Références scientifiques',
        presetLabel: 'Type d\'expérience',
        presetGender: 'Genre \u2014 Supprimer les pronoms genrés',
        presetSentiment: 'Sentiment \u2014 Amplifier positif/négatif',
        presetDomain: 'Domaine \u2014 Amplifier scientifique/poétique',
        presetCustom: 'Expérience personnalisée',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ex. Le médecin dit au patient',
        customBoostLabel: 'Tokens à amplifier (séparés par des virgules)',
        customBoostPlaceholder: 'p. ex. sombre,ombre,nuit',
        customSuppressLabel: 'Tokens à supprimer (séparés par des virgules)',
        customSuppressPlaceholder: 'p. ex. lumière,soleil,clair',
        numSamplesLabel: 'Échantillons par condition',
        temperatureLabel: 'Température',
        maxTokensLabel: 'Tokens max',
        seedLabel: 'Seed de base',
        runBtn: 'Lancer l\'expérience',
        running: 'Expérience de biais en cours...',
        baselineTitle: 'Baseline (sans manipulation)',
        groupTitle: 'Groupe : {name}',
        modeSuppress: 'supprimé',
        modeBoost: 'amplifié',
        tokensLabel: 'Tokens',
        sampleSeedLabel: 'Seed',
        genderDesc: 'Supprime tous les pronoms genrés et observe quels défauts le modèle choisit.',
        sentimentDesc: 'Amplifie les mots positifs ou négatifs et mesure l\'impact sur l\'ensemble du flux textuel.',
        domainDesc: 'Amplifie le vocabulaire scientifique ou poétique et observe les changements de registre.',
        interpretationTitle: 'Interprétation',
        interpreting: 'Analyse des résultats...',
        interpretationError: 'Impossible de générer l\'interprétation'
      },
      error: {
        gpuUnreachable: 'Service GPU inaccessible. Est-il en marche ?',
        loadFailed: 'Échec du chargement du modèle.',
        operationFailed: 'Échec de l\'opération.'
      }
    },
    crossmodal: {
      headerTitle: 'Laboratoire crossmodal',
      headerSubtitle: 'Son à partir d\'espaces latents : manipulation d\'embeddings T5, génération audio guidée par image, transfert crossmodal',
      explanationToggle: 'Afficher l\'explication détaillée',
      generate: 'Générer',
      generating: 'Génération...',
      result: 'Résultat',
      seed: 'Seed',
      generationTime: 'Temps de génération',
      tabs: {
        synth: {
          label: 'Synthé audio latent',
          short: 'Manipulation d\'embeddings T5',
          title: 'Synthé audio latent',
          description: 'Manipulation directe de l\'espace de conditionnement T5 de Stable Audio (768d). Interpolez entre les prompts, extrapolez au-delà du prompt, mettez à l\'échelle les embeddings et injectez du bruit. Boucles ultra-courtes, quasi temps réel.'
        },
        mmaudio: {
          label: 'MMAudio',
          short: 'Image/texte vers audio (CVPR 2025)',
          title: 'MMAudio — Vidéo/Image vers audio',
          description: 'L\'image et le texte entrent dans le même réseau comme signaux séparés — l\'image n\'est pas traduite en langage, les deux guident simultanément la génération sonore. Le modèle a été entraîné conjointement sur vidéo et audio, apprenant des associations directes entre ce qui est vu et ce qui est entendu. Jusqu\'à 8s, 44,1kHz, ~1,2s de calcul. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
        },
        guidance: {
          label: 'Guidage ImageBind',
          short: 'Guidage par gradient basé sur l\'image',
          title: 'Guidage par gradient ImageBind',
          description: 'Guidage par gradient pendant le processus de débruitage de Stable Audio. ImageBind fournit un espace partagé de 1024d pour l\'image et l\'audio — le gradient de la similarité cosinus guide la génération audio vers l\'embedding de l\'image.'
        }
      },
      synth: {
        explainWhatTitle: 'Que fait le Synth\u00e9 audio latent ?',
        explainWhatText: 'Stable Audio g\u00e9n\u00e8re du son \u00e0 partir de texte. Le texte est converti par un encodeur T5 en un vecteur num\u00e9rique de 768 dimensions \u2014 c\'est ce vecteur que vous manipulez ici. Au lieu de changer seulement \u00ab ce que \u00bb le mod\u00e8le g\u00e9n\u00e8re (via le prompt), vous changez \u00ab comment \u00bb le mod\u00e8le comprend le texte en interne. Deux prompts qui semblent similaires peuvent \u00eatre tr\u00e8s \u00e9loign\u00e9s dans cet espace \u2014 et vice versa.',
        explainHowTitle: 'Comment utiliser cet outil ?',
        explainHowText: 'Entrez un texte dans le Prompt A \u2014 cela d\u00e9termine le son de base. Optionnel : Prompt B comme point cible. Le curseur Alpha contr\u00f4le le m\u00e9lange : \u00e0 0 vous entendez uniquement A, \u00e0 1 uniquement B, \u00e0 0.5 un m\u00e9lange. Les valeurs au-dessus de 1 extrapolent au-del\u00e0 de B (le son devient plus extr\u00eame), les valeurs en dessous de 0 vont dans la direction oppos\u00e9e. Magnitude met \u00e0 l\'\u00e9chelle l\'ensemble de l\'embedding \u2014 des valeurs plus \u00e9lev\u00e9es produisent des sons plus intenses. Noise injecte de l\'al\u00e9atoire et cr\u00e9e des variations impr\u00e9visibles. La bande spectrale (sous le bouton Generate) affiche les 768 dimensions sous forme de barres. Vous pouvez d\u00e9placer des dimensions individuelles en cliquant et glissant, manipulant directement le son. Un clic droit r\u00e9initialise une dimension.',
        promptA: 'Prompt A (Base)',
        promptAPlaceholder: 'p. ex. vagues de l\'océan',
        promptB: 'Prompt B (Optionnel, pour interpolation)',
        promptBPlaceholder: 'p. ex. mélodie de piano',
        alpha: 'Alpha (Interpolation)',
        alphaHint: '0 = A uniquement, 1 = B uniquement, entre = mélange, >1 ou <0 = extrapolation',
        magnitude: 'Magnitude (Mise à l\'échelle)',
        magnitudeHint: 'Mise à l\'échelle globale de l\'embedding (1.0 = inchangé)',
        noise: 'Bruit',
        noiseHint: 'Bruit gaussien sur l\'embedding (0 = pas de bruit)',
        duration: 'Durée (s)',
        steps: 'Étapes',
        cfg: 'CFG',
        durationHint: 'Durée du clip audio généré en secondes',
        stepsHint: 'Étapes de débruitage. Plus = meilleure qualité.',
        cfgHint: 'Classifier-Free Guidance pour la génération audio',
        seedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
        loop: 'Lecture en boucle',
        loopOn: 'Boucle activée',
        loopOff: 'Boucle désactivée',
        stop: 'Arrêter',
        looping: 'En boucle',
        playing: 'Lecture',
        stopped: 'Arrêté',
        transpose: 'Transposition (demi-tons)',
        midiSection: 'Contrôle MIDI',
        midiUnsupported: 'Web MIDI n\'est pas pris en charge par ce navigateur.',
        midiInput: 'Entrée MIDI',
        midiNone: '(aucun)',
        midiMappings: 'Mappages CC',
        midiNoteC3: 'Note (C3 = Réf)',
        midiGenerate: 'Générer + Transposer',
        midiPitch: 'Hauteur rel. C3',
        loopInterval: 'Intervalle de boucle',
        loopOptimize: 'Auto-optimiser',
        loopPingPong: 'Ping-pong',
        loopIntervalHint: 'Début/fin de la région de boucle — raccourcissez la fin pour couper le fade-out de Stable Audio',
        modeLoop: 'Boucle',
        modePingPong: 'Ping-Pong',
        modeWavetable: 'Wavetable',
        modeRate: 'Tempo (rapide)',
        modePitch: 'Hauteur (OLA)',
        wavetableScan: 'Position de balayage',
        wavetableScanHint: 'Morphing entre les trames (0 = début, 1 = fin)',
        wavetableFrames: '{count} trames',
        midiScan: 'Position de balayage',
        adsrTitle: 'Enveloppe ADSR',
        adsrAttack: 'A',
        adsrDecay: 'D',
        adsrSustain: 'S',
        adsrRelease: 'R',
        adsrHint: 'Enveloppe pour les notes MIDI (Attaque/Déclin/Sustain/Relâchement)',
        play: 'Lecture',
        normalize: 'Normaliser le volume',
        peak: 'Crête',
        crossfade: 'Fondu enchaîné',
        transposeHint: 'Décale la hauteur en demi-tons',
        crossfadeHint: 'Durée du fondu enchaîné à la frontière de boucle (ms)',
        normalizeHint: 'Normalise le volume à l\'amplitude maximale',
        saveRaw: 'Enregistrer brut',
        saveLoop: 'Enregistrer boucle',
        embeddingStats: 'Statistiques d\'embedding',
        dimensions: {
          section: 'Explorateur de dimensions',
          hint: 'Glissez sur les barres = définir l\'offset. Peignez horizontalement = plusieurs dimensions.',
          resetAll: 'Tout réinitialiser',
          hoverActivation: 'Activation',
          hoverOffset: 'Offset',
          rightClickReset: 'Clic droit = réinitialiser',
          sortDiff: 'Trié par différence de prompt',
          sortMagnitude: 'Trié par activation',
          activeOffsets: '{count} offsets actifs',
          applyAndGenerate: 'Appliquer et regénérer',
          undo: 'Annuler',
          redo: 'Rétablir'
        }
      },
      mmaudio: {
        explainWhatTitle: 'Que fait MMAudio ?',
        explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) a \u00e9t\u00e9 entra\u00een\u00e9 conjointement sur la vid\u00e9o et l\'audio. Il ne traduit pas une image en texte puis en son, mais traite image et texte comme des signaux parall\u00e8les dans le m\u00eame r\u00e9seau. Le mod\u00e8le a appris quels sons correspondent \u00e0 quelles sc\u00e8nes visuelles \u2014 une for\u00eat produit des chants d\'oiseaux, une rue du bruit de circulation, une guitare des sons de pincement.',
        explainHowTitle: 'Comment utiliser cet outil ?',
        explainHowText: 'T\u00e9l\u00e9chargez une image et/ou entrez un prompt texte \u2014 utiliser les deux ensemble donne les r\u00e9sultats les plus riches. L\'image seule g\u00e9n\u00e8re des sons correspondant au contenu visuel. Le prompt texte peut orienter le son en compl\u00e9ment ou \u00eatre utilis\u00e9 seul sans image. Dans le Prompt n\u00e9gatif, d\u00e9crivez les sons que vous NE voulez PAS entendre (p. ex. \u00ab parole, musique \u00bb). Duration d\u00e9finit la dur\u00e9e (1-8 secondes). CFG Strength contr\u00f4le la fid\u00e9lit\u00e9 du mod\u00e8le au prompt \u2014 des valeurs basses (2-3) produisent des r\u00e9sultats plus vari\u00e9s, des valeurs hautes (6-8) plus fid\u00e8les au prompt.',
        imageUpload: 'Télécharger une image (optionnel)',
        prompt: 'Prompt texte (optionnel)',
        promptPlaceholder: 'p. ex. feu de camp crépitant',
        negativePrompt: 'Prompt négatif',
        duration: 'Durée (s)',
        maxDuration: 'Max 8s (limite du modèle)',
        cfg: 'CFG',
        steps: 'Étapes',
        compareHint: 'Comparer : Texte seul vs. Image + Texte'
      },
      guidance: {
        explainWhatTitle: 'Que fait le Guidage ImageBind ?',
        explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) r\u00e9unit six sens \u2014 image, son, texte, profondeur, chaleur, mouvement \u2014 dans un \u00ab langage \u00bb commun. Cet outil exploite ce terrain commun : pendant que le son se g\u00e9n\u00e8re \u00e9tape par \u00e9tape, il demande constamment \u00ab Est-ce que \u00e7a ressemble \u00e0 l\'image ? \u00bb et corrige la direction. La similarit\u00e9 cosinus dans le r\u00e9sultat montre \u00e0 quel point le son g\u00e9n\u00e9r\u00e9 s\'est rapproch\u00e9 du contenu de l\'image.',
        explainHowTitle: 'Comment utiliser cet outil ?',
        explainHowText: 'T\u00e9l\u00e9chargez une image \u2014 c\'est la direction cible pour le son. Optionnel : un prompt texte pour un guidage suppl\u00e9mentaire. Le curseur \u00ab \u03bb Guidance Strength \u00bb est le param\u00e8tre le plus important : des valeurs basses (0.01-0.05) laissent beaucoup de libert\u00e9 au son, des valeurs hautes (0.3-1.0) le lient \u00e9troitement \u00e0 l\'image. \u00ab Warmup Steps \u00bb d\u00e9termine \u00e0 partir de quelle \u00e9tape le guidage par image commence \u2014 des valeurs basses d\u00e9marrent imm\u00e9diatement, des valeurs plus \u00e9lev\u00e9es laissent la structure de base se former librement d\'abord. Total Steps et Duration contr\u00f4lent la qualit\u00e9 et la dur\u00e9e.',
        referencesTitle: 'Références scientifiques',
        imageUpload: 'Télécharger une image',
        prompt: 'Prompt de base (optionnel)',
        promptPlaceholder: 'p. ex. paysage sonore ambiant',
        lambda: 'Force de guidage',
        lambdaHint: 'À quel point l\'image guide la génération audio',
        warmupSteps: 'Étapes de préchauffage',
        warmupHint: 'Guidage par gradient uniquement pendant les N premières étapes',
        totalSteps: 'Étapes totales',
        duration: 'Durée (s)',
        cfg: 'CFG',
        totalStepsHint: 'Étapes totales de débruitage. Plus = meilleure qualité.',
        durationHint: 'Durée du clip audio généré en secondes',
        cosineSimilarity: 'Similarité cosinus (proximité image-audio)'
      }
    }
  },
  edutainment: {
    ui: {
      didYouKnow: '\ud83e\udd14 Le saviez-vous ?',
      learnMore: '\ud83d\udcda En savoir plus',
      currentlyHappening: '\u26a1 En ce moment :',
      energyUsed: 'Énergie utilisée',
      co2Produced: 'CO\u2082 produit'
    },
    energy: {
      kids_1: '\ud83d\udca1 Les images IA ont besoin d\'\u00e9lectricit\u00e9 \u2014 autant que charger votre t\u00e9l\u00e9phone pendant 3 heures !',
      kids_2: '\ud83d\udd0c Le GPU est comme un super calculateur qui consomme beaucoup d\'\u00e9nergie !',
      kids_3: '\u26a1 Chaque image a besoin d\'autant d\'\u00e9nergie qu\'une ampoule LED allum\u00e9e pendant 10 minutes !',
      youth_1: '\u26a1 Un GPU utilise {watts}W pendant la g\u00e9n\u00e9ration \u2014 comme un petit radiateur !',
      youth_2: '\ud83d\udd0b Une image utilise environ 0,01-0,02 kWh \u2014 \u00e7a semble peu, mais \u00e7a s\'accumule !',
      youth_3: '\ud83c\udf21\ufe0f Le GPU atteint {temp}\u00b0C en ce moment \u2014 c\'est pourquoi il a besoin de refroidissement !',
      expert_1: '\ud83d\udcca Temps r\u00e9el : {watts}W \u00e0 {util}% d\'utilisation = {kwh} kWh jusqu\'ici',
      expert_2: '\ud83d\udd25 Limite TDP : {tdp}W | Actuel : {watts}W ({percent}% de la limite)',
      expert_3: '\ud83d\udcbe VRAM : {used}/{total} Go ({percent}%) \u2014 mod\u00e8le + activations'
    },
    data: {
      kids_1: '\ud83e\uddee Le GPU fait 10 milliards de calculs en ce moment \u2014 plus vite que tu ne peux compter !',
      kids_2: '\ud83c\udfa8 L\'image est cr\u00e9\u00e9e en 50 petites \u00e9tapes \u2014 comme un puzzle qui se r\u00e9sout tout seul !',
      kids_3: '\ud83e\udde9 Des millions de nombres volent \u00e0 travers le GPU en ce moment !',
      youth_1: '\ud83d\udd04 Chaque image passe par ~50 \u00ab \u00e9tapes de d\u00e9bruitage \u00bb \u2014 50 tours de suppression de bruit !',
      youth_2: '\ud83d\udcd0 8 milliards de param\u00e8tres sont interrog\u00e9s \u2014 par image !',
      youth_3: '\ud83e\udde0 L\'IA \u00ab pense \u00bb en vecteurs avec des milliers de dimensions \u2014 comme des coordonn\u00e9es dans un espace.',
      expert_1: '\ud83d\udd2c MMDiT : Multimodal Diffusion Transformer \u2014 texte + image dans des couches d\'attention jointes',
      expert_2: '\ud83d\udcc8 Self-Attention : complexit\u00e9 O(n\u00b2) \u2014 chaque token \u00ab voit \u00bb tous les autres',
      expert_3: '\u2699\ufe0f Classifier-Free Guidance : \u00e9quilibre entre influence du prompt et cr\u00e9ativit\u00e9'
    },
    model: {
      kids_1: '\ud83c\udf93 Le mod\u00e8le IA a regard\u00e9 des millions d\'images pour apprendre \u00e0 peindre !',
      kids_2: '\ud83e\udd16 L\'IA est comme un artiste qui n\'oublie jamais ce qu\'il a vu !',
      kids_3: '\u2728 8 milliards de connexions dans le mod\u00e8le \u2014 plus que les \u00e9toiles visibles dans le ciel !',
      youth_1: '\ud83e\udde0 SD3.5 Large a 8 milliards de param\u00e8tres \u2014 comme 8 milliards de n\u0153uds de d\u00e9cision.',
      youth_2: '\ud83d\udcda 3 encodeurs de texte travaillent ensemble : CLIP-L, CLIP-G et T5-XXL',
      youth_3: '\ud83d\udd22 Le mod\u00e8le a besoin de {vram} Go de VRAM rien que pour \u00eatre charg\u00e9 !',
      expert_1: '\ud83c\udfd7\ufe0f Architecture : Rectified Flow + MMDiT avec 38 blocs transformer',
      expert_2: '\ud83d\udcca Quantification FP16/FP8 : compromis pr\u00e9cision vs. VRAM',
      expert_3: '\ud83d\udd17 LoRA : Low-Rank Adaptation \u2014 seulement 0,1% des param\u00e8tres r\u00e9-entra\u00een\u00e9s'
    },
    ethics: {
      kids_1: '\ud83c\udf0d L\'IA apprend \u00e0 partir d\'images sur internet \u2014 c\'est pourquoi il est important de respecter l\'art des autres !',
      kids_2: '\u2696\ufe0f Tous les artistes n\'ont pas \u00e9t\u00e9 consult\u00e9s pour savoir si l\'IA pouvait apprendre d\'eux.',
      kids_3: '\ud83e\udd1d Une bonne IA respecte le travail des gens !',
      youth_1: '\ud83d\udcdc Les donn\u00e9es d\'entra\u00eenement proviennent souvent d\'internet. Les artistes d\u00e9battent : usage \u00e9quitable ou copie ?',
      youth_2: '\ud83c\udfdb\ufe0f Le r\u00e8glement IA de l\'UE exige la transparence : d\'o\u00f9 viennent les donn\u00e9es d\'entra\u00eenement ?',
      youth_3: '\ud83d\udcad Question : \u00e0 qui appartient r\u00e9ellement une image g\u00e9n\u00e9r\u00e9e par IA ?',
      expert_1: '\u26a0\ufe0f LAION-5B a \u00e9t\u00e9 partiellement cr\u00e9\u00e9 sans le consentement des cr\u00e9ateurs \u2014 zone grise juridique.',
      expert_2: '\ud83d\udccb R\u00e8glement IA de l\'UE Art. 52 : obligation d\'\u00e9tiquetage pour le contenu g\u00e9n\u00e9r\u00e9 par IA',
      expert_3: '\ud83d\udd0d Model Cards & Datasheets : bonne pratique pour la transparence ML'
    },
    environment: {
      kids_1: '\u2601\ufe0f Chaque image IA produit un peu de CO\u2082 \u2014 comme conduire une voiture, mais moins !',
      kids_2: '\ud83c\udf31 R\u00e9fl\u00e9chis : cette image vaut-elle l\'\u00e9lectricit\u00e9 ?',
      kids_3: '\u2600\ufe0f L\'\u00e9nergie pour l\'IA vient souvent de centrales \u00e9lectriques \u2014 certaines propres, d\'autres non.',
      youth_1: '\ud83c\udfed Mix \u00e9lectrique allemand : ~400g CO\u2082 par kWh \u2014 \u00e7a s\'accumule !',
      youth_2: '\ud83d\udcc8 {co2}g CO\u2082 pour cette image \u2014 avec 1000 images, cela ferait {totalKg} kg !',
      youth_3: '\ud83d\udca1 Conseil : g\u00e9n\u00e9rez moins d\'images, mais de mani\u00e8re plus r\u00e9fl\u00e9chie \u2014 \u00e9conomise de l\'\u00e9nergie et du CO\u2082.',
      expert_1: '\ud83d\udcca Calcul : {watts}W \u00d7 {seconds}s \u00f7 3600 \u00d7 400g/kWh = {co2}g CO\u2082',
      expert_2: '\ud83d\udd2c \u00c9missions Scope 2 : la localisation du centre de donn\u00e9es est d\u00e9cisive',
      expert_3: '\u26a1 PUE (Power Usage Effectiveness) : surco\u00fbt \u00e9nerg\u00e9tique suppl\u00e9mentaire pour le refroidissement'
    },
    iceberg: {
      drawPrompt: 'La g\u00e9n\u00e9ration IA utilise beaucoup d\'\u00e9nergie. Dessinez des icebergs et voyez ce qui se passe...',
      redraw: 'Redessiner',
      startMelting: 'Commencer la fonte',
      melting: 'L\'iceberg fond...',
      melted: 'Fondu !',
      meltedMessage: '{co2}g CO\u2082 produits',
      comparison: 'Cette quantit\u00e9 de CO\u2082 fait fondre environ {volume} cm\u00b3 de glace arctique.',
      comparisonInfo: '(Chaque tonne de CO\u2082 = environ 6m\u00b3 de glace de mer perdue)',
      gpuPower: 'Consommation \u00e9lectrique de la carte graphique',
      gpuTemp: 'Temp\u00e9rature de la carte graphique',
      co2Info: '\u00c9missions de CO\u2082 li\u00e9es \u00e0 la consommation \u00e9lectrique (bas\u00e9 sur le mix \u00e9nerg\u00e9tique allemand)',
      drawAgain: 'Dessiner plus d\'icebergs...'
    },
    pixel: {
      grafikkarte: 'Carte graphique',
      energieverbrauch: 'Consommation d\'\u00e9nergie',
      co2Menge: 'Quantit\u00e9 de CO\u2082',
      smartphoneComparison: 'Il faudrait \u00e9teindre votre t\u00e9l\u00e9phone pendant {minutes} minutes pour compenser cette consommation de CO\u2082 !',
      clickToProcess: 'Cliquez sur les pixels de donn\u00e9es pour g\u00e9n\u00e9rer une mini image !'
    },
    forest: {
      trees: 'Arbres',
      clickToPlant: 'Cliquez pour planter des arbres ! L\u00e0 o\u00f9 vous plantez un arbre, l\'usine dispara\u00eetra.',
      gameOver: 'La for\u00eat est perdue !',
      treesPlanted: 'Vous avez plant\u00e9 {count} arbres.',
      complete: 'G\u00e9n\u00e9ration termin\u00e9e',
      comparison: 'Un arbre moyen a besoin de {minutes} minutes pour absorber cette quantit\u00e9 de CO\u2082.'
    },
    rareearth: {
      clickToClean: 'Cliquez sur le lac pour retirer les boues toxiques !',
      sludgeRemoved: 'Boues retir\u00e9es',
      environmentHealth: 'Environnement',
      gameOverInactive: 'Vous avez abandonn\u00e9... l\'extraction continue',
      infoBanner: 'L\'extraction de terres rares pour les puces GPU laisse des boues toxiques et d\u00e9truit les \u00e9cosyst\u00e8mes. Vos efforts de nettoyage ne peuvent pas suivre la vitesse d\'extraction.',
      instructionsCooldown: '\u23f3 {seconds} s',
      statsGpu: 'GPU',
      statsHealth: 'Environnement',
      statsSludge: 'Boues retir\u00e9es'
    }
  }
}
