export const es = {
  app: {
    title: 'UCDCAE AI LAB',
    subtitle: 'Transformaciones creativas con IA'
  },
  form: {
    inputLabel: 'Tu texto',
    inputPlaceholder: 'p. ej. Una flor en la pradera',
    schemaLabel: 'Estilo de transformación',
    executeModeLabel: 'Modo de ejecución',
    safetyLabel: 'Nivel de seguridad',
    generateButton: 'Generar'
  },
  schemas: {
    dada: 'Dada (Aleatorio y absurdo)',
    bauhaus: 'Bauhaus (Geométrico)',
    stillepost: 'Teléfono descompuesto (Iterativo)'
  },
  executionModes: {
    eco: 'Eco (Rápido)',
    fast: 'Fast (Equilibrado)',
    best: 'Best (Calidad)'
  },
  safetyLevels: {
    kids: 'Niños',
    youth: 'Jóvenes',
    adult: 'Adultos',
    research: 'Investigación'
  },
  stages: {
    pipeline_starting: 'Iniciando Pipeline',
    translation_and_safety: 'Traducción y seguridad',
    interception: 'Transformación',
    pre_output_safety: 'Seguridad de salida',
    media_generation: 'Generación de imagen',
    completed: 'Completado'
  },
  status: {
    idle: 'Listo',
    executing: 'Pipeline en ejecución...',
    connectionSlow: 'Conexión lenta, reintentando...',
    completed: '¡Pipeline completado!',
    error: 'Ocurrió un error'
  },
  entities: {
    input: 'Entrada',
    translation: 'Traducción',
    safety: 'Verificación de seguridad',
    interception: 'Transformación',
    safety_pre_output: 'Seguridad de salida',
    media: 'Imagen generada'
  },
  properties: {
    chill: 'tranquilo',
    chaotic: 'caótico',
    narrative: 'contar historias',
    algorithmic: 'seguir reglas',
    historical: 'historia',
    contemporary: 'presente',
    explore: 'probar IA',
    create: 'crear arte',
    playful: 'lúdico',
    serious: 'serio'
  },
  phase2: {
    title: 'Entrada de Prompt',
    userInput: 'Tu entrada',
    yourInput: 'Tu entrada',
    yourIdea: 'Tu idea: ¿QUÉ debería tratar esto?',
    rules: 'Tus reglas: ¿CÓMO debe implementarse tu idea?',
    yourInstructions: 'Tus instrucciones',
    what: 'QUÉ',
    how: 'CÓMO',
    userInputPlaceholder: 'p. ej. Una flor en la pradera',
    inputPlaceholder: 'Tu texto aparece aquí...',
    metaPrompt: 'Instrucción artística',
    instruction: 'Instrucción',
    transformation: 'Transformación artística',
    metaPromptPlaceholder: 'Describe la transformación...',
    result: 'Resultado',
    expectedResult: 'Resultado esperado',
    execute: 'Ejecutar Pipeline',
    executing: 'Ejecutando...',
    transforming: 'LLM transformando...',
    startTransformation: 'Iniciar transformación',
    letsGo: '¡Vamos!',
    modified: 'Modificado',
    reset: 'Restablecer',
    loadingConfig: 'Cargando configuración...',
    loadingMetaPrompt: 'Cargando meta-prompt...',
    errorLoadingConfig: 'Error al cargar la configuración',
    errorLoadingMetaPrompt: 'Error al cargar el meta-prompt',
    threeForces: '3 fuerzas trabajando juntas',
    twoForces: 'QUÉ + CÓMO → LLM → Resultado',
    yourPrompt: 'Tu Prompt:',
    writeYourText: 'Escribe tu texto...',
    examples: 'Ejemplos',
    estimatedTime: '~12 segundos',
    stage12Time: '~5-10 segundos',
    willAppearAfterExecution: 'Aparecerá después de la ejecución...',
    back: 'Atrás',
    retry: 'Reintentar',
    transformedPrompt: 'Prompt transformado',
    notYetTransformed: 'Aún no transformado...',
    transform: 'Transformar',
    reTransform: 'Intentar de nuevo de forma diferente',
    startAI: 'IA, procesa mi entrada',
    aiWorking: 'La IA está trabajando...',
    continueToMedia: 'Continuar a generación de imagen',
    readyForMedia: 'Listo para generación de imagen',
    stage1: 'Etapa 1: Traducción + Seguridad...',
    stage2: 'Etapa 2: Transformación...',
    selectMedia: 'Elige tu medio:',
    mediaImage: 'Imagen',
    mediaAudio: 'Audio',
    mediaVideo: 'Video',
    media3D: '3D',
    comingSoon: 'Próximamente',
    generateMedia: '¡Iniciar!'
  },
  phase3: {
    generating: 'Se está generando la imagen...',
    generatingHint: '~30 segundos'
  },
  common: {
    back: 'Atrás',
    loading: 'Cargando...',
    error: 'Error',
    retry: 'Reintentar',
    cancel: 'Cancelar',
    checkingSafety: 'Verificando...'
  },
  gallery: {
    title: 'Favoritos',
    empty: 'Aún no hay favoritos',
    favorite: 'Agregar a favoritos',
    unfavorite: 'Eliminar de favoritos',
    continue: 'Continuar editando',
    restore: 'Restaurar sesión',
    viewMine: 'Mis favoritos',
    viewAll: 'Todos los favoritos'
  },
  settings: {
    authRequired: 'Autenticación requerida',
    authPrompt: 'Por favor, ingresa la contraseña para acceder a la configuración:',
    passwordPlaceholder: 'Ingresar contraseña...',
    authenticate: 'Iniciar sesión',
    authenticating: 'Autenticando...',
    // Admin page
    title: 'Administración',
    tabs: {
      export: 'Datos de investigación',
      config: 'Configuración',
      demos: 'Demo de minijuegos',
      matrix: 'Matriz de modelos'
    },
    loading: 'Cargando configuración...',
    presets: {
      title: 'Presets de modelos',
      help: 'Usa la pestaña <strong>Matriz de modelos</strong> para ver todos los presets disponibles y aplicarlos con un clic.',
      openMatrix: 'Abrir Matriz de modelos'
    },
    testingTools: {
      title: 'Herramientas de prueba para educadores',
      help: 'Prueba y explora los minijuegos y animaciones pedagógicas antes de usarlos con los estudiantes.',
      openPreview: 'Abrir vista previa de minijuegos',
      pixelEditor: 'Editor de plantillas de píxeles',
      includes: 'Incluye: Animación de píxeles, Derretimiento de iceberg, Juego del bosque, Tierras raras'
    },
    general: {
      title: 'Configuración general',
      uiMode: 'Modo de interfaz',
      uiModeHelp: 'Nivel de complejidad de la interfaz',
      kids: 'Niños (8–12)',
      youth: 'Jóvenes (13–17)',
      expert: 'Experto',
      safetyLevel: 'Nivel de seguridad',
      defaultLanguage: 'Idioma predeterminado',
      germanDe: 'Alemán (de)',
      englishEn: 'Inglés (en)',
      turkishTr: 'Turco (tr)',
      koreanKo: 'Coreano (ko)',
      ukrainianUk: 'Ucraniano (uk)',
      frenchFr: 'Francés (fr)',
      spanishEs: 'Español (es)'
    },
    safety: {
      kidsTitle: 'Niños (8–12)',
      kidsDesc: 'Todos los filtros activos: §86a, DSGVO, Protección de menores (parámetros para niños), verificación VLM de imágenes',
      youthTitle: 'Jóvenes (13–17)',
      youthDesc: 'Todos los filtros activos: §86a, DSGVO, Protección de menores (parámetros para jóvenes), verificación VLM de imágenes',
      adultTitle: 'Adultos',
      adultDesc: '§86a + DSGVO activos. Sin protección de menores, sin verificación VLM de imágenes.',
      researchTitle: 'Modo de investigación',
      researchDesc: 'NINGÚN filtro de seguridad activo. Solo permitido para instituciones de investigación en el contexto de proyectos científicos.'
    },
    safetyModels: {
      title: 'Modelos de seguridad locales',
      help: 'Local vía Ollama — los nombres de personas y las verificaciones de seguridad nunca salen del sistema',
      safetyModel: 'Modelo de seguridad',
      safetyModelHelp: 'Modelo de guardia para seguridad de contenido (§86a, protección de menores)',
      dsgvoModel: 'Modelo DSGVO-Verify',
      dsgvoModelHelp: 'Modelo de propósito general para verificación NER DSGVO (no es un modelo de guardia)',
      vlmModel: 'Modelo VLM de seguridad',
      vlmModelHelp: 'Modelo de visión para verificación de seguridad de imágenes generadas (niños/jóvenes)',
      fast: 'rápido, mínimo',
      recommended: 'recomendado'
    },
    dsgvo: {
      title: 'Advertencia DSGVO',
      notCompliant: 'Los siguientes modelos <strong>NO cumplen con DSGVO</strong> (datos procesados fuera de la UE):',
      compliantHint: 'Opciones conformes con DSGVO:'
    },
    models: {
      title: 'Configuración de modelos',
      help: 'Identificadores de modelo con prefijo de proveedor: local/, mistral/, anthropic/, openai/, openrouter/',
      matrixAdvised: 'Se recomienda usar la Matriz de modelos. Sin embargo, puedes configurar tus ajustes aquí libremente.',
      ollamaAvailable: '{count} modelos Ollama disponibles (escribe o selecciona del menú)',
      stage1Text: 'Etapa 1 - Modelo de texto',
      stage1Vision: 'Etapa 1 - Modelo de visión',
      stage2Interception: 'Etapa 2 - Modelo de intercepción',
      stage2Optimization: 'Etapa 2 - Modelo de optimización',
      stage3: 'Etapa 3 - Modelo de traducción/seguridad',
      stage4Legacy: 'Etapa 4 - Modelo legacy',
      chatHelper: 'Modelo de asistente de chat',
      imageAnalysis: 'Modelo de análisis de imágenes',
      coding: 'Generación de código (Tone.js, p5.js)'
    },
    api: {
      title: 'Configuración de API',
      llmProvider: 'Proveedor de LLM',
      localFramework: 'Framework de LLM local',
      externalProvider: 'Proveedor de LLM externo',
      cloudProvider: 'Proveedor de LLM en la nube - requiere clave API',
      noneLocal: 'Ninguno (solo local, DSGVO)',
      mistralEu: 'Mistral AI (basado en la UE, DSGVO)',
      anthropicDirect: 'Anthropic Direct API (NO DSGVO)',
      openaiDirect: 'OpenAI Direct API (NO DSGVO)',
      openrouterDirect: 'OpenRouter (NO DSGVO, enrutamiento UE disponible)',
      mistralInfo: 'Mistral AI (basado en la UE)',
      mistralDsgvo: 'Conforme con DSGVO (infraestructura UE)',
      anthropicInfo: 'Anthropic Direct API',
      anthropicNotDsgvo: 'NO conforme con DSGVO',
      anthropicWarning: 'Datos procesados fuera de la UE. Usar solo en contextos no educativos.',
      openaiInfo: 'OpenAI Direct API',
      openaiNotDsgvo: 'NO conforme con DSGVO (basado en EE.UU.)',
      openaiWarning: 'Datos procesados en Estados Unidos. Usar solo en contextos no educativos.',
      openrouterInfo: 'OpenRouter',
      openrouterNotDsgvo: 'NO conforme con DSGVO (empresa estadounidense)',
      openrouterWarning: 'Enrutamiento de servidor UE configurable en los ajustes de OpenRouter, pero la empresa es estadounidense.',
      storedIn: 'Almacenado en',
      currentKey: 'Actual'
    },
    save: {
      saveApply: 'Guardar y aplicar',
      saving: 'Guardando...',
      applying: 'Aplicando...',
      success: 'Configuración guardada y aplicada',
      presetApplied: 'Preset aplicado: {preset}'
    }
  },
  pipeline: {
    yourInput: 'Tu entrada',
    result: 'Resultado',
    generatedMedia: 'Imagen generada'
  },
  landing: {
    subtitlePrefix: 'Plataforma de experimentación pedagógico-artística de la',
    subtitleSuffix: 'para el uso explorativo de IA generativa en la educación mediática cultural-estética',
    research: '',
    features: {
      textTransformation: {
        title: 'Transformación de texto',
        description: 'Cambio de perspectiva a través de IA — tu prompt se transforma mediante lentes artístico-pedagógicas en imagen, video y sonido.'
      },
      imageTransformation: {
        title: 'Transformación de imagen',
        description: 'Transforma imágenes a través de diferentes modelos y perspectivas en nuevas imágenes y videos.'
      },
      multiImage: {
        title: 'Fusión de imágenes',
        description: 'Combina múltiples imágenes y fusiónalas en nuevas composiciones mediante modelos de IA.'
      },
      canvas: {
        title: 'Canvas Workflow',
        description: 'Composición visual de flujos de trabajo — conecta módulos mediante arrastrar y soltar en pipelines de IA personalizados.'
      },
      music: {
        title: 'Generación de música',
        description: 'Creación musical con IA mediante letras, etiquetas y control estilístico.'
      },
      latentLab: {
        title: 'Latent Lab',
        description: 'Investigación del espacio vectorial — surrealización, eliminación de dimensiones, interpolación de embeddings.'
      }
    }
  },
  research: {
    locked: 'Solo disponible en modo de investigación',
    lockedHint: 'Requiere nivel de seguridad "Adulto" o "Investigación" (config.py)',
    complianceTitle: 'Aviso del modo de investigación',
    complianceWarning: 'En el modo de investigación, no hay filtros de seguridad activos para prompts ni imágenes generadas. Pueden producirse resultados inesperados o inapropiados.',
    complianceAge: 'Este modo no se recomienda para personas menores de 16 años.',
    complianceConfirm: 'Confirmo que he comprendido los avisos',
    complianceCancel: 'Cancelar',
    complianceProceed: 'Continuar'
  },
  presetOverlay: {
    title: 'Elegir perspectiva',
    close: 'Cerrar'
  },
  imageUpload: {
    clickHere: 'Haz clic aquí',
    orDragImage: 'o arrastra una imagen aquí',
    formatHint: 'PNG, JPG, WEBP (máx. 10MB)',
    invalidFormat: 'Formato de archivo no válido. Solo se permiten PNG, JPG y WEBP.',
    fileTooLarge: 'Archivo demasiado grande. Máximo: {max}MB',
    uploadFailed: 'Error al subir',
    infoOriginal: 'Original:',
    infoSize: 'Tamaño:'
  },
  mediaInput: {
    choosePreset: 'Elegir perspectiva',
    translateToEnglish: 'Traducir al inglés',
    copy: 'Copiar',
    paste: 'Pegar',
    delete: 'Eliminar',
    loading: 'Cargando...',
    contentBlocked: 'Contenido bloqueado'
  },
  nav: {
    about: 'Acerca de',
    impressum: 'Aviso legal',
    privacy: 'Privacidad',
    docs: 'Documentación',
    language: 'Cambiar idioma',
    settings: 'Configuración',
    canvas: 'Canvas Workflow'
  },
  canvas: {
    title: 'Canvas Workflow',
    newWorkflow: 'Nuevo flujo de trabajo',
    importWorkflow: 'Importar',
    exportWorkflow: 'Exportar',
    execute: 'Ejecutar',
    ready: 'Listo',
    errors: 'errores',
    discardWorkflow: '¿Descartar flujo de trabajo actual?',
    importError: 'Error al importar archivo',
    selectTransformation: 'Seleccionar transformación',
    selectOutput: 'Seleccionar modelo de salida',
    search: 'Buscar...',
    noResults: 'No se encontraron resultados',
    dragHint: 'Haz clic o arrastra módulos al canvas',
    editNameHint: '(doble clic para editar)',
    modules: 'Módulos',
    toggleSidebar: 'Alternar barra lateral',
    dsgvoTooltip: 'Los flujos de trabajo de Canvas pueden usar APIs externas de LLM. El cumplimiento de DSGVO es responsabilidad del usuario.',
    batchExecute: 'Ejecución por lotes',
    batchExecution: 'Ejecución por lotes',
    batchAbort: 'Cancelar lote',
    abort: 'Cancelar',
    cancel: 'Cancelar',
    loading: 'Cargando...',
    executingWorkflow: 'Ejecutando flujo de trabajo...',
    starting: 'Iniciando...',
    nodes: 'nodos',
    batchRunCount: 'Número de ejecuciones',
    batchUseSeed: 'Usar Seed base',
    batchBaseSeed: 'Seed base',
    batchSeedHint: 'Cada ejecución: seed + índice',
    batchStart: 'Iniciar lote',
    stage: {
      configSelectPlaceholder: 'Seleccionar...',
      evaluationCriteriaFallback: 'Criterios de evaluación...',
      feedbackInputTitle: 'Entrada de retroalimentación',
      deleteTitle: 'Eliminar',
      selectLlmPlaceholder: 'Seleccionar LLM...',
      resizeTitle: 'Redimensionar',
      input: {
        promptPlaceholder: 'Tu prompt...'
      },
      imageInput: {
        uploadLabel: 'Subir imagen'
      },
      interception: {
        contextPromptLabel: 'Prompt de contexto',
        contextPromptPlaceholder: 'Instrucciones de transformación...'
      },
      translation: {
        translationPromptLabel: 'Prompt de traducción',
        translationPromptPlaceholder: 'Instrucciones de traducción...'
      },
      modelAdaption: {
        targetModelLabel: 'Modelo objetivo',
        noAdaptionOption: 'Sin adaptación',
        videoModelsOption: 'Modelos de video',
        audioModelsOption: 'Modelos de audio'
      },
      comparisonEvaluator: {
        criteriaLabel: 'Criterios de comparación',
        criteriaPlaceholder: 'p. ej. Comparar por originalidad, claridad, detalle...',
        infoText: 'Conecta hasta 3 salidas de texto'
      },
      seed: {
        modeLabel: 'Modo',
        modeFixed: 'Fijo',
        modeRandom: 'Aleatorio',
        valueLabel: 'Valor',
        baseLabel: 'Base'
      },
      resolution: {
        customOption: 'Personalizado',
        widthLabel: 'Ancho',
        heightLabel: 'Alto'
      },
      collector: {
        emptyText: 'Esperando ejecución...'
      },
      evaluation: {
        typeLabel: 'Tipo de evaluación',
        typeCreativity: 'Creatividad',
        typeQuality: 'Calidad',
        typeCustom: 'Personalizado',
        criteriaLabel: 'Criterios de evaluación',
        outputTypeLabel: 'Tipo de salida',
        outputCommentary: 'Comentario + Binario',
        outputScore: 'Comentario + Puntuación + Binario',
        outputAll: 'Todo',
        evalPassTitle: 'Aprobado (adelante)',
        evalFailTitle: 'Retroalimentación (atrás)',
        evalCommentaryTitle: 'Comentario (adelante)'
      },
      imageEvaluation: {
        visionModelPlaceholder: 'Seleccionar modelo de visión...',
        frameworkLabel: 'Marco de análisis',
        frameworkPanofsky: 'Historia del arte (Panofsky)',
        frameworkEducational: 'Teoría educativa',
        frameworkEthical: 'Ético',
        frameworkCritical: 'Crítico/Decolonial',
        frameworkCustom: 'Personalizado',
        customPromptLabel: 'Prompt de análisis',
        customPromptPlaceholder: 'Describe cómo debe analizarse la imagen...'
      },
      display: {
        imageAlt: 'Vista previa',
        emptyText: 'Vista previa (después de la ejecución)'
      }
    }
  },
  about: {
    title: 'Acerca del UCDCAE AI LAB',
    intro: 'El UCDCAE AI LAB es una plataforma de experimentación pedagógico-artística de la UNESCO Chair in Digital Culture and Arts in Education para el uso explorativo de inteligencia artificial generativa en la educación mediática cultural-estética. Fue desarrollada dentro de los proyectos AI4ArtsEd y COMeARTS.',
    project: {
      title: 'El proyecto',
      description: 'La IA está transformando la sociedad y el mundo laboral; cada vez más se convierte en tema de educación. El proyecto explora oportunidades, condiciones y límites del uso pedagógico de la inteligencia artificial (IA) en entornos de educación cultural sensibles a la diversidad.',
      paragraph2: 'En tres subproyectos — Pedagogía General (TPap), Informática (TPinf) y Educación Artística (TPkp) — la investigación práctica pedagógica de IA orientada a la creatividad y la concepción y programación informática de IA se entrelazan en estrecha cooperación. Desde el inicio, el proyecto involucra sistemáticamente a profesionales artístico-pedagógicos en el proceso de diseño; actúa como puente entre la implementación pedagógico-práctica profesional (relacionada con la calidad, estética, ética y valores) por un lado y el proceso de implementación y entrenamiento del subproyecto informático por el otro.',
      paragraph3: 'Un proceso de diseño participativo de aproximadamente dos años tiene como objetivo producir una tecnología de IA de código abierto que explore en qué medida los sistemas de IA pueden incorporar principios artístico-pedagógicos en su nivel estructural bajo condiciones reales favorables.',
      paragraph4: 'El enfoque está en a) la futura aplicabilidad y valor agregado de tecnologías altamente innovadoras para la educación cultural, b) el alcance y los límites de la alfabetización en IA de docentes y estudiantes, y c) la cuestión general de la evaluabilidad y valoración de la transformación de entornos pedagógicos por actores complejos no humanos en términos de ética pedagógica y evaluación tecnológica.',
      moreInfo: 'Más información:'
    },
    subproject: {
      title: 'Subproyecto "Pedagogía General"',
      description: 'El subproyecto "Pedagogía General" investiga posibilidades y límites de un proceso de diseño artístico-pedagógico de IA basado en investigación práctica participativa en el marco de la pregunta de investigación conjunta del proyecto colaborativo. Para ello, realiza una serie de investigaciones, análisis, talleres de expertos y espacios abiertos en el primer año del proyecto. La fase posterior del proyecto, diseñada como un ciclo de retroalimentación en varios ciclos, explora el uso de un prototipo con profesionales pedagógicos y artistas-educadores, particularmente en la educación cultural no formal, como un proceso educativo relacional y colectivo transformador.'
    },
    team: {
      title: 'Equipo',
      projectLead: 'Dirección del proyecto',
      leadName: 'Prof. Dr. Benjamin Jörissen',
      leadInstitute: 'Instituto de Educación',
      leadChair: 'Cátedra de Educación con enfoque en Cultura y Educación Estética',
      leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
      researcher: 'Investigador/a asociado/a',
      researcherName: 'Vanessa Baumann',
      researcherInstitute: 'Instituto de Educación',
      researcherChair: 'Cátedra de Educación con enfoque en Cultura y Educación Estética',
      researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
    },
    funding: {
      title: 'Financiado por'
    }
  },
  legal: {
    impressum: {
      title: 'Aviso legal',
      publisher: 'Editor',
      represented: 'Representada por el presidente',
      responsible: 'Responsable del contenido',
      authority: 'Autoridad supervisora',
      moreInfo: 'Información adicional',
      moreInfoText: 'Aviso legal completo de FAU:',
      funding: 'Financiado por'
    },
    privacy: {
      title: 'Política de privacidad',
      notice: 'Aviso: El contenido generado se almacena en el servidor con fines de investigación. No se recopilan datos de usuario ni de IP. Las imágenes subidas no se almacenan.',
      usage: 'El uso de esta plataforma está exclusivamente permitido para socios de cooperación registrados del UCDCAE AI LAB. Se aplican los acuerdos de protección de datos realizados en este contexto. Si tienes preguntas, contacta a vanessa.baumann@fau.de.'
    }
  },
  docs: {
    title: 'Documentación y guía',
    intro: {
      title: 'Bienvenida',
      content: 'Experimentos creativos con transformaciones de IA.'
    },
    gettingStarted: {
      title: 'Primeros pasos',
      step1: 'Selecciona propiedades de los cuadrantes',
      step2: 'Ingresa texto o imagen',
      step3: 'Inicia la transformación'
    },
    modes: {
      title: 'Modos',
      mode1: { name: 'Directo', desc: 'Experimentos rápidos' },
      mode2: { name: 'Texto', desc: 'Transformaciones basadas en texto' },
      mode3: { name: 'Imagen', desc: 'Procedimientos basados en imagen' }
    },
    support: {
      title: 'Soporte',
      content: 'Para preguntas:'
    },
    wikipedia: {
      title: 'Investigación en Wikipedia',
      subtitle: 'Conocimiento sobre el mundo como parte de procesos artísticos',
      feature: 'Los procesos artísticos requieren no solo conocimiento estético, sino también conocimiento sobre hechos del mundo. La IA investiga en Wikipedia durante la transformación para encontrar información factual.',
      languages: 'Se admiten más de 70 idiomas',
      languagesDesc: 'La IA elige automáticamente la Wikipedia en el idioma apropiado para cada tema:',
      examples: {
        nigeria: 'Tema sobre Nigeria → Hausa, Yoruba, Igbo o inglés',
        india: 'Tema sobre India → Hindi, Tamil, Bengali u otros idiomas regionales',
        indigenous: 'Culturas indígenas → Quechua, Māori, Inuktitut, etc.'
      },
      why: 'Transparencia: ¿Qué sabe la IA?',
      whyDesc: 'El sistema muestra todos los intentos de investigación: tanto los artículos encontrados (como enlaces clicables) como los términos para los que no se encontró nada. Esto hace visible lo que la IA cree que sabe — y lo que no.',
      culturalRespect: 'Invitación a investigar por tu cuenta',
      culturalRespectDesc: 'Los enlaces de Wikipedia mostrados son una invitación a aprender más por tu cuenta. Haz clic en los enlaces para verificar las fuentes y ampliar tu propio conocimiento.',
      limitations: 'La investigación de la IA es una ayuda, no un sustituto de tu propio estudio del tema.'
    }
  },
  multiImage: {
    image1Label: 'Imagen 1',
    image2Label: 'Imagen 2 (opcional)',
    image3Label: 'Imagen 3 (opcional)',
    contextLabel: 'Describe lo que quieres hacer con las imágenes',
    contextPlaceholder: 'p. ej. Inserta la casa de la imagen 2 y el caballo de la imagen 3 en la imagen 1. Conserva los colores y el estilo de la imagen 1.',
    modeTitle: 'Múltiples imágenes → Imagen',
    selectConfig: 'Elige tu modelo:',
    generating: 'Fusionando imágenes...'
  },
  imageTransform: {
    imageLabel: 'Tu imagen',
    contextLabel: 'Describe lo que quieres cambiar en la imagen',
    contextPlaceholder: 'p. ej. Transfórmala en una pintura al óleo... Hazla más colorida... Agrega un atardecer...'
  },
  textTransform: {
    inputLabel: '¿Tu idea = QUÉ?',
    inputTooltip: 'Ingresa de qué debería tratar tu creación.',
    inputPlaceholder: 'p. ej. Un festival en mi calle: ...',
    contextLabel: '¿Tus reglas = CÓMO?',
    contextTooltip: '¡Ingresa cómo debe presentarse tu idea, o haz clic en el ícono del círculo!',
    contextPlaceholder: '¡p. ej. Describe todo como lo perciben los pájaros en los árboles!',
    resultLabel: 'Idea + Reglas = Prompt',
    resultPlaceholder: 'El prompt aparecerá después de hacer clic en iniciar (o ingresa tu propio texto)',
    optimizedLabel: 'Prompt optimizado para el modelo',
    optimizedPlaceholder: 'El prompt optimizado aparecerá después de seleccionar el modelo.'
  },
  training: {
    info: {
      title: 'Acerca del entrenamiento LoRA',
      studioDescription: 'Entrena modelos LoRA personalizados para Stable Diffusion 3.5 Large con tus propias imágenes.',
      description: 'Este entrenamiento integrado está diseñado para pruebas rápidas.',
      limitations: 'Limitaciones',
      limitationDuration: 'El entrenamiento tarda de 1 a 3 horas',
      limitationBlocking: 'Bloquea la generación de imágenes durante el entrenamiento',
      limitationConfig: 'Opciones de configuración limitadas',
      showMore: 'Más información',
      showLess: 'Mostrar menos'
    },
    placeholders: {
      projectName: 'p. ej. Nuestro edificio escolar',
      triggerWords: 'p. ej. nuestro_edificio_escolar, patio, aula'
    },
    labels: {
      projectName: 'Nombre del proyecto',
      triggerWords: 'Palabras clave',
      triggerHelp: 'Etiquetas separadas por comas. La primera = activador principal, el resto = etiquetas adicionales por imagen.',
      images: 'Imágenes de entrenamiento (se recomiendan 10–50)',
      dropZone: 'Haz clic o arrastra imágenes aquí',
      imagesSelected: '{count} imágenes seleccionadas',
      logs: 'Registros de entrenamiento',
      waiting: 'Esperando que inicie el entrenamiento...'
    },
    buttons: {
      start: 'Iniciar entrenamiento',
      stop: 'Detener',
      inProgress: 'Entrenamiento en curso...',
      delete: 'Eliminar archivos del proyecto (DSGVO)',
      cancel: 'Cancelar'
    },
    vram: {
      title: 'Verificación de VRAM de GPU',
      checking: 'Verificando VRAM...',
      used: 'en uso',
      free: 'libre',
      notEnough: 'No hay suficiente VRAM libre para el entrenamiento (se necesitan {gb} GB).',
      clearQuestion: '¿Liberar VRAM para continuar?',
      enough: 'Hay suficiente VRAM disponible para el entrenamiento.',
      clearing: 'Liberando VRAM...',
      newFree: 'Nuevo disponible',
      clearBtn: 'Liberar VRAM de ComfyUI + Ollama'
    }
  },
  safetyBadges: {
    '§86a': '§86a',
    '86a_filter': '§86a',
    age_filter: 'Filtro de edad',
    dsgvo_ner: 'DSGVO',
    dsgvo_llm: 'DSGVO',
    translation: '\u2192 EN',
    fast_filter: 'Contenido',
    llm_context_check: 'Contenido (LLM)',
    llm_safety_check: 'Protección de menores',
    llm_check_failed: 'Verificación fallida',
    disabled: '\u2014'
  },
  safetyBlocked: {
    vlm: 'Tu prompt estaba bien, pero la imagen generada fue marcada como inapropiada por una IA de análisis de imágenes. Esto puede ocurrir — la generación de imágenes no siempre es predecible. ¡Simplemente inténtalo de nuevo, cada generación es diferente!',
    para86a: 'Tu prompt fue bloqueado porque contiene símbolos o términos prohibidos según la ley alemana (§86a StGB). Esta regla nos protege a todos del odio y la violencia. ¡Prueba con un tema diferente!',
    dsgvo: 'Tu prompt fue bloqueado porque contiene algo que parece un nombre de persona. Esto está protegido por el Reglamento General de Protección de Datos (DSGVO). Usa descripciones como "una chica" o "un anciano" en lugar de nombres.',
    kids: 'Tu prompt fue bloqueado por el filtro de seguridad infantil. Algunos términos no son adecuados para niños porque pueden ser atemorizantes o perturbadores. ¡Intenta describir tu idea con palabras más amigables!',
    youth: 'Tu prompt fue bloqueado por el filtro de protección de menores. Algunos contenidos tampoco son adecuados para adolescentes. ¡Intenta reformular tu idea!',
    generic: 'Tu prompt fue bloqueado por el sistema de seguridad. El sistema te protege de contenido inapropiado. ¡Prueba con otra formulación!',
    inputImage: 'La imagen subida fue marcada como inapropiada por una IA de análisis de imágenes. Por favor, usa una imagen diferente.',
    vlmSaw: 'La IA de imágenes vio',
    systemUnavailable: 'El sistema de seguridad (Ollama) no responde, por lo que no es posible continuar con el procesamiento. Por favor, contacta al administrador del sistema.',
    suggestionLoading: 'Espera, tengo una idea...',
    suggestionError: '¡No pude generar una sugerencia en este momento. Simplemente inténtalo de nuevo con otras palabras!'
  },
  splitCombine: {
    infoTitle: 'Split & Combine - Fusión de vectores semánticos',
    infoDescription: 'Este flujo de trabajo fusiona dos prompts a nivel de vectores semánticos. El resultado no es una simple mezcla, sino una conexión matemática más profunda de los espacios de significado.',
    purposeTitle: 'Propósito pedagógico',
    purposeText: 'Explora cómo los modelos de IA representan el significado como espacios numéricos. ¿Qué sucede cuando fusionamos matemáticamente conceptos diferentes?',
    techTitle: 'Detalles técnicos',
    techText: 'Modelo: SD3.5 Large | Codificador: DualCLIP (CLIP-G + T5-XXL)'
  },
  partialElimination: {
    infoTitle: 'Eliminación parcial - Deconstrucción vectorial',
    infoDescription: 'Este flujo de trabajo manipula específicamente partes del vector semántico. Al eliminar ciertas dimensiones, podemos observar qué aspectos del significado se pierden.',
    purposeTitle: 'Propósito pedagógico',
    purposeText: 'Comprende cómo se codifica el significado a través de diferentes dimensiones del espacio vectorial. ¿Qué queda cuando "apagamos" partes?',
    techTitle: 'Detalles técnicos',
    techText: 'Modelo: SD3.5 Large | Codificador: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
    encoderLabel: 'Codificador de texto',
    modeLabel: 'Modo de eliminación',
    dimensionRange: 'Rango de dimensiones',
    selected: 'Seleccionado',
    dimensions: 'Dimensiones',
    emptyTitle: 'Esperando generación...',
    emptySubtitle: 'Los resultados aparecerán aquí',
    referenceLabel: 'Imagen de referencia',
    referenceDesc: 'Salida sin manipular (original)',
    innerLabel: 'Rango interior eliminado',
    outerLabel: 'Rango exterior eliminado'
  },
  surrealizer: {
    infoTitle: 'Surrealizer — Extrapolación más allá de lo conocido',
    infoDescription: 'Dos "cerebros" de IA leen tu texto: CLIP-L entiende el lenguaje a través de imágenes, T5 lo entiende de forma puramente lingüística. El control deslizante no simplemente mezcla entre ambos — empuja la imagen mucho más allá de lo que T5 solo produciría. La IA debe entonces interpretar vectores que nunca encontró durante el entrenamiento. El resultado: alucinaciones de IA — imágenes que ningún prompt podría producir directamente.',
    purposeTitle: 'El control deslizante',
    purposeText: 'α < 0: CLIP-L amplificado, T5 negado — las 3328 dimensiones superiores (donde CLIP-L tiene relleno de ceros) reciben vectores T5 invertidos. Los patrones de atención cruzada en el transformer se invierten: alucinaciones guiadas visualmente. ◆ α = 0: CLIP-L puro — imagen normal. ◆ α = 1: T5-XXL puro — todavía normal, pero calidad diferente. ◆ α > 1: extrapolación más allá de T5. Con α = 20 la fórmula empuja el embedding 19× más allá de T5 hacia un espacio vectorial inexplorado — alucinaciones guiadas lingüísticamente. ◆ Punto óptimo: α = 15–35.',
    techTitle: 'Cómo funciona',
    techText: 'Tu prompt se envía a través de dos codificadores por separado: CLIP-L (entrenado visualmente, 77 tokens, 768 dimensiones → rellenado a 4096) y T5-XXL (entrenado lingüísticamente, 512 tokens, 4096 dimensiones). Las primeras 77 posiciones de tokens se fusionan: (1-α)·CLIP-L + α·T5. Los tokens T5 restantes (78–512) permanecen sin cambios como ancla semántica — mantienen la imagen ligada a tu texto sin importar cuán extremo sea α. Con α > 1 esto no es mezcla sino extrapolación: vectores que ningún entrenamiento jamás produjo. Con α < 0, T5 se niega y CLIP-L se amplifica — alucinaciones cualitativamente diferentes porque los patrones de atención cruzada en el transformer se invierten.',
    sliderLabel: 'Extrapolación (α)',
    sliderNormal: 'normal',
    sliderWeird: 'extraño',
    sliderCrazy: 'loco',
    sliderExtremeWeird: 'súper extraño',
    sliderExtremeCrazy: 'súper loco',
    sliderHint: "α<0: más allá de CLIP {'|'} α=0: CLIP puro {'|'} α=1: T5 puro {'|'} α>1: más allá de T5",
    expandLabel: 'Expandir prompt para T5',
    expandSuggest: 'Prompt corto detectado — la expansión T5 mejora significativamente los resultados con pocas palabras.',
    expandHint: 'Tu prompt tiene pocas palabras (~{count} tokens CLIP). Para alucinaciones óptimas, la IA puede expandir narrativamente el contexto T5.',
    expandActive: 'Expandiendo prompt...',
    expandResultLabel: 'Expansión T5 (solo codificador T5)',
    advancedLabel: 'Configuración avanzada',
    negativeLabel: 'Prompt negativo',
    negativeHint: 'Extrapolado con el mismo α. Determina de QUÉ se aleja la imagen en la extrapolación — diferentes negativos producen estéticas fundamentalmente diferentes.',
    cfgLabel: 'Escala CFG',
    cfgHint: 'Classifier-Free Guidance: fuerza de la influencia del prompt. Mayor = efecto más fuerte, menos variación.'
  },
  musicGeneration: {
    infoTitle: 'Generación de música',
    infoDescription: 'Crea música a partir de texto y etiquetas de estilo. La IA genera melodías, ritmos y armonías basándose en tus letras y especificaciones de género.',
    purposeTitle: 'Propósito pedagógico',
    purposeText: '¿Explora cómo la IA interpreta conceptos musicales. ¿Cómo afecta la elección de palabras en las letras a la melodía?',
    lyricsLabel: 'Letras (Texto)',
    lyricsPlaceholder: '[Verso]\nTus letras aquí...\n\n[Estribillo]\nEstribillo...',
    tagsLabel: 'Etiquetas de estilo',
    tagsPlaceholder: 'pop, piano, alegre, voz femenina, 120bpm',
    selectModel: 'Elige un modelo de música:',
    generate: 'Generar música',
    generating: 'Generando música...'
  },
  musicGen: {
    simpleMode: 'Simple',
    advancedMode: 'Avanzado',
    lyricsLabel: 'Letras',
    lyricsPlaceholder: 'Escribe las letras de tu canción con marcadores de estructura como [Verso], [Estribillo], [Puente]...\n\nEjemplo:\n[Verso]\nde doo doo doo\nde blaa blaa blaa\n\n[Estribillo]\nes todo lo que quiero cantarte',
    tagsLabel: 'Etiquetas de estilo',
    tagsPlaceholder: 'Género, estado de ánimo, instrumentos...\n\nEjemplo: ska, agresivo, alegre, alta definición, trío de bajo y saxofón',
    refineButton: 'Refinar letras y etiquetas',
    refinedLyricsLabel: 'Letras refinadas',
    refinedLyricsPlaceholder: 'Tus letras refinadas aparecerán aquí...',
    refiningLyricsMessage: 'La IA está refinando tus letras...',
    refinedTagsLabel: 'Etiquetas refinadas',
    refinedTagsPlaceholder: 'Las etiquetas de estilo refinadas aparecerán aquí...',
    refiningTagsMessage: 'La IA está generando etiquetas de estilo coincidentes...',
    selectModel: 'Elige un modelo de música',
    generateButton: 'Generar música',
    quality: 'Calidad'
  },
  musicGenV2: {
    lyricsWorkshop: 'Taller de letras',
    lyricsInput: 'Tu texto',
    lyricsPlaceholder: 'Escribe letras, un tema, palabras clave o un estado de ánimo...',
    themeToLyrics: 'De palabras clave a letras de canción',
    refineLyrics: 'Estructurar letras de canción',
    resultLabel: 'Resultado',
    resultPlaceholder: 'Tus letras aparecerán aquí...',
    expandingTheme: 'La IA está escribiendo letras de canción a partir de tus palabras clave...',
    refiningLyrics: 'La IA está estructurando tus letras de canción...',
    soundExplorer: 'Explorador de sonido',
    suggestFromLyrics: 'Sugerir desde las letras',
    suggestingTags: 'La IA está analizando tus letras...',
    mostImportant: 'más importante',
    dimGenre: 'Género',
    dimTimbre: 'Timbre',
    dimGender: 'Voz',
    dimMood: 'Estado de ánimo',
    dimInstrument: 'Instrumentos',
    dimScene: 'Escena',
    dimRegion: 'Región (UNESCO)',
    dimTopic: 'Tema',
    audioLength: 'Duración del audio',
    generateButton: 'Generar música',
    selectModel: 'Modelo',
    customTags: 'Etiquetas personalizadas',
    customTagsPlaceholder: 'p. ej. acoustic,dreamy,summer_vibes'
  },
  latentLab: {
    tabs: {
      image: 'Laboratorio de imagen',
      textlab: 'Latent Text Lab',
      crossmodal: 'Laboratorio crossmodal'
    },
    imageLab: {
      headerTitle: 'Laboratorio de imagen — Investigación del espacio vectorial visual',
      headerSubtitle: 'Cinco herramientas para investigar cómo los modelos de difusión generan imágenes a partir de texto: desde el denoising a través de la atención y la fusión hasta la aritmética vectorial.',
      tabs: {
        archaeology: {
          label: 'Arqueología de denoising',
          short: 'Observa cómo trabaja el modelo'
        },
        attention: {
          label: 'Cartografía de atención',
          short: 'Ve dónde mira el modelo'
        },
        fusion: {
          label: 'Fusión de codificadores',
          short: 'Mezcla surrealista'
        },
        probing: {
          label: 'Feature Probing',
          short: 'Análisis a nivel de dimensiones'
        },
        algebra: {
          label: 'Álgebra de conceptos',
          short: 'Aritmética vectorial'
        }
      }
    },
    comingSoon: 'Esta herramienta se implementará en una versión futura.',
    shared: {
      negativeHint: 'Términos que el modelo debe evitar activamente (p. ej. "borroso, texto")',
      stepsHint: 'Más pasos = mayor calidad pero mayor tiempo de generación',
      cfgHint: 'Classifier-Free Guidance: mayor = adherencia más fuerte al prompt, menos variación',
      seedHint: '-1 = aleatorio, valor fijo = resultado reproducible',
      recordingActive: 'Grabación activa',
      recordingCount: '{count} grabación | {count} grabaciones',
      recordingTooltip: 'Los datos de investigación se guardan automáticamente',
    },
    attention: {
      headerTitle: 'Cartografía de atención — ¿Qué palabra dirige qué región de la imagen?',
      headerSubtitle: 'Para cada palabra del prompt, una superposición de mapa de calor sobre la imagen generada muestra DÓNDE en la imagen esa palabra tuvo más influencia. Esto revela cómo el modelo distribuye espacialmente los conceptos semánticos.',
      explanationToggle: 'Mostrar explicación detallada',
      explainWhatTitle: '¿Qué muestra esta herramienta?',
      explainWhatText: 'Cuando un modelo de difusión genera una imagen, no lee el prompt palabra por palabra como un conjunto de instrucciones. En su lugar, un mecanismo llamado "atención" distribuye la influencia de cada palabra a través de diferentes regiones de la imagen. La palabra "casa" influye principalmente en la región donde aparece la casa — pero también en áreas vecinas, porque el modelo entiende el contexto de toda la escena. Esta herramienta hace visible esa distribución: haz clic en una palabra y observa qué regiones de la imagen se iluminan.',
      explainHowTitle: '¿Cómo leo el mapa de calor?',
      explainHowText: 'Color brillante e intenso = influencia fuerte de la palabra en esa región. Color oscuro o ausente = poca influencia. Si seleccionas varias palabras, aparecen en colores diferentes. Nota: los mapas NO tienen bordes perfectamente definidos — esto no es un error, sino que muestra que el modelo procesa conceptos contextualmente, no de forma aislada. Una "casa" en una escena de granja también tiene cierta influencia sobre animales y campos, porque el modelo entiende la escena como un todo.',
      explainReadTitle: '¿Qué revelan los dos controles deslizantes?',
      explainReadText: 'El control deslizante del paso de denoising muestra CUÁNDO en el proceso de generación de 25 pasos estás viendo la atención. Los pasos iniciales muestran la planificación del diseño general, los pasos finales la asignación de detalles. El selector de profundidad de la red muestra DÓNDE en el transformer se mide la atención: las capas superficiales (cerca de la entrada) muestran la planificación de la composición global, las capas intermedias la asignación semántica, las capas profundas el ajuste fino. Ambos ejes son independientes — vale la pena explorar sistemáticamente diferentes combinaciones.',
      techTitle: 'Detalles técnicos',
      techText: 'SD3.5 usa un MMDiT (Multimodal Diffusion Transformer) con atención conjunta: los tokens de imagen y texto se atienden mutuamente a través de 24 bloques transformer. Reemplazamos el procesador SDPA predeterminado con un procesador softmax(QK^T/√d) manual en 3 bloques seleccionados para extraer la submatriz de atención texto→imagen. Los mapas tienen resolución de 64x64 (cuadrícula de parches), escalados a la resolución de la imagen mediante interpolación bilineal. SD3.5 usa dos codificadores de texto: CLIP-L (BPE, 77 tokens) y T5-XXL (SentencePiece, 512 tokens). Ambos se pueden alternar aquí para ver cómo diferentes estrategias de tokenización afectan la atención.',
      referencesTitle: 'Referencias de investigación',
      promptLabel: 'Prompt',
      promptPlaceholder: 'p. ej. Una casa se encuentra en un paisaje, rodeada de tierras de cultivo, naturaleza y animales. Se pueden ver algunas personas.',
      generate: 'Generar + Analizar',
      generating: 'Generando imagen y extrayendo atención...',
      emptyHint: 'Ingresa un prompt y haz clic en Generar para visualizar los mapas de atención del modelo.',
      advancedLabel: 'Configuración avanzada',
      negativeLabel: 'Prompt negativo',
      stepsLabel: 'Pasos',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      tokensLabel: 'Tokens',
      tokensHint: 'Haz clic en una o más palabras. Los sub-tokens (p. ej. "Ca"+"sa") se combinan automáticamente. Múltiples palabras aparecen en diferentes colores.',
      timestepLabel: 'Paso de denoising',
      timestepHint: 'Los modelos de difusión generan imágenes en 25 pasos, de ruido a imagen. Los pasos iniciales establecen la estructura general, los pasos finales refinan los detalles. Este control deslizante muestra a qué atiende el modelo en cada paso.',
      step: 'Paso',
      layerLabel: 'Profundidad de la red',
      layerHint: 'En cada paso de denoising, la señal pasa por las 24 capas del transformer. Las capas superficiales (cerca de la entrada) capturan la composición global, las capas intermedias la asignación semántica, las capas profundas (cerca de la salida) los detalles finos. Ambos controles son independientes: paso = cuándo en el proceso, profundidad = dónde en la red.',
      layerEarly: 'Superficial (Composición)',
      layerMid: 'Intermedia (Semántica)',
      layerLate: 'Profunda (Detalle)',
      opacityLabel: 'Mapa de calor',
      opacityHint: 'Intensidad de la superposición de color sobre la imagen.',
      baseImageLabel: 'Imagen base',
      baseColor: 'Color',
      baseBW: 'B/N',
      baseOff: 'Desactivado',
      baseImageHint: 'Color muestra la imagen original. B/N la desatura para que los colores del mapa de calor destaquen. Desactivado oculta la imagen por completo y muestra solo el mapa de atención.',
      encoderLabel: 'Codificador de texto',
      encoderClipL: 'CLIP-L (77 Tokens)',
      encoderT5: 'T5-XXL (512 Tokens)',
      encoderHint: 'SD3.5 usa dos codificadores de texto con diferente tokenización. CLIP-L usa BPE (Byte-Pair Encoding), T5-XXL usa SentencePiece. Compara cómo ambos codificadores procesan el mismo prompt y qué regiones de la imagen dirige cada uno.',
      download: 'Descargar imagen'
    },
    probing: {
      headerTitle: 'Feature Probing — ¿Qué dimensiones codifican qué?',
      headerSubtitle: 'Compara dos prompts y descubre qué dimensiones del embedding codifican la diferencia semántica. Transfiere selectivamente dimensiones individuales para ver cómo afectan la imagen.',
      explanationToggle: 'Mostrar explicación detallada',
      explainWhatTitle: '¿Qué muestra esta herramienta?',
      explainWhatText: 'Cada palabra es convertida por el codificador de texto en un vector de alta dimensionalidad (p. ej. 4096 dimensiones para T5). Cuando cambias una palabra en el prompt — p. ej. "rojo" a "azul" — ciertas dimensiones cambian más que otras. Esta herramienta te muestra CUÁLES dimensiones cambian más y te permite transferir selectivamente dimensiones individuales del prompt B al prompt A.',
      explainHowTitle: '¿Cómo funciona la transferencia?',
      explainHowText: 'El gráfico de barras muestra todas las dimensiones ordenadas por magnitud de diferencia. Usa los controles de rango de posición (Desde/Hasta) para seleccionar una ventana — p. ej. solo las 100 principales o específicamente las posiciones 880–920. Al hacer clic en "Transferir" se regenera la imagen con los mismos ajustes (¡mismo seed!) — pero con las dimensiones seleccionadas del prompt B. Esto te permite ver exactamente qué "codifican" esas dimensiones.',
      explainReadTitle: '¿Cómo leo el gráfico de barras?',
      explainReadText: 'Cada barra representa una dimensión del embedding. La longitud muestra cuánto difiere esa dimensión entre el prompt A y B. Las dimensiones con grandes diferencias son las portadoras más probables del cambio semántico. Pero nota: los embeddings son distribuidos — a menudo se necesitan varias dimensiones juntas para producir un cambio visible.',
      techTitle: 'Detalles técnicos',
      techText: 'SD3.5 usa tres codificadores de texto: CLIP-L (768d), CLIP-G (1280d) y T5-XXL (4096d). Puedes examinar cada uno individualmente. La diferencia se calcula como desviación absoluta media a través de todas las posiciones de tokens: mean(abs(B-A), dim=tokens). La transferencia reemplaza las dimensiones seleccionadas en todas las posiciones de tokens simultáneamente.',
      referencesTitle: 'Referencias de investigación',
      promptALabel: 'Prompt A (Original)',
      promptBLabel: 'Prompt B (Comparación)',
      promptAPlaceholder: 'p. ej. Una casa roja junto al lago',
      promptBPlaceholder: 'p. ej. Una casa azul junto al lago',
      encoderLabel: 'Codificador',
      encoderAll: 'Todos (recomendado)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      analyzeBtn: 'Analizar',
      analyzing: 'Codificando y comparando prompts...',
      transferBtn: 'Transferir dimensiones vectoriales seleccionadas del Prompt B a la imagen generada',
      transferring: 'Generando imagen con embedding modificado...',
      rankFromLabel: 'Desde posición',
      rankToLabel: 'Hasta posición',
      sliderLabel: 'Seleccionar dimensiones del Prompt B',
      range1Label: 'Rango 1',
      range2Label: 'Rango 2',
      addRange: 'Agregar rango',
      selectionDesc: '{count} dimensiones del Prompt B seleccionadas (posición {ranges} de {total})',
      listTitle: 'Las {count} dimensiones del Prompt B con la mayor diferencia respecto al Prompt A',
      sortAsc: 'Ascendente',
      sortDesc: 'Descendente',
      originalLabel: 'Original (Prompt A)',
      modifiedLabel: 'Modificado (Transferencia del Prompt B)',
      modifiedHint: 'Selecciona un rango de posiciones abajo y haz clic en "Transferir" — esto mostrará el prompt A con las dimensiones transferidas del B (mismo seed).',
      noDifference: 'Los embeddings son idénticos — cambia el prompt B.',
      advancedLabel: 'Configuración avanzada',
      negativeLabel: 'Prompt negativo',
      stepsLabel: 'Pasos',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      selectAll: 'Todos',
      selectNone: 'Ninguno',
      encoderHint: 'Todos = todos los codificadores combinados. CLIP-L/CLIP-G/T5 = aísla un codificador para el análisis.',
      sliderHint: 'Selecciona un rango de posiciones de las dimensiones de embedding más importantes (ordenadas por diferencia entre A y B).',
      transferHint: 'Transfiere las dimensiones seleccionadas del Prompt B al Prompt A y genera una nueva imagen.',
      downloadOriginal: 'Descargar original',
      downloadModified: 'Descargar modificado'
    },
    algebra: {
      headerTitle: 'Álgebra de conceptos \u2014 Aritmética vectorial sobre embeddings de imagen',
      headerSubtitle: 'Aplica la famosa analogía word2vec a la generación de imágenes: Rey \u2212 Hombre + Mujer \u2248 Reina. Tres prompts se codifican y combinan algebraicamente.',
      explanationToggle: 'Mostrar explicación detallada',
      explainWhatTitle: '¿Qué muestra esta herramienta?',
      explainWhatText: 'En 2013, Mikolov demostró que los embeddings de palabras codifican relaciones semánticas como direcciones lineales: el vector de "Rey" menos "Hombre" más "Mujer" produce un vector cercano a "Reina". Esta herramienta aplica esa idea a los codificadores de texto de SD3.5: en lugar de palabras individuales, manipulas embeddings de prompts completos. El resultado es una imagen que contiene el concepto A pero con B reemplazado por C.',
      explainHowTitle: '¿Cómo funciona el álgebra — y por qué no simplemente usar un prompt negativo?',
      explainHowText: 'Ingresas tres prompts: A (base), B (restar) y C (agregar). La fórmula es: Resultado = A \u2212 Escala\u2081\u00d7B + Escala\u2082\u00d7C. Los controles de escala regulan la intensidad: a 1.0, B se resta completamente y C se agrega completamente. A 0.5, solo la mitad. Valores superiores a 1.0 amplifican el efecto. \u2014 ¿Por qué no simplemente usar "A + C" como prompt y "B" como prompt negativo? Porque eso hace algo fundamentalmente diferente: un prompt negativo dirige el proceso de denoising lejos de B en CADA uno de los 25 pasos — el modelo decide paso a paso cómo interpretar "no B". El álgebra de conceptos en cambio calcula un nuevo vector ANTES de la generación de la imagen: la sustracción ocurre en el espacio de embeddings, no en el proceso de difusión. El resultado es un vector único que codifica directamente "A sin B-idad más C-idad". El prompt negativo dice "no hagas esto". El álgebra dice "quita este concepto y pon aquel" — una operación quirúrgica en el espacio de significado en lugar de una estrategia de evitación paso a paso.',
      explainReadTitle: '¿Qué significan los resultados?',
      explainReadText: 'A la izquierda ves la imagen de referencia (solo prompt A, mismo seed). A la derecha, el resultado del álgebra. Si la analogía funciona, la imagen derecha debería mostrar el concepto A pero con el cambio semántico B\u2192C. Ejemplo: "Atardecer en la playa" \u2212 "Playa" + "Montañas" \u2248 "Atardecer sobre montañas". La distancia L2 muestra cuán lejos se ha movido el resultado del original. \u2014 ¿Es conmutativa la operación? No. La sustracción de B y la adición de C ocurren en relación al vector A. La dirección B\u2192C solo tiene sentido en el contexto de A: "Rey \u2212 Hombre" elimina las direcciones "masculinas" del vector Rey, "+ Mujer" agrega las direcciones "femeninas" — el resultado aterriza cerca de "Reina". C no se coloca quirúrgicamente donde se eliminó B; simplemente se agrega. Que esto funcione demuestra que las relaciones semánticas se codifican como direcciones lineales consistentes en el espacio vectorial.',
      techTitle: 'Detalles técnicos',
      techText: 'El álgebra se realiza sobre los embeddings del codificador seleccionado: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), o todos combinados (589 tokens \u00d7 4096d). La misma operación también se aplica a los embeddings agrupados (2048d). Ambas imágenes usan el mismo seed para una comparación justa.',
      referencesTitle: 'Referencias de investigación',
      promptALabel: 'Prompt A (Base)',
      promptAPlaceholder: 'p. ej. Atardecer en la playa con palmeras',
      promptBLabel: 'Prompt B (Restar)',
      promptBPlaceholder: 'p. ej. Playa con palmeras',
      promptCLabel: 'Prompt C (Agregar)',
      promptCPlaceholder: 'p. ej. Montañas cubiertas de nieve',
      formulaLabel: 'A \u2212 B + C = ?',
      encoderLabel: 'Codificador',
      encoderAll: 'Todos (recomendado)',
      encoderClipL: 'CLIP-L (768d)',
      encoderClipG: 'CLIP-G (1280d)',
      encoderT5: 'T5-XXL (4096d)',
      generateBtn: 'Calcular',
      generating: 'Calculando embeddings y generando imágenes...',
      referenceLabel: 'Referencia (Prompt A)',
      resultLabel: 'Resultado (A \u2212 B + C)',
      l2Label: 'Distancia L2 del original',
      advancedLabel: 'Configuración avanzada',
      negativeLabel: 'Prompt negativo',
      stepsLabel: 'Pasos',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      scaleSubLabel: 'Escala de sustracción',
      scaleAddLabel: 'Escala de adición',
      encoderHint: 'Todos = todos los codificadores combinados. CLIP-L/CLIP-G/T5 = aísla un codificador para la aritmética.',
      scaleSubHint: 'Peso de la sustracción (B). Mayor = eliminación más fuerte del concepto B.',
      scaleAddHint: 'Peso de la adición (C). Mayor = inyección más fuerte del concepto C.',
      l2Hint: 'Distancia euclidiana en el espacio de embeddings. Menor = más similar, mayor = más diferente.',
      downloadReference: 'Descargar referencia',
      downloadResult: 'Descargar resultado',
      resultHint: 'Ingresa tres prompts y haz clic en Calcular \u2014 el resultado de la aritmética vectorial aparecerá aquí.'
    },
    archaeology: {
      headerTitle: 'Arqueología de denoising \u2014 ¿Cómo se convierte el ruido en imagen?',
      headerSubtitle: 'Observa cada paso de denoising. Los modelos de difusión no dibujan de izquierda a derecha \u2014 trabajan en todas partes simultáneamente, de formas generales a detalles finos.',
      explanationToggle: 'Mostrar explicación detallada',
      explainWhatTitle: '¿Qué muestra esta herramienta?',
      explainWhatText: 'Un modelo de difusión crea una imagen eliminando ruido progresivamente. A diferencia de dibujar de izquierda a derecha, el modelo trabaja en TODAS las regiones de la imagen simultáneamente. En los primeros pasos, surgen estructuras generales: ¿Dónde está arriba, dónde abajo? ¿Dónde está el horizonte? En los pasos intermedios, aparece contenido semántico: objetos, formas, colores. Los pasos finales refinan texturas y detalles. Esta herramienta hace visible cada paso individual.',
      explainHowTitle: '¿Cómo uso esta herramienta?',
      explainHowText: 'Ingresa un prompt y haz clic en Generar. El modelo produce 25 imágenes intermedias (una por paso de denoising). Estas aparecen como una tira de película debajo. Haz clic en una miniatura o usa el control deslizante de la línea de tiempo para ver cada paso a tamaño completo. Compara los pasos iniciales y finales: ¿Cuándo "sabe" el modelo lo que está dibujando?',
      explainReadTitle: '¿Qué revelan las tres fases?',
      explainReadText: 'Pasos iniciales (1\u20138): Composición global — estructura básica, distribución de color, planificación del diseño. Pasos intermedios (9\u201317): Emergencia semántica — los objetos se vuelven reconocibles, las formas se cristalizan. Pasos finales (18\u201325): Refinamiento de detalles — texturas, bordes, patrones finos. Las transiciones son graduales, pero las fases muestran claramente: el modelo primero "planifica" globalmente, luego refina localmente. Particularmente revelador: El primer paso no muestra píxeles detallados, sino parches de color. Esto se debe a que el ruido se genera en el espacio latente (128\u00d7128 a 16 canales), no en el espacio de píxeles. El VAE traduce cada píxel latente en un parche de ~8\u00d78 píxeles — incluso el ruido gaussiano puro se convierte en agrupaciones de color coherentes. El modelo nunca "piensa" en píxeles individuales, sino siempre en este espacio comprimido.',
      techTitle: 'Detalles técnicos',
      techText: 'SD3.5 Large usa Rectified Flow como programador con 25 pasos predeterminados. En cada paso, los vectores latentes actuales se decodifican a través del VAE (1024\u00d71024 JPEG). El VAE (Variational Autoencoder) traduce el espacio latente matemático en píxeles. La representación latente es 128\u00d7128 a 16 canales — cada píxel latente corresponde a un parche de ~8\u00d78 píxeles en la imagen. Por eso incluso el primer paso muestra agrupaciones de color en lugar de ruido fino de píxeles: el VAE interpreta vectores aleatorios de 16 dimensiones como parches de color coherentes.',
      referencesTitle: 'Referencias de investigación',
      promptLabel: 'Prompt',
      promptPlaceholder: 'p. ej. Un mercado en una ciudad medieval con personas, edificios y una fuente',
      generate: 'Generar',
      generating: 'Generando imagen \u2014 registrando cada paso...',
      emptyHint: 'Ingresa un prompt y haz clic en Generar para visualizar el proceso de denoising.',
      advancedLabel: 'Configuración avanzada',
      negativeLabel: 'Prompt negativo',
      stepsLabel: 'Pasos',
      cfgLabel: 'CFG',
      seedLabel: 'Seed',
      filmstripLabel: 'Tira de película del denoising',
      timelineLabel: 'Paso',
      phaseEarly: 'Composición',
      phaseMid: 'Semántica',
      phaseLate: 'Detalle',
      phaseEarlyDesc: 'Surge la estructura global y la distribución de color',
      phaseMidDesc: 'Los objetos y formas se vuelven reconocibles',
      phaseLateDesc: 'Las texturas y detalles finos se agudizan',
      finalImageLabel: 'Imagen final (resolución completa)',
      timelineHint: 'Recorre los pasos de denoising — muestra cómo la imagen emerge del ruido hasta la composición final.',
      download: 'Descargar imagen'
    },
    textLab: {
      headerTitle: 'Latent Text Lab \u2014 Deconstrucción científica de LLM',
      headerSubtitle: 'Ingeniería de representaciones, arqueología comparativa de modelos y análisis sistemático de sesgos: Tres herramientas basadas en investigación para examinar modelos de lenguaje.',
      explanationToggle: 'Mostrar explicación',
      modelPanel: {
        presetLabel: 'Preset',
        presetNone: 'Sin preset (ID personalizado)',
        customModelLabel: 'ID de modelo HuggingFace',
        customModelPlaceholder: 'p. ej. meta-llama/Llama-3.2-1B',
        quantizationLabel: 'Cuantización',
        quantAuto: 'Auto',
        quantizationHint: 'bf16 = calidad completa, int8 = mitad de VRAM, int4 = VRAM mínima pero menor calidad',
      },
      temperatureHint: 'Aleatoriedad de la generación de texto. Bajo = determinístico, alto = más creativo.',
      maxTokensHint: 'Número máximo de tokens generados (partes de palabras).',
      textSeedHint: '-1 = aleatorio, valor fijo = resultado reproducible',
      tabs: {
        repeng: { label: 'Ingeniería de representaciones', short: 'Encuentra vectores de dirección en LLMs' },
        compare: { label: 'Comparación de modelos', short: 'Compara dos LLMs capa por capa' },
        bias: { label: 'Arqueología de sesgos', short: 'Descubre sesgos ocultos en LLMs' },
      },
      repeng: {
        title: 'Ingeniería de representaciones',
        subtitle: 'Encuentra direcciones de conceptos en el espacio de activación y dirige la generación',
        explainWhatTitle: '¿Qué muestra este experimento?',
        explainWhatText: 'Basado en Zou et al. (2023) \u201cRepresentation Engineering\u201d y Li et al. (2024) \u201cInference-Time Intervention\u201d. Los LLMs codifican conceptos abstractos (verdad, sentimiento, ética) como direcciones en el espacio de activación de alta dimensionalidad. Mediante pares contrastivos (oración verdadera vs. falsa), se puede extraer la dirección que codifica un concepto. Agregar esta dirección en tiempo de ejecución cambia el comportamiento del modelo de forma dirigida — sin reentrenamiento. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
        explainHowTitle: '¿Cómo lo uso?',
        explainHowText: 'Este experimento extrae una \u201cdirección de verdad\u201d del modelo. Los pares contrastivos pre-llenados contienen cada uno una afirmación verdadera y una falsa. A partir de las diferencias en las activaciones, el sistema calcula una dirección en el espacio de alta dimensionalidad. Cuando esta dirección se invierte (\u03b1 = -1), el modelo debería generar una respuesta incorrecta para un prompt factual — aunque "sabe" la respuesta correcta. Recomendación: Los prompts en inglés funcionan significativamente mejor ya que la mayoría de los LLMs de código abierto fueron entrenados principalmente con datos en inglés.',
        referencesTitle: 'Referencias de investigación',
        expectedResults: 'Resultados esperados: Con \u03b1 = 0 (línea base), el modelo genera la respuesta correcta. Con \u03b1 = -1 (inversión), debería aparecer una respuesta incorrecta — este es el núcleo del experimento. Con \u03b1 = +1, poco cambia porque el modelo ya responde correctamente. Más allá de |\u03b1| > 2, dominan los artefactos (repeticiones, sinsentidos). El "punto óptimo" según Zou et al. es |\u03b1| entre 0.5 y 2.0. Una varianza explicada > 50% indica una separación limpia — por debajo, los pares contrastivos son demasiado similares o muy pocos (se recomiendan al menos 3).',
        pairsTitle: 'Pares contrastivos',
        pairsSubtitle: 'Se recomiendan al menos 3 pares. Cada par debería diferir solo en el concepto objetivo (verdadero vs. falso). Los ejemplos son editables.',
        positiveLabel: 'Positivo (verdadero)',
        negativeLabel: 'Negativo (falso)',
        positivePlaceholder: 'p. ej. La capital de Francia es París',
        negativePlaceholder: 'p. ej. La capital de Francia es Berlín',
        addPair: 'Agregar par',
        removePair: 'Eliminar',
        targetLayerLabel: 'Capa objetivo',
        targetLayerHint: 'Qué capa del transformer recibe el vector de dirección. Diferentes capas influyen en diferentes aspectos de la generación de texto.',
        targetLayerAuto: 'Última capa',
        findDirection: 'Encontrar dirección',
        finding: 'Calculando dirección del concepto...',
        directionFound: 'Dirección del concepto encontrada',
        varianceLabel: 'Varianza explicada',
        dimLabel: 'Dimensiones',
        projectionsTitle: 'Proyecciones de pares contrastivos',
        testTitle: 'Prueba + Manipulación',
        testSubtitle: 'Ingresa una oración y dirige la generación a lo largo de la dirección del concepto',
        testPromptLabel: 'Prompt de prueba',
        testPromptPlaceholder: 'p. ej. La capital de Alemania es',
        alphaLabel: 'Intensidad de manipulación (\u03b1)',
        alphaHint: 'Intensidad del vector de dirección. 0 = sin efecto, mayor = influencia más fuerte de los pares contrastivos.',
        temperatureLabel: 'Temperatura',
        maxTokensLabel: 'Máx. tokens',
        seedLabel: 'Seed (-1 = aleatorio)',
        generateBtn: 'Generar con manipulación',
        generating: 'Ejecutando generación manipulada...',
        baselineLabel: 'Línea base (sin manipulación)',
        manipulatedLabel: 'Manipulado (\u03b1 = {alpha})',
        projectionLabel: 'Proyección sobre la dirección del concepto',
        interpretationTitle: 'Interpretación',
        interpreting: 'Analizando resultados...',
        interpretationError: 'No se pudo generar la interpretación'
      },
      compare: {
        title: 'Arqueología comparativa de modelos',
        subtitle: 'Carga dos modelos y compara sistemáticamente sus representaciones internas',
        explainWhatTitle: '¿Qué muestra este experimento?',
        explainWhatText: 'Basado en Belinkov (2022) \u201cProbing Classifiers\u201d y Olsson et al. (2022) \u201cIn-Context Learning and Induction Heads\u201d. El mapa de calor muestra CKA (Centered Kernel Alignment) entre capas de ambos modelos. Una alta similitud significa que estas capas representan la información de manera similar. Las capas iniciales (sintaxis) a menudo son similares — las capas finales (semántica) divergen más fuertemente. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
        explainHowTitle: '¿Cómo lo uso?',
        explainHowText: 'El Modelo A es el preset activo. Elige un segundo modelo (Modelo B) — mediante preset o ID personalizado de HuggingFace — y cárgalo. Ingresa texto y haz clic en \u201cComparar\u201d. El mapa de calor CKA muestra qué capas representan la información de manera similar. Las generaciones de ambos modelos con el mismo seed muestran cuán diferente completan el mismo prompt.',
        referencesTitle: 'Referencias de investigación',
        modelATitle: 'Modelo A (del selector de presets)',
        modelAHint: 'Cambiar mediante el menú de presets arriba',
        modelBTitle: 'Modelo B (segundo modelo)',
        modelBPresetLabel: 'Preset',
        modelBCustomLabel: 'ID de modelo HuggingFace',
        modelBCustomPlaceholder: 'p. ej. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        modelBLoadBtn: 'Cargar Modelo B',
        modelBLoaded: 'Modelo B cargado',
        modelBNone: 'Modelo B no cargado',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ej. El gato se sentó en la alfombra y observó los pájaros',
        seedLabel: 'Seed',
        temperatureLabel: 'Temperatura',
        maxTokensLabel: 'Máx. tokens',
        compareBtn: 'Comparar',
        comparing: 'Comparando modelos...',
        heatmapTitle: 'Alineación de capas (CKA)',
        heatmapAxisA: 'Modelo A \u2014 Capas',
        heatmapAxisB: 'Modelo B \u2014 Capas',
        heatmapExplain: 'Celdas brillantes = alta similitud de representación. Patrones diagonales muestran que los modelos procesan la información en un orden similar.',
        attentionTitle: 'Comparación de atención (última capa)',
        modelALabel: 'Modelo A',
        modelBLabel: 'Modelo B',
        generationTitle: 'Comparación de generación (mismo seed)',
        layerStatsTitle: 'Estadísticas de capas',
        interpretationTitle: 'Interpretación',
        interpreting: 'Analizando resultados...',
        interpretationError: 'No se pudo generar la interpretación'
      },
      bias: {
        title: 'Arqueología de sesgos',
        subtitle: 'Experimentos sistemáticos de sesgos mediante manipulación controlada de tokens',
        explainWhatTitle: '¿Qué muestra este experimento?',
        explainWhatText: 'Basado en Zou et al. (2023) \u201cRepresentation Engineering\u201d y Bricken et al. (2023) \u201cTowards Monosemanticity\u201d. En lugar de manipulación libre, esta herramienta investiga sesgos sistemáticos: ¿Qué sucede cuando se suprimen todos los pronombres masculinos? ¿Qué género elige el modelo por defecto? \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
        explainHowTitle: '¿Cómo lo uso?',
        explainHowText: 'Elige un tipo de experimento (género, sentimiento, dominio o tokens personalizados). Ingresa un prompt que invite al modelo a continuar (p. ej. \u201cEl doctor le dijo al paciente\u201d). Los resultados muestran la generación base vs. la manipulada — las diferencias revelan qué sesgos están codificados en el modelo.',
        referencesTitle: 'Referencias de investigación',
        presetLabel: 'Tipo de experimento',
        presetGender: 'Género \u2014 Suprimir pronombres de género',
        presetSentiment: 'Sentimiento \u2014 Reforzar positivo/negativo',
        presetDomain: 'Dominio \u2014 Reforzar científico/poético',
        presetCustom: 'Experimento personalizado',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ej. El doctor le dijo al paciente',
        customBoostLabel: 'Tokens a reforzar (separados por comas)',
        customBoostPlaceholder: 'p. ej. dark,shadow,night',
        customSuppressLabel: 'Tokens a suprimir (separados por comas)',
        customSuppressPlaceholder: 'p. ej. light,sun,bright',
        numSamplesLabel: 'Muestras por condición',
        temperatureLabel: 'Temperatura',
        maxTokensLabel: 'Máx. tokens',
        seedLabel: 'Seed base',
        runBtn: 'Ejecutar experimento',
        running: 'Ejecutando experimento de sesgos...',
        baselineTitle: 'Línea base (sin manipulación)',
        groupTitle: 'Grupo: {name}',
        modeSuppress: 'suprimido',
        modeBoost: 'reforzado',
        tokensLabel: 'Tokens',
        sampleSeedLabel: 'Seed',
        genderDesc: 'Suprime todos los pronombres de género y observa qué valores predeterminados elige el modelo.',
        sentimentDesc: 'Refuerza palabras positivas o negativas y mide cuán fuertemente se afecta todo el flujo de texto.',
        domainDesc: 'Refuerza vocabulario científico o poético y observa cambios de registro.',
        interpretationTitle: 'Interpretación',
        interpreting: 'Analizando resultados...',
        interpretationError: 'No se pudo generar la interpretación'
      },
      error: {
        gpuUnreachable: 'Servicio GPU inalcanzable. ¿Está ejecutándose?',
        loadFailed: 'Error al cargar el modelo.',
        operationFailed: 'La operación falló.'
      }
    },
    crossmodal: {
      headerTitle: 'Laboratorio crossmodal',
      headerSubtitle: 'Sonido desde espacios latentes: manipulación de embeddings T5, generación de audio guiada por imagen, transferencia crossmodal',
      explanationToggle: 'Mostrar explicación detallada',
      generate: 'Generar',
      generating: 'Generando...',
      result: 'Resultado',
      seed: 'Seed',
      generationTime: 'Tiempo de generación',
      tabs: {
        synth: {
          label: 'Latent Audio Synth',
          short: 'Manipulación de embeddings T5',
          title: 'Latent Audio Synth',
          description: 'Manipulación directa del espacio de condicionamiento T5 de Stable Audio (768d). Interpola entre prompts, extrapola más allá del prompt, escala embeddings e inyecta ruido. Bucles ultracortos, casi en tiempo real.'
        },
        mmaudio: {
          label: 'MMAudio',
          short: 'Imagen/texto a audio (CVPR 2025)',
          title: 'MMAudio — Video/Imagen a audio',
          description: 'La imagen y el texto entran en la misma red como señales separadas — la imagen no se traduce a lenguaje, ambas guían la generación de sonido simultáneamente. El modelo fue entrenado conjuntamente en video y audio, aprendiendo asociaciones directas entre lo que se ve y lo que se escucha. Hasta 8s, 44.1kHz, ~1.2s de cómputo. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
        },
        guidance: {
          label: 'ImageBind Guidance',
          short: 'Guía de imagen por gradiente',
          title: 'ImageBind Gradient Guidance',
          description: 'Dirección basada en gradiente durante el proceso de denoising de Stable Audio. ImageBind proporciona un espacio compartido de 1024d para imagen y audio — el gradiente de la similitud coseno guía la generación de audio hacia el embedding de la imagen.'
        }
      },
      synth: {
        explainWhatTitle: '¿Qué hace el Latent Audio Synth?',
        explainWhatText: 'Stable Audio genera sonido a partir de texto. El texto es convertido por un codificador T5 en un vector numérico de 768 dimensiones \u2014 este vector es lo que manipulas aquí. En lugar de solo cambiar \u201cqué\u201d genera el modelo (mediante el prompt), cambias \u201ccómo\u201d el modelo entiende internamente el texto. Dos prompts que suenan similar pueden estar lejos en este espacio \u2014 y viceversa.',
        explainHowTitle: '¿Cómo uso esta herramienta?',
        explainHowText: 'Ingresa texto en el Prompt A \u2014 esto determina el sonido base. Opcional: Prompt B como punto objetivo. El control deslizante Alpha controla la mezcla: en 0 escuchas solo A, en 1 solo B, en 0.5 una mezcla. Valores superiores a 1 extrapolan más allá de B (el sonido se vuelve más extremo), valores inferiores a 0 van en la dirección opuesta. Magnitude escala todo el embedding \u2014 valores más altos producen sonidos más intensos. Noise inyecta aleatoriedad y crea variaciones impredecibles. La Spectral Strip (debajo del botón Generar) muestra las 768 dimensiones como barras. Puedes desplazar dimensiones individuales haciendo clic y arrastrando, manipulando directamente el sonido. Clic derecho restablece una dimensión.',
        promptA: 'Prompt A (Base)',
        promptAPlaceholder: 'p. ej. olas del océano',
        promptB: 'Prompt B (Opcional, para interpolación)',
        promptBPlaceholder: 'p. ej. melodía de piano',
        alpha: 'Alpha (Interpolación)',
        alphaHint: '0 = solo A, 1 = solo B, intermedio = mezcla, >1 o <0 = extrapolación',
        magnitude: 'Magnitud (Escalado)',
        magnitudeHint: 'Escalado global del embedding (1.0 = sin cambios)',
        noise: 'Ruido',
        noiseHint: 'Ruido gaussiano sobre el embedding (0 = sin ruido)',
        duration: 'Duración (s)',
        steps: 'Pasos',
        cfg: 'CFG',
        durationHint: 'Duración del clip de audio generado en segundos',
        stepsHint: 'Pasos de denoising. Más = mayor calidad.',
        cfgHint: 'Classifier-Free Guidance para generación de audio',
        seedHint: '-1 = aleatorio, valor fijo = resultado reproducible',
        loop: 'Reproducción en bucle',
        loopOn: 'Bucle activado',
        loopOff: 'Bucle desactivado',
        stop: 'Detener',
        looping: 'En bucle',
        playing: 'Reproduciendo',
        stopped: 'Detenido',
        transpose: 'Transposición (semitonos)',
        midiSection: 'Control MIDI',
        midiUnsupported: 'Web MIDI no es compatible con este navegador.',
        midiInput: 'Entrada MIDI',
        midiNone: '(ninguno)',
        midiMappings: 'Asignaciones CC',
        midiNoteC3: 'Nota (C3 = Ref)',
        midiGenerate: 'Generar + Transponer',
        midiPitch: 'Tono rel. C3',
        loopInterval: 'Intervalo de bucle',
        loopOptimize: 'Auto-optimizar',
        loopPingPong: 'Ping-pong',
        loopIntervalHint: 'Inicio/fin de la región del bucle — acorta el final para recortar el desvanecimiento de Stable Audio',
        modeLoop: 'Bucle',
        modePingPong: 'Ping-Pong',
        modeWavetable: 'Wavetable',
        modeRate: 'Tempo (rápido)',
        modePitch: 'Tono (OLA)',
        wavetableScan: 'Posición de escaneo',
        wavetableScanHint: 'Transición entre cuadros (0 = inicio, 1 = final)',
        wavetableFrames: '{count} cuadros',
        midiScan: 'Posición de escaneo',
        adsrTitle: 'Envolvente ADSR',
        adsrAttack: 'A',
        adsrDecay: 'D',
        adsrSustain: 'S',
        adsrRelease: 'R',
        adsrHint: 'Envolvente para notas MIDI (Attack/Decay/Sustain/Release)',
        play: 'Reproducir',
        normalize: 'Normalizar volumen',
        peak: 'Pico',
        crossfade: 'Crossfade',
        transposeHint: 'Desplaza el tono en semitonos',
        crossfadeHint: 'Tiempo de crossfade en el límite del bucle (ms)',
        normalizeHint: 'Normaliza el volumen a la amplitud máxima',
        saveRaw: 'Guardar original',
        saveLoop: 'Guardar bucle',
        embeddingStats: 'Estadísticas del embedding',
        dimensions: {
          section: 'Explorador de dimensiones',
          hint: 'Arrastra sobre las barras = establece desplazamiento. Pinta horizontalmente = múltiples dimensiones.',
          resetAll: 'Restablecer todo',
          hoverActivation: 'Activación',
          hoverOffset: 'Desplazamiento',
          rightClickReset: 'Clic derecho = restablecer',
          sortDiff: 'Ordenado por diferencia de prompt',
          sortMagnitude: 'Ordenado por activación',
          activeOffsets: '{count} desplazamientos activos',
          applyAndGenerate: 'Aplicar y regenerar',
          undo: 'Deshacer',
          redo: 'Rehacer'
        }
      },
      mmaudio: {
        explainWhatTitle: '¿Qué hace MMAudio?',
        explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) fue entrenado conjuntamente en video y audio. No traduce una imagen a texto y luego a sonido, sino que procesa imagen y texto como señales paralelas en la misma red. El modelo aprendió qué sonidos corresponden a qué escenas visuales — un bosque produce canto de pájaros, una calle produce ruido de tráfico, una guitarra produce sonidos de punteo.',
        explainHowTitle: '¿Cómo uso esta herramienta?',
        explainHowText: 'Sube una imagen y/o ingresa un prompt de texto \u2014 usar ambos juntos produce los resultados más ricos. La imagen sola genera sonidos que coinciden con el contenido visual. El prompt de texto puede dirigir adicionalmente el sonido o usarse solo sin imagen. En el Prompt Negativo, describe sonidos que NO quieres escuchar (p. ej. \u201cvoz, música\u201d). Duración establece la longitud (1\u20138 segundos). Intensidad CFG controla cuán estrictamente el modelo sigue el prompt \u2014 valores bajos (2\u20133) producen resultados más variados, valores altos (6\u20138) son más fieles al prompt.',
        imageUpload: 'Subir imagen (opcional)',
        prompt: 'Prompt de texto (opcional)',
        promptPlaceholder: 'p. ej. crepitar de fogata',
        negativePrompt: 'Prompt negativo',
        duration: 'Duración (s)',
        maxDuration: 'Máx. 8s (límite del modelo)',
        cfg: 'CFG',
        steps: 'Pasos',
        compareHint: 'Comparar: Solo texto vs. Imagen + Texto'
      },
      guidance: {
        explainWhatTitle: '¿Qué hace ImageBind Guidance?',
        explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) lleva seis sentidos \u2014 imagen, sonido, texto, profundidad, calor, movimiento \u2014 a un \u201clenguaje\u201d compartido. Esta herramienta usa ese terreno común: mientras el sonido se genera paso a paso, constantemente pregunta \u201c¿Ya suena como la imagen?\u201d y corrige la dirección. La similitud coseno en el resultado muestra cuán cerca llegó el sonido generado al contenido de la imagen.',
        explainHowTitle: '¿Cómo uso esta herramienta?',
        explainHowText: 'Sube una imagen \u2014 esta es la dirección objetivo para el sonido. Opcional: un prompt de texto para dirección adicional. El control deslizante \u201c\u03bb Intensidad de guía\u201d es el parámetro más importante: valores bajos (0.01\u20130.05) dan al sonido mucha libertad, valores altos (0.3\u20131.0) lo atan estrechamente a la imagen. \u201cPasos de calentamiento\u201d determina desde qué paso la guía de imagen se activa \u2014 valores bajos comienzan inmediatamente, valores más altos dejan que la estructura básica emerja sin guía primero. Pasos totales y duración controlan calidad y longitud.',
        referencesTitle: 'Referencias de investigación',
        imageUpload: 'Subir imagen',
        prompt: 'Prompt base (opcional)',
        promptPlaceholder: 'p. ej. paisaje sonoro ambiental',
        lambda: 'Intensidad de guía',
        lambdaHint: 'Cuán fuertemente la imagen dirige la generación de audio',
        warmupSteps: 'Pasos de calentamiento',
        warmupHint: 'Guía por gradiente solo durante los primeros N pasos',
        totalSteps: 'Pasos totales',
        duration: 'Duración (s)',
        cfg: 'CFG',
        totalStepsHint: 'Pasos de denoising totales. Más = mayor calidad.',
        durationHint: 'Duración del clip de audio generado en segundos',
        cosineSimilarity: 'Similitud coseno (proximidad imagen-audio)'
      }
    }
  },
  edutainment: {
    ui: {
      didYouKnow: '\u{1F914} ¿Sabías que...?',
      learnMore: '\u{1F4DA} Aprende más',
      currentlyHappening: '\u26A1 Sucediendo ahora:',
      energyUsed: 'Energía utilizada',
      co2Produced: 'CO\u2082 producido'
    },
    energy: {
      kids_1: '\u{1F4A1} ¡Las imágenes de IA necesitan electricidad — tanta como cargar tu teléfono durante 3 horas!',
      kids_2: '\u{1F50C} ¡La GPU es como una supercalculadora que consume mucha energía!',
      kids_3: '\u26A1 ¡Cada imagen necesita tanta energía como mantener una luz LED encendida durante 10 minutos!',
      youth_1: '\u26A1 Una GPU usa {watts}W mientras genera — ¡como un pequeño calentador!',
      youth_2: '\u{1F50B} Una imagen usa aproximadamente 0.01-0.02 kWh — suena poco, ¡pero se acumula!',
      youth_3: '\u{1F321}\uFE0F La GPU está alcanzando {temp}°C ahora mismo — ¡por eso necesita enfriamiento!',
      expert_1: '\u{1F4CA} Tiempo real: {watts}W al {util}% de utilización = {kwh} kWh hasta ahora',
      expert_2: '\u{1F525} Límite TDP: {tdp}W | Actual: {watts}W ({percent}% del límite)',
      expert_3: '\u{1F4BE} VRAM: {used}/{total} GB ({percent}%) — modelo + activaciones'
    },
    data: {
      kids_1: '\u{1F9EE} ¡La GPU está calculando 10 mil millones de veces ahora mismo — más rápido de lo que puedes contar!',
      kids_2: '\u{1F3A8} ¡La imagen se crea en 50 pequeños pasos — como un rompecabezas que se resuelve solo!',
      kids_3: '\u{1F9E9} ¡Millones de números están volando a través de la GPU ahora mismo!',
      youth_1: '\u{1F504} Cada imagen pasa por ~50 "pasos de denoising" — ¡50 rondas de eliminación de ruido!',
      youth_2: '\u{1F4D0} ¡Se consultan 8 mil millones de parámetros — por imagen!',
      youth_3: '\u{1F9E0} La IA "piensa" en vectores con miles de dimensiones — como coordenadas en un espacio.',
      expert_1: '\u{1F52C} MMDiT: Multimodal Diffusion Transformer — texto + imagen en capas de atención conjunta',
      expert_2: '\u{1F4C8} Self-Attention: complejidad O(n\u00B2) — cada token "ve" a todos los demás',
      expert_3: '\u2699\uFE0F Classifier-Free Guidance: equilibrio entre influencia del prompt y creatividad'
    },
    model: {
      kids_1: '\u{1F393} ¡El modelo de IA miró millones de imágenes para aprender a pintar!',
      kids_2: '\u{1F916} ¡La IA es como un artista que nunca olvida lo que ha visto!',
      kids_3: '\u2728 ¡8 mil millones de conexiones en el modelo — más que las estrellas que puedes ver en el cielo!',
      youth_1: '\u{1F9E0} SD3.5 Large tiene 8 mil millones de parámetros — como 8 mil millones de nodos de decisión.',
      youth_2: '\u{1F4DA} 3 codificadores de texto trabajan juntos: CLIP-L, CLIP-G y T5-XXL',
      youth_3: '\u{1F522} ¡El modelo necesita {vram} GB de VRAM solo para cargarse!',
      expert_1: '\u{1F3D7}\uFE0F Arquitectura: Rectified Flow + MMDiT con 38 bloques transformer',
      expert_2: '\u{1F4CA} Cuantización FP16/FP8: compromiso entre precisión y VRAM',
      expert_3: '\u{1F517} LoRA: Low-Rank Adaptation — solo el 0.1% de los parámetros reentrenados'
    },
    ethics: {
      kids_1: '\u{1F30D} ¡La IA aprende de imágenes en internet — por eso es importante ser justo con el arte de otras personas!',
      kids_2: '\u2696\uFE0F No se preguntó a todos los artistas si la IA podía aprender de ellos.',
      kids_3: '\u{1F91D} ¡Una buena IA respeta el trabajo de las personas!',
      youth_1: '\u{1F4DC} Los datos de entrenamiento a menudo provienen de internet. Los artistas debaten: ¿Uso justo o copia?',
      youth_2: '\u{1F3DB}\uFE0F El EU AI Act exige transparencia: ¿De dónde provienen los datos de entrenamiento?',
      youth_3: '\u{1F4AD} Pregunta: ¿A quién pertenece realmente una imagen generada por IA?',
      expert_1: '\u26A0\uFE0F LAION-5B fue creado parcialmente sin consentimiento de los creadores — zona gris legal.',
      expert_2: '\u{1F4CB} EU AI Act Art. 52: Requisito de etiquetado para contenido generado por IA',
      expert_3: '\u{1F50D} Model Cards y Datasheets: Mejores prácticas para transparencia en ML'
    },
    environment: {
      kids_1: '\u2601\uFE0F ¡Cada imagen de IA produce un poco de CO\u2082 — como conducir un auto, pero menos!',
      kids_2: '\u{1F331} Piensa: ¿Vale la pena la electricidad para esta imagen?',
      kids_3: '\u{1F31E} La energía para la IA a menudo proviene de centrales eléctricas — algunas limpias, otras no.',
      youth_1: '\u{1F3ED} Red eléctrica alemana: ~400g CO\u2082 por kWh — ¡eso se acumula!',
      youth_2: '\u{1F4C8} {co2}g CO\u2082 para esta imagen — ¡con 1000 imágenes serían {totalKg} kg!',
      youth_3: '\u{1F4A1} Consejo: Genera menos imágenes, pero con más reflexión — ahorra energía y CO\u2082.',
      expert_1: '\u{1F4CA} Cálculo: {watts}W \u00D7 {seconds}s \u00F7 3600 \u00D7 400g/kWh = {co2}g CO\u2082',
      expert_2: '\u{1F52C} Emisiones Scope 2: la ubicación del centro de datos es decisiva',
      expert_3: '\u26A1 PUE (Power Usage Effectiveness): Sobrecarga energética adicional por enfriamiento'
    },
    iceberg: {
      drawPrompt: 'La generación con IA usa mucha energía. Dibuja icebergs y observa lo que sucede...',
      redraw: 'Redibujar',
      startMelting: 'Iniciar derretimiento',
      melting: 'Iceberg derritiéndose...',
      melted: '¡Derretido!',
      meltedMessage: '{co2}g CO\u2082 producidos',
      comparison: 'Esta cantidad de CO\u2082 derrite aproximadamente {volume} cm\u00B3 de hielo ártico.',
      comparisonInfo: '(Cada tonelada de CO\u2082 = aprox. 6m\u00B3 de pérdida de hielo marino)',
      gpuPower: 'Consumo de energía de la tarjeta gráfica',
      gpuTemp: 'Temperatura de la tarjeta gráfica',
      co2Info: 'Emisiones de CO\u2082 por consumo de energía (basado en la mezcla energética alemana)',
      drawAgain: 'Dibuja más icebergs...'
    },
    pixel: {
      grafikkarte: 'Tarjeta gráfica',
      energieverbrauch: 'Consumo de energía',
      co2Menge: 'Cantidad de CO\u2082',
      smartphoneComparison: '¡Tendrías que mantener tu teléfono apagado durante {minutes} minutos para compensar este consumo de CO\u2082!',
      clickToProcess: '¡Haz clic en los píxeles de datos para generar una mini imagen!'
    },
    forest: {
      trees: 'Árboles',
      clickToPlant: '¡Haz clic para plantar árboles! Donde plantes un árbol, la fábrica desaparecerá.',
      gameOver: '¡El bosque se ha perdido!',
      treesPlanted: 'Plantaste {count} árboles.',
      complete: 'Generación completa',
      comparison: 'Un árbol promedio necesita {minutes} minutos para absorber esta cantidad de CO\u2082.'
    },
    rareearth: {
      clickToClean: '¡Haz clic en el lago para eliminar el lodo tóxico!',
      sludgeRemoved: 'Lodo eliminado',
      environmentHealth: 'Medio ambiente',
      gameOverInactive: 'Te rendiste... la minería continúa',
      infoBanner: 'La minería de tierras raras para chips de GPU deja lodo tóxico y destruye ecosistemas. Tus esfuerzos de limpieza no pueden igualar la velocidad de extracción.',
      instructionsCooldown: '\u23F3 {seconds}s',
      statsGpu: 'GPU',
      statsHealth: 'Medio ambiente',
      statsSludge: 'Lodo eliminado'
    }
  }
}
