import { createI18n } from 'vue-i18n'

export const SUPPORTED_LANGUAGES = [
  { code: 'de', label: 'Deutsch' },
  { code: 'en', label: 'English' },
  { code: 'tr', label: 'Türkçe' },
  { code: 'ko', label: '한국어' },
  { code: 'uk', label: 'Українська' },
  { code: 'fr', label: 'Français' },
] as const

export type SupportedLanguage = typeof SUPPORTED_LANGUAGES[number]['code']

/** Localized string object — en is mandatory, all others optional with fallback */
export type LocalizedString = { en: string; [key: string]: string }

/** Resolve a localized string for the given locale, falling back to English */
export function localized(obj: Record<string, string>, locale: string): string {
  return obj[locale] || obj.en || ''
}

const messages = {
  de: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Kreative KI-Transformationen'
    },
    form: {
      inputLabel: 'Dein Text',
      inputPlaceholder: 'z.B. Eine Blume auf der Wiese',
      schemaLabel: 'Transformationsstil',
      executeModeLabel: 'Ausführungsmodus',
      safetyLabel: 'Sicherheitsstufe',
      generateButton: 'Generieren'
    },
    schemas: {
      dada: 'Dada (Zufällig & Absurd)',
      bauhaus: 'Bauhaus (Geometrisch)',
      stillepost: 'Stille Post (Iterativ)'
    },
    executionModes: {
      eco: 'Eco (Schnell)',
      fast: 'Fast (Ausgewogen)',
      best: 'Best (Qualität)'
    },
    safetyLevels: {
      kids: 'Kinder',
      youth: 'Jugend',
      adult: 'Erwachsene',
      research: 'Forschung'
    },
    stages: {
      pipeline_starting: 'Pipeline startet',
      translation_and_safety: 'Übersetzung & Sicherheit',
      interception: 'Transformation',
      pre_output_safety: 'Ausgabe-Sicherheit',
      media_generation: 'Bild-Generierung',
      completed: 'Abgeschlossen'
    },
    status: {
      idle: 'Bereit',
      executing: 'Pipeline läuft...',
      connectionSlow: 'Verbindung langsam, Versuch läuft...',
      completed: 'Pipeline abgeschlossen!',
      error: 'Fehler aufgetreten'
    },
    entities: {
      input: 'Eingabe',
      translation: 'Übersetzung',
      safety: 'Sicherheitscheck',
      interception: 'Transformation',
      safety_pre_output: 'Ausgabe-Sicherheit',
      media: 'Generiertes Bild'
    },
    properties: {
      chill: 'chillig',
      chaotic: 'wild',
      narrative: 'Geschichten erzählen',
      algorithmic: 'nach Regeln gehen',
      historical: 'Geschichte',
      contemporary: 'Gegenwart',
      explore: 'KI austesten',
      create: 'Kunst machen',
      playful: 'bisschen verrückt',
      serious: 'eher ernst'
    },
    phase2: {
      title: 'Prompt-Eingabe',
      userInput: 'Dein Input',
      yourInput: 'Dein Input',
      yourIdea: 'Deine Idee: Um WAS soll es hier gehen?',
      rules: 'Deine Regeln: WIE soll Deine Idee umgesetzt werden?',
      yourInstructions: 'Deine Anweisungen',
      what: 'WAS',
      how: 'WIE',
      userInputPlaceholder: 'z.B. Eine Blume auf der Wiese',
      inputPlaceholder: 'Dein Text erscheint hier...',
      metaPrompt: 'Künstlerische Anweisung',
      instruction: 'Instruction',
      transformation: 'Künstlerische Transformation',
      metaPromptPlaceholder: 'Beschreibe die Transformation...',
      result: 'Ergebnis',
      expectedResult: 'Erwartetes Ergebnis',
      execute: 'Pipeline ausführen',
      executing: 'Läuft...',
      transforming: 'LLM transformiert...',
      startTransformation: 'Transformation starten',
      letsGo: 'Ok, leg los!',
      modified: 'Geändert',
      reset: 'Zurücksetzen',
      loadingConfig: 'Lade Konfiguration...',
      loadingMetaPrompt: 'Lade Meta-Prompt...',
      errorLoadingConfig: 'Fehler beim Laden der Konfiguration',
      errorLoadingMetaPrompt: 'Fehler beim Laden des Meta-Prompts',
      threeForces: '3 Kräfte wirken zusammen',
      twoForces: 'WAS + WIE → LLM → Ergebnis',
      yourPrompt: 'Dein Prompt:',
      writeYourText: 'Schreibe deinen Text...',
      examples: 'Beispiele',
      estimatedTime: '~12 Sekunden',
      stage12Time: '~5-10 Sekunden',
      willAppearAfterExecution: 'Wird nach Ausführung erscheinen...',
      back: 'Zurück',
      retry: 'Erneut versuchen',
      transformedPrompt: 'Transformierter Prompt',
      notYetTransformed: 'Noch nicht transformiert...',
      transform: 'Transformieren',
      reTransform: 'Noch mal anders',
      startAI: 'KI, bearbeite meine Eingabe',
      aiWorking: 'KI arbeitet...',
      continueToMedia: 'Weiter zum Bild generieren',
      readyForMedia: 'Bereit für Bildgenerierung',
      stage1: 'Stage 1: Übersetzung + Sicherheit...',
      stage2: 'Stage 2: Transformation...',
      selectMedia: 'Wähle dein Medium:',
      mediaImage: 'Bild',
      mediaAudio: 'Sound',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Bald verfügbar',
      generateMedia: 'Start!'
    },
    phase3: {
      generating: 'Bild wird generiert...',
      generatingHint: '~30 Sekunden'
    },
    common: {
      back: 'Zurück',
      loading: 'Lädt...',
      error: 'Fehler',
      retry: 'Erneut versuchen',
      cancel: 'Abbrechen',
      checkingSafety: 'Prüft...'
    },
    gallery: {
      title: 'Favoriten',  // Session 145: "Meine" redundant mit Switch
      empty: 'Noch keine Favoriten',
      favorite: 'Zu Favoriten',
      unfavorite: 'Aus Favoriten entfernen',
      continue: 'Weiterentwickeln',
      restore: 'Wiederherstellen',
      viewMine: 'Meine Favoriten',  // Session 145
      viewAll: 'Alle Favoriten'  // Session 145
    },
    settings: {
      authRequired: 'Authentifizierung erforderlich',
      authPrompt: 'Bitte geben Sie das Passwort ein, um auf die Einstellungen zuzugreifen:',
      passwordPlaceholder: 'Passwort eingeben...',
      authenticate: 'Anmelden',
      authenticating: 'Authentifiziere...',
      // Admin page
      title: 'Administration',
      tabs: {
        export: 'Forschungsdaten',
        config: 'Konfiguration',
        demos: 'Minigame-Demo',
        matrix: 'Modell-Matrix'
      },
      loading: 'Einstellungen laden...',
      presets: {
        title: 'Modell-Presets',
        help: 'Verwenden Sie den Reiter <strong>Modell-Matrix</strong>, um alle verfügbaren Presets zu sehen und mit einem Klick anzuwenden.',
        openMatrix: 'Modell-Matrix öffnen'
      },
      testingTools: {
        title: 'Testtools für Pädagog*innen',
        help: 'Testen und erkunden Sie die pädagogischen Minigames und Animationen, bevor Sie sie mit Lernenden nutzen.',
        openPreview: 'Minigame-Vorschau öffnen',
        pixelEditor: 'Pixel Template Editor',
        includes: 'Enthält: Pixel-Animation, Eisberg-Schmelzen, Wald-Spiel, Seltene Erden'
      },
      general: {
        title: 'Allgemeine Konfiguration',
        uiMode: 'UI-Modus',
        uiModeHelp: 'Komplexitätsstufe der Oberfläche',
        kids: 'Kinder (8–12)',
        youth: 'Jugend (13–17)',
        expert: 'Expert',
        safetyLevel: 'Sicherheitsstufe',
        defaultLanguage: 'Standardsprache',
        germanDe: 'Deutsch (de)',
        englishEn: 'Englisch (en)',
        turkishTr: 'Türkisch (tr)',
        koreanKo: 'Koreanisch (ko)',
        ukrainianUk: 'Ukrainisch (uk)',
        frenchFr: 'Französisch (fr)'
      },
      safety: {
        kidsTitle: 'Kinder (8–12)',
        kidsDesc: 'Alle Filter aktiv: §86a, DSGVO, Jugendschutz (altersgerechte Parameter), VLM-Bildcheck',
        youthTitle: 'Jugend (13–17)',
        youthDesc: 'Alle Filter aktiv: §86a, DSGVO, Jugendschutz (Youth-Parameter), VLM-Bildcheck',
        adultTitle: 'Erwachsene',
        adultDesc: '§86a + DSGVO aktiv. Kein Jugendschutz, kein VLM-Bildcheck.',
        researchTitle: 'Forschungsmodus',
        researchDesc: 'KEINE Sicherheitsfilter aktiv. Ausschließlich zur Nutzung durch Forschungseinrichtungen im Rahmen wissenschaftlicher Forschungsprojekte zulässig.'
      },
      safetyModels: {
        title: 'Lokale Sicherheitsmodelle',
        help: 'Lokal via Ollama — Personennamen und Safety-Checks verlassen nie das System',
        safetyModel: 'Sicherheitsmodell',
        safetyModelHelp: 'Guard-Modell für Content-Safety (§86a, Jugendschutz)',
        dsgvoModel: 'DSGVO-Verifikationsmodell',
        dsgvoModelHelp: 'General-Purpose-Modell für DSGVO-NER-Verifikation (kein Guard-Modell)',
        vlmModel: 'VLM-Sicherheitsmodell',
        vlmModelHelp: 'Vision-Modell für Post-Generierungs-Bildsicherheitsprüfung (kids/youth)',
        fast: 'schnell, minimal',
        recommended: 'empfohlen'
      },
      dsgvo: {
        title: 'DSGVO-Warnung',
        notCompliant: 'Die folgenden Modelle sind <strong>NICHT DSGVO-konform</strong> (Daten werden außerhalb der EU verarbeitet):',
        compliantHint: 'DSGVO-konforme Optionen:'
      },
      models: {
        title: 'Modellkonfiguration',
        help: 'Modell-Bezeichner mit Anbieter-Präfix: local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: 'Die Nutzung der Modell-Matrix wird empfohlen. Sie können Ihre Einstellungen hier aber frei konfigurieren.',
        ollamaAvailable: '{count} Ollama-Modelle verfügbar (eingeben oder aus Dropdown wählen)',
        stage1Text: 'Stufe 1 – Textmodell',
        stage1Vision: 'Stufe 1 – Vision-Modell',
        stage2Interception: 'Stufe 2 – Interception-Modell',
        stage2Optimization: 'Stufe 2 – Optimierungsmodell',
        stage3: 'Stufe 3 – Übersetzungs-/Sicherheitsmodell',
        stage4Legacy: 'Stufe 4 – Legacy-Modell',
        chatHelper: 'Chat-Hilfsmodell',
        imageAnalysis: 'Bildanalyse-Modell',
        coding: 'Code-Generierung (Tone.js, p5.js)'
      },
      api: {
        title: 'API-Konfiguration',
        llmProvider: 'LLM-Anbieter',
        localFramework: 'Lokales LLM-Framework',
        externalProvider: 'Externer LLM-Anbieter',
        cloudProvider: 'Cloud-LLM-Anbieter – API-Schlüssel erforderlich',
        noneLocal: 'Keine (nur lokal, DSGVO)',
        mistralEu: 'Mistral AI (EU-basiert)',
        anthropicDirect: 'Anthropic Direct API (NICHT DSGVO)',
        openaiDirect: 'OpenAI Direct API (NICHT DSGVO)',
        openrouterDirect: 'OpenRouter (NICHT DSGVO, EU-Routing verfügbar)',
        mistralInfo: 'Mistral AI (EU-basiert)',
        mistralDsgvo: 'DSGVO-konform (EU-Infrastruktur)',
        anthropicInfo: 'Anthropic Direct API',
        anthropicNotDsgvo: 'NICHT DSGVO-konform',
        anthropicWarning: 'Daten werden außerhalb der EU verarbeitet. Nur für nicht-pädagogische Kontexte verwenden.',
        openaiInfo: 'OpenAI Direct API',
        openaiNotDsgvo: 'NICHT DSGVO-konform (US-basiert)',
        openaiWarning: 'Daten werden in den USA verarbeitet. Nur für nicht-pädagogische Kontexte verwenden.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: 'NICHT DSGVO-konform (US-Unternehmen)',
        openrouterWarning: 'EU-Server-Routing in den OpenRouter-Einstellungen konfigurierbar, aber Unternehmen sitzt in den USA.',
        storedIn: 'Gespeichert in',
        currentKey: 'Aktuell'
      },
      save: {
        saveApply: 'Speichern & Anwenden',
        saving: 'Speichere...',
        applying: 'Anwenden...',
        success: 'Einstellungen gespeichert und angewendet',
        presetApplied: 'Preset angewendet: {preset}'
      }
    },
    pipeline: {
      yourInput: 'Dein Input',
      result: 'Ergebnis',
      generatedMedia: 'Erzeugtes Bild'
    },
    landing: {
      subtitlePrefix: 'Pädagogisch-künstlerische Experimentierplattform des',
      subtitleSuffix: 'für den explorativen Einsatz von generativer KI in der kulturell-ästhetischen Medienbildung',
      research: '',
      features: {
        textTransformation: {
          title: 'Text-Transformation',
          description: 'Perspektivwechsel durch KI: Finde und verändere Deine Ideen durch künstlerische Haltungen und Verfremdungen.'
        },
        imageTransformation: {
          title: 'Bild-Transformation',
          description: 'Bilder durch verschiedene Modelle und Perspektiven in neue Bilder und Videos verwandeln.'
        },
        multiImage: {
          title: 'Bildfusion',
          description: 'Mehrere Bilder kombinieren und durch KI-Modelle zu neuen Bild-Kompositionen verschmelzen.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Visuelle Workflow-Komposition — Module per Drag & Drop zu eigenen KI-Pipelines verbinden.'
        },
        music: {
          title: 'Musikgenerierung',
          description: 'Experimentiere mit Musik, Sound und Lyrics.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vektorraum-Forschung — Surrealisierung, Dimensionselimination, Embedding-Interpolation.'
        }
      }
    },
    research: {
      locked: 'Nur im Forschungsmodus verfügbar',
      lockedHint: 'Erfordert Safety-Level „Erwachsene" oder „Forschung" (config.py)',
      complianceTitle: 'Hinweis zum Forschungsmodus',
      complianceWarning: 'Im Forschungsmodus sind keine Sicherheitsfilter für Prompts und generierte Bilder aktiv. Es können unerwartete oder unangemessene Ergebnisse entstehen.',
      complianceAge: 'Dieser Modus ist nicht empfohlen für Personen unter 16 Jahren.',
      complianceConfirm: 'Ich bestätige, dass ich die Hinweise verstanden habe',
      complianceCancel: 'Abbrechen',
      complianceProceed: 'Fortfahren'
    },
    presetOverlay: {
      title: 'Perspektive wählen',
      close: 'Schließen'
    },
    imageUpload: {
      clickHere: 'Klicke hier',
      orDragImage: 'oder ziehe ein Bild hierher',
      formatHint: 'PNG, JPG, WEBP (max 10MB)',
      invalidFormat: 'Ungültiges Dateiformat. Nur PNG, JPG und WEBP erlaubt.',
      fileTooLarge: 'Datei zu groß. Maximum: {max}MB',
      uploadFailed: 'Upload fehlgeschlagen',
      infoOriginal: 'Original:',
      infoSize: 'Größe:'
    },
    mediaInput: {
      choosePreset: 'Perspektive wählen',
      translateToEnglish: 'Ins Englische übersetzen',
      copy: 'Kopieren',
      paste: 'Einfügen',
      delete: 'Löschen',
      loading: 'Lädt...',
      contentBlocked: 'Inhalt blockiert'
    },
    nav: {
      about: 'Über das Projekt',
      impressum: 'Impressum',
      privacy: 'Datenschutz',
      docs: 'Dokumentation',
      language: 'Sprache wechseln',
      settings: 'Einstellungen',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'Neuer Workflow',
      importWorkflow: 'Importieren',
      exportWorkflow: 'Exportieren',
      execute: 'Ausführen',
      ready: 'Bereit',
      errors: 'Fehler',
      discardWorkflow: 'Aktuellen Workflow verwerfen?',
      importError: 'Fehler beim Importieren der Datei',
      selectTransformation: 'Transformation wählen',
      selectOutput: 'Ausgabe-Modell wählen',
      search: 'Suchen...',
      noResults: 'Keine Ergebnisse gefunden',
      dragHint: 'Klicke oder ziehe Module auf die Arbeitsfläche',
      editNameHint: '(doppelklicken zum Bearbeiten)',
      modules: 'Module',
      toggleSidebar: 'Sidebar ein/aus',
      dsgvoTooltip: 'Canvas-Workflows können externe LLM-APIs nutzen. Die DSGVO-Konformität liegt in der Verantwortung der Nutzer:innen.',
      batchExecute: 'Batch-Ausführung',
      batchExecution: 'Batch-Ausführung',
      batchAbort: 'Batch abbrechen',
      abort: 'Abbrechen',
      cancel: 'Abbrechen',
      loading: 'Laden...',
      executingWorkflow: 'Workflow wird ausgeführt...',
      starting: 'Starte...',
      nodes: 'Knoten',
      batchRunCount: 'Anzahl Runs',
      batchUseSeed: 'Basis-Seed verwenden',
      batchBaseSeed: 'Basis-Seed',
      batchSeedHint: 'Jeder Run: Seed + Index',
      batchStart: 'Batch starten',
      stage: {
        configSelectPlaceholder: 'Auswählen...',
        evaluationCriteriaFallback: 'Bewertungskriterien...',
        feedbackInputTitle: 'Feedback-Eingang',
        deleteTitle: 'Löschen',
        selectLlmPlaceholder: 'LLM wählen...',
        resizeTitle: 'Größe ändern',
        input: {
          promptPlaceholder: 'Dein Prompt...'
        },
        imageInput: {
          uploadLabel: 'Bild hochladen'
        },
        interception: {
          contextPromptLabel: 'Context-Prompt',
          contextPromptPlaceholder: 'Transformations-Anweisungen...'
        },
        translation: {
          translationPromptLabel: 'Übersetzungs-Prompt',
          translationPromptPlaceholder: 'Übersetzungsanweisungen...'
        },
        modelAdaption: {
          targetModelLabel: 'Zielmodell',
          noAdaptionOption: 'Keine Adaption',
          videoModelsOption: 'Video-Modelle',
          audioModelsOption: 'Audio-Modelle'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Vergleichs-Kriterien',
          criteriaPlaceholder: 'z.B. Vergleiche nach Originalität, Klarheit, Detailreichtum...',
          infoText: 'Verbinde bis zu 3 Text-Outputs'
        },
        seed: {
          modeLabel: 'Modus',
          modeFixed: 'Fest',
          modeRandom: 'Zufällig',
          valueLabel: 'Wert',
          baseLabel: 'Basis'
        },
        resolution: {
          customOption: 'Benutzerdefiniert',
          widthLabel: 'Breite',
          heightLabel: 'Höhe'
        },
        collector: {
          emptyText: 'Warte auf Ausführung...'
        },
        evaluation: {
          typeLabel: 'Bewertungstyp',
          typeCreativity: 'Kreativität',
          typeQuality: 'Qualität',
          typeCustom: 'Eigene',
          criteriaLabel: 'Bewertungskriterien',
          outputTypeLabel: 'Ausgabe-Typ',
          outputCommentary: 'Kommentar + Binary',
          outputScore: 'Kommentar + Score + Binary',
          outputAll: 'Alle',
          evalPassTitle: 'Bestanden (weiter)',
          evalFailTitle: 'Feedback (Rückkanal)',
          evalCommentaryTitle: 'Kommentar (weiter)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Vision-Modell wählen...',
          frameworkLabel: 'Analyse-Framework',
          frameworkPanofsky: 'Kunsthistorisch (Panofsky)',
          frameworkEducational: 'Bildungstheoretisch',
          frameworkEthical: 'Ethisch',
          frameworkCritical: 'Kritisch/Dekolonial',
          frameworkCustom: 'Eigene Anweisung',
          customPromptLabel: 'Analyse-Prompt',
          customPromptPlaceholder: 'Beschreibe, wie das Bild analysiert werden soll...'
        },
        display: {
          imageAlt: 'Vorschau',
          emptyText: 'Vorschau (nach Ausführung)'
        }
      }
    },
    about: {
      title: 'Über das UCDCAE AI LAB',
      intro: 'Das UCDCAE AI LAB ist eine pädagogisch-künstlerische Experimentierplattform des UNESCO Chair in Digital Culture and Arts in Education für den explorativen Einsatz von generativer Künstlicher Intelligenz in der kulturell-ästhetischen Medienbildung. Es wurde im Rahmen der Projekte AI4ArtsEd und COMeARTS entwickelt.',
      project: {
        title: 'Das Projekt',
        description: 'KI verändert Gesellschaft und Arbeitswelt; sie wird zunehmend Thema der Bildung. Das Projekt sondiert Chancen, Bedingungen und Grenzen des pädagogischen Einsatzes künstlicher Intelligenz (KI) in kulturell diversitätssensiblen Settings der Kulturellen Bildung (KuBi).',
        paragraph2: 'In drei Teilprojekten – Allgemeinpädagogik (TPap), Informatik (TPinf) und Kunstpädagogik (TPkp) – greifen kreativitätsorientierte pädagogische KI-Praxisforschung und informatische KI-Konzeption und Programmierung in enger Kooperation ineinander. Das Projekt bezieht hierzu von Beginn an künstlerisch-pädagogische Praxisakteure in den Gestaltungsprozess systematisch ein; es agiert als Brücke zwischen der professionellen (qualitätsbezogenen, ästhetischen, ethischen und wertebezogenen) pädagogisch-praktischen Implementation einerseits und dem Umsetzungs- und Trainingsprozess des informatischen Teilprojekts andererseits.',
        paragraph3: 'Aus einem insgesamt ca. zweijährigen partizipativen Designprozess soll eine Opensource-KI-Technologie hervorgehen, die auslotet, inwieweit KI-Systeme unter günstigen Realbedingungen bereits auf ihrer Strukturebene künstlerisch-pädagogische Maßgaben einbeziehen können.',
        paragraph4: 'Dabei stehen a) die zukünftige Anwendbarkeit und der Mehrgewinn hochinnovativer Technologien für die Kulturelle Bildung im Zentrum, b) Reichweite und Grenzen der KI-Literacy von Lehrenden und Lernenden, sowie c) die übergreifende Frage nach der Bewertbarkeit und Bewertung der Transformation pädagogischer Settings durch komplexe nonhumane Akteure im Sinne einer pädagogischen Ethik und Technikfolgenabschätzung.',
        moreInfo: 'Weitere Informationen:'
      },
      subproject: {
        title: 'Teilprojekt "Allgemeine Pädagogik"',
        description: 'Das Teilprojekt "Allgemeine Pädagogik" beforscht im Rahmen der dem Verbundprojekt gemeinsamen Fragestellung Möglichkeiten und Grenzen eines auf partizipativer Praxisforschung aufsetzenden künstlerisch-pädagogischen KI-Designprozesses. Es führt zu diesem Zweck im ersten Projektjahr eine Serie von Recherchen, Analysen, Expert_innenworkshops und OpenSpaces durch. Die nachfolgende, in mehreren Zyklen als Feedback-Loop angelegte Projektphase erforscht den Einsatz eines Prototypen mit pädagogischen Prakter_innen und Artist-Educators v.a. der non-formalen kulturellen Bildung als relationalen und kollektiven transformativen Bildungsprozess.'
      },
      team: {
        title: 'Team',
        projectLead: 'Projektleitung',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Institut für Pädagogik',
        leadChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
        leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
        researcher: 'Wissenschaftliche Mitarbeiterin',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Institut für Pädagogik',
        researcherChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
        researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
      },
      funding: {
        title: 'Gefördert vom'
      }
    },
    legal: {
      impressum: {
        title: 'Impressum',
        publisher: 'Herausgeber',
        represented: 'Vertreten durch den Präsidenten',
        responsible: 'Inhaltlich verantwortlich gem. § 18 Abs. 2 MStV',
        authority: 'Zuständige Aufsichtsbehörde',
        moreInfo: 'Weitere Informationen',
        moreInfoText: 'Das vollständige Impressum der FAU:',
        funding: 'Gefördert vom'
      },
      privacy: {
        title: 'Datenschutzerklärung',
        notice: 'Hinweis: Generierte Inhalte werden zu Forschungszwecken auf dem Server gespeichert. Es werden keine User- oder IP-Daten erfasst. Hochgeladene Bilder werden nicht gespeichert.',
        usage: 'Die Nutzung dieser Plattform ist ausschließlich eingetragenen Kooperationspartnern des UCDCAE AI LAB erlaubt. Es gelten die in diesem Rahmen vereinbarten datenschutzbezogenen Absprachen. Haben Sie hierzu Fragen, melden Sie sich bitte bei vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Dokumentation & Anleitung',
      intro: {
        title: 'Willkommen',
        content: 'Kreative Experimente mit KI-Transformationen.'
      },
      gettingStarted: {
        title: 'Erste Schritte',
        step1: 'Eigenschaften aus Quadranten wählen',
        step2: 'Text oder Bild eingeben',
        step3: 'Transformation starten'
      },
      modes: {
        title: 'Modi',
        mode1: { name: 'Direkt', desc: 'Schnelle Experimente' },
        mode2: { name: 'Text', desc: 'Textbasierte Transformationen' },
        mode3: { name: 'Bild', desc: 'Bildbasierte Verfahren' }
      },
      support: {
        title: 'Unterstützung',
        content: 'Bei Fragen:'
      },
      wikipedia: {
        title: 'Wikipedia-Recherche',
        subtitle: 'Wissen über die Welt als Teil künstlerischer Prozesse',
        feature: 'Künstlerische Prozesse erfordern nicht nur ästhetisches Wissen, sondern auch Wissen über Sachverhalte in der Welt. Die KI recherchiert während der Transformation auf Wikipedia, um faktische Informationen zu finden.',
        languages: 'Über 70 Sprachen werden unterstützt',
        languagesDesc: 'Die KI wählt automatisch die passende sprachliche Wikipedia für das jeweilige Thema:',
        examples: {
          nigeria: 'Thema über Nigeria → Hausa, Yoruba, Igbo oder Englisch',
          india: 'Thema über Indien → Hindi, Tamil, Bengali oder andere regionale Sprachen',
          indigenous: 'Indigene Kulturen → Quechua, Māori, Inuktitut usw.'
        },
        why: 'Transparenz: Was weiß die KI?',
        whyDesc: 'Das System zeigt alle Recherche-Versuche an: Sowohl gefundene Artikel (als anklickbare Links) als auch Begriffe, zu denen nichts gefunden wurde. So wird sichtbar, was die KI zu wissen meint – und was nicht.',
        culturalRespect: 'Einladung zum Selbst-Recherchieren',
        culturalRespectDesc: 'Die angezeigten Wikipedia-Links sind eine Einladung, selbst mehr zu erfahren. Klicken Sie auf die Links, um die Quellen zu prüfen und Ihr eigenes Wissen zu erweitern.',
        limitations: 'Die KI-Recherche ist ein Hilfsmittel, kein Ersatz für eigene Auseinandersetzung mit dem Thema.'
      }
    },
    multiImage: {
      image1Label: 'Bild 1',
      image2Label: 'Bild 2 (optional)',
      image3Label: 'Bild 3 (optional)',
      contextLabel: 'Sage was Du mit den Bildern machen möchtest',
      contextPlaceholder: 'z.B. Füge das Haus aus Bild 2 und das Pferd aus Bild 3 in Bild 1 ein. Behalte Farben und Stil von Bild 1 bei.',
      modeTitle: 'Mehrere Bilder → Bild',
      selectConfig: 'Wähle dein Modell:',
      generating: 'Bilder werden fusioniert...'
    },
    imageTransform: {
      imageLabel: 'Dein Bild',
      contextLabel: 'Sage was Du an dem Bild verändern möchtest',
      contextPlaceholder: 'z.B. Verwandle es in ein Ölgemälde... Mache es bunter... Füge einen Sonnenuntergang hinzu...'
    },
    videoGeneration: {
      promptLabel: 'Deine Video-Idee',
      promptPlaceholder: 'z.B. Ein Heißluftballon schwebt über einer Berglandschaft bei Sonnenuntergang...',
      modelLabel: 'Wähle ein Video-Modell:',
      generating: 'Video wird generiert...'
    },
    textTransform: {
      inputLabel: 'Deine Idee = WAS?',
      inputTooltip: 'Hier trägst Du ein, worum es gehen soll.',
      inputPlaceholder: 'z.B. Ein Fest in meiner Straße: ...',
      contextLabel: 'Deine Regeln = WIE?',
      contextTooltip: 'Hier trägst Du ein, wie Deine Idee dargestellt werden soll, oder klicke auf das Kreis-Symbol!',
      contextPlaceholder: 'z.B. Beschreibe alles so, wie es die Vögel auf den Bäumen wahrnehmen!',
      resultLabel: 'Idee + Regeln = Prompt',
      resultPlaceholder: 'Prompt erscheint nach Start-Klick (oder eigenen Text eingeben)',
      optimizedLabel: 'Modell-Optimierter Prompt',
      optimizedPlaceholder: 'Der optimierte Prompt erscheint nach Modellauswahl.'
    },
    training: {
      info: {
        title: 'Hinweis zum LoRA-Training',
        studioDescription: 'Trainiere eigene LoRA-Modelle für Stable Diffusion 3.5 Large mit deinen Bildern.',
        description: 'Dieses eingebaute Training ist für schnelle Tests gedacht.',
        limitations: 'Einschränkungen',
        limitationDuration: 'Training dauert 1-3 Stunden',
        limitationBlocking: 'Blockiert die Bildgenerierung während des Trainings',
        limitationConfig: 'Begrenzte Konfigurationsmöglichkeiten',
        showMore: 'Mehr erfahren',
        showLess: 'Weniger anzeigen'
      },
      placeholders: {
        projectName: 'z.B. Unser Schulgebäude',
        triggerWords: 'z.B. unser_schulgebaeude, schulhof, klassenzimmer'
      },
      labels: {
        projectName: 'Projektname',
        triggerWords: 'Trigger-Wörter',
        triggerHelp: 'Kommagetrennte Tags. Erstes = Haupt-Trigger, Rest = zusätzliche Tags pro Bild.',
        images: 'Trainingsbilder (10–50 empfohlen)',
        dropZone: 'Bilder hierher ziehen oder klicken',
        imagesSelected: '{count} Bilder ausgewählt',
        logs: 'Trainings-Log',
        waiting: 'Warte auf Trainingsstart...'
      },
      buttons: {
        start: 'Training starten',
        stop: 'Stopp',
        inProgress: 'Training läuft...',
        delete: 'Projektdaten löschen (DSGVO)',
        cancel: 'Abbrechen'
      },
      vram: {
        title: 'GPU VRAM Prüfung',
        checking: 'Prüfe VRAM...',
        used: 'belegt',
        free: 'frei',
        notEnough: 'Nicht genügend freier VRAM für das Training (benötigt {gb} GB).',
        clearQuestion: 'VRAM freigeben um fortzufahren?',
        enough: 'Genügend VRAM für das Training verfügbar.',
        clearing: 'Gebe VRAM frei...',
        newFree: 'Neu verfügbar',
        clearBtn: 'ComfyUI + Ollama VRAM freigeben'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Altersfilter',
      dsgvo_ner: 'DSGVO',
      dsgvo_llm: 'DSGVO',
      translation: '\u2192 EN',
      fast_filter: 'Inhalt',
      llm_context_check: 'Inhalt (LLM)',
      llm_safety_check: 'Jugendschutz',
      llm_check_failed: 'Pr\u00FCfung fehlgeschlagen',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Dein Prompt war in Ordnung, aber das erzeugte Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Das kann passieren \u2014 die Bildgenerierung ist nicht immer vorhersagbar. Versuche es einfach nochmal, jede Generierung ist anders!',
      para86a: 'Dein Prompt wurde blockiert, weil er Symbole oder Begriffe enth\u00E4lt, die nach deutschem Recht (\u00A786a StGB) verboten sind. Diese Regel sch\u00FCtzt uns alle vor Hass und Gewalt. Versuche es mit einem anderen Thema!',
      dsgvo: 'Dein Prompt wurde blockiert, weil er etwas enth\u00E4lt, das wie ein Personenname aussieht. Das ist durch die Datenschutzgrundverordnung (DSGVO) gesch\u00FCtzt. Verwende stattdessen Beschreibungen wie \"ein M\u00E4dchen\" oder \"ein alter Mann\" statt Namen.',
      kids: 'Dein Prompt wurde vom Kinder-Schutzfilter blockiert. Manche Begriffe sind f\u00FCr Kinder nicht geeignet, weil sie erschreckend oder verst\u00F6rend sein k\u00F6nnen. Versuche, deine Idee mit freundlicheren Worten zu beschreiben!',
      youth: 'Dein Prompt wurde vom Jugendschutzfilter blockiert. Manche Inhalte sind auch f\u00FCr Jugendliche nicht geeignet. Versuche, deine Idee anders zu formulieren!',
      generic: 'Dein Prompt wurde vom Sicherheitssystem blockiert. Das System sch\u00FCtzt dich vor ungeeigneten Inhalten. Versuche es mit einer anderen Formulierung!',
      inputImage: 'Das hochgeladene Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Bitte verwende ein anderes Bild.',
      vlmSaw: 'Die Bild-KI sah',
      systemUnavailable: 'Das Sicherheitssystem (Ollama) reagiert nicht, daher kann keine weitere Verarbeitung erfolgen. Bitte den Systemadministrator kontaktieren.',
      suggestionLoading: 'Moment, ich habe eine Idee...',
      suggestionError: 'Ich konnte gerade keinen Vorschlag generieren. Versuch es einfach nochmal anders!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine - Semantische Vektorfusion',
      infoDescription: 'Dieser Workflow fusioniert zwei Prompts auf der Ebene semantischer Vektoren. Das Ergebnis ist keine einfache Mischung, sondern eine tiefere mathematische Verbindung der Bedeutungsräume.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Erkunde, wie KI-Modelle Bedeutung als Zahlenräume repräsentieren. Was passiert, wenn wir verschiedene Konzepte mathematisch verschmelzen?',
      techTitle: 'Technische Details',
      techText: 'Modell: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Partial Elimination - Vektor-Dekonstruktion',
      infoDescription: 'Dieser Workflow manipuliert gezielt Teile des semantischen Vektors. Durch das Eliminieren bestimmter Dimensionen können wir beobachten, welche Aspekte der Bedeutung verloren gehen.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Verstehe, wie Bedeutung in verschiedenen Dimensionen des Vektorraums kodiert ist. Was bleibt übrig, wenn wir Teile "ausschalten"?',
      techTitle: 'Technische Details',
      techText: 'Modell: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Text-Encoder',
      modeLabel: 'Eliminationsmodus',
      dimensionRange: 'Dimensions-Bereich',
      selected: 'Ausgewählt',
      dimensions: 'Dimensionen',
      emptyTitle: 'Warte auf Generierung...',
      emptySubtitle: 'Die Ergebnisse erscheinen hier',
      referenceLabel: 'Referenzbild',
      referenceDesc: 'Unmanipulierte Ausgabe (Original)',
      innerLabel: 'Innerer Bereich eliminiert',
      outerLabel: 'Äußerer Bereich eliminiert'
    },
    surrealizer: {
      infoTitle: 'Surrealisierer — Extrapolation jenseits des Bekannten',
      infoDescription: 'Zwei KI-"Gehirne" lesen deinen Text: CLIP-L versteht Sprache durch Bilder, T5 versteht sie rein sprachlich. Der Regler mischt nicht einfach zwischen beiden — er schiebt das Bild weit über das hinaus, was T5 allein erzeugen würde. Die KI muss dann Vektoren interpretieren, die sie im Training nie gesehen hat. Das Ergebnis: KI-Halluzinationen — Bilder, die kein Prompt direkt erzeugen könnte.',
      purposeTitle: 'Der Regler',
      purposeText: 'α < 0: CLIP-L wird verstärkt, T5 negiert — die oberen 3328 Dimensionen (wo CLIP-L nur Nullen hat) erhalten invertierte T5-Vektoren. Die Cross-Attention-Muster im Transformer kehren sich um: visuell getriebene Halluzinationen. ◆ α = 0: reines CLIP-L — normales Bild. ◆ α = 1: reines T5-XXL — noch normal, aber andere Qualität. ◆ α > 1: Extrapolation über T5 hinaus. Bei α = 20 schiebt die Formel das Embedding 19× über T5 hinweg in unerforschten Vektorraum — sprachlich getriebene Halluzinationen. ◆ Sweet Spot: α = 15–35.',
      techTitle: 'Wie es funktioniert',
      techText: 'Dein Prompt wird getrennt durch zwei Encoder geschickt: CLIP-L (visuell trainiert, 77 Tokens, 768 Dimensionen → aufgefüllt auf 4096) und T5-XXL (sprachlich trainiert, 512 Tokens, 4096 Dimensionen). Die ersten 77 Token-Positionen werden per Formel fusioniert: (1-α)·CLIP-L + α·T5. Die restlichen T5-Tokens (78–512) bleiben unverändert als semantischer Anker — sie halten das Bild an deinem Text fest, egal wie extrem α wird. Bei α > 1 entsteht keine Mischung, sondern Extrapolation: Vektoren, die kein Training je erzeugt hat. Bei α < 0 wird T5 negiert und CLIP-L verstärkt — qualitativ andere Halluzinationen, weil die Cross-Attention-Muster im Transformer invertiert werden.',
      sliderLabel: 'Extrapolation (α)',
      sliderNormal: 'normal',
      sliderWeird: 'weird',
      sliderCrazy: 'crazy',
      sliderExtremeWeird: 'super weird',
      sliderExtremeCrazy: 'super crazy',
      sliderHint: "α<0: über CLIP hinaus {'|'} α=0: reines CLIP {'|'} α=1: reines T5 {'|'} α>1: über T5 hinaus",
      expandLabel: 'Prompt für T5 erweitern',
      expandSuggest: 'Kurzer Prompt erkannt — T5-Erweiterung verbessert die Ergebnisse bei wenigen Wörtern deutlich.',
      expandHint: 'Dein Prompt hat wenige Wörter (~{count} CLIP-Tokens). Für optimale Halluzinationen kann die KI den T5-Kontext narrativ erweitern.',
      expandActive: 'Erweitere Prompt...',
      expandResultLabel: 'T5-Erweiterung (nur für T5-Encoder)',
      advancedLabel: 'Weitere Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      negativeHint: 'Wird mit gleichem α extrapoliert. Bestimmt, woVON das Bild weg-extrapoliert wird — verschiedene Negativ-Prompts erzeugen grundlegend verschiedene Bildästhetiken.',
      cfgLabel: 'CFG Scale',
      cfgHint: 'Classifier-Free Guidance: Stärke des Prompt-Einflusses. Höher = stärkerer Effekt, weniger Variation.'
    },
    musicGeneration: {
      infoTitle: 'Musik-Generierung',
      infoDescription: 'Erstelle Musik aus Texten und Style-Tags. Die KI generiert Melodien, Rhythmen und Harmonien basierend auf deinen Lyrics und Genre-Angaben.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Erkunde wie KI musikalische Konzepte interpretiert. Wie beeinflusst die Wortwahl in den Lyrics die Melodie?',
      lyricsLabel: 'Lyrics (Text)',
      lyricsPlaceholder: '[Verse]\nDeine Lyrics hier...\n\n[Chorus]\nRefrain...',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: 'Wähle ein Musik-Modell:',
      generate: 'Musik generieren',
      generating: 'Musik wird generiert...'
    },
    musicGen: {
      simpleMode: 'Einfach',
      advancedMode: 'Erweitert',
      lyricsLabel: 'Lyrics',
      lyricsPlaceholder: 'Schreibe deine Song-Lyrics mit Strukturmarkern wie [Verse], [Chorus], [Bridge]...\n\nBeispiel:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'Genre, Stimmung, Instrumente...\n\nBeispiel: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: 'Lyrics & Tags verfeinern',
      refinedLyricsLabel: 'Verfeinerte Lyrics',
      refinedLyricsPlaceholder: 'Hier erscheinen deine verfeinerten Lyrics...',
      refiningLyricsMessage: 'Die KI verfeinert deine Lyrics...',
      refinedTagsLabel: 'Verfeinerte Tags',
      refinedTagsPlaceholder: 'Hier erscheinen die verfeinerten Style Tags...',
      refiningTagsMessage: 'Die KI generiert passende Style Tags...',
      selectModel: 'Wähle ein Musik-Modell',
      generateButton: 'Musik generieren',
      quality: 'Qualität'
    },
    musicGenV2: {
      lyricsWorkshop: 'Lyrics Workshop',
      lyricsInput: 'Dein Text',
      lyricsPlaceholder: 'Schreibe Lyrics, ein Thema, Stichworte oder eine Stimmung...',
      themeToLyrics: 'Stichworte zu Songtext',
      refineLyrics: 'Songtext strukturieren',
      resultLabel: 'Ergebnis',
      resultPlaceholder: 'Hier erscheinen deine Lyrics...',
      expandingTheme: 'Die KI schreibt einen Songtext aus deinen Stichworten...',
      refiningLyrics: 'Die KI strukturiert deinen Songtext...',
      soundExplorer: 'Sound Explorer',
      suggestFromLyrics: 'Aus Lyrics vorschlagen',
      suggestingTags: 'Die KI analysiert deine Lyrics...',
      mostImportant: 'wichtigste',
      dimGenre: 'Genre',
      dimTimbre: 'Klangfarbe',
      dimGender: 'Stimme',
      dimMood: 'Stimmung',
      dimInstrument: 'Instrumente',
      dimScene: 'Szene',
      dimRegion: 'Region (UNESCO)',
      dimTopic: 'Thema',
      audioLength: 'Audio-Länge',
      generateButton: 'Musik generieren',
      selectModel: 'Modell',
      customTags: 'Eigene Tags',
      customTagsPlaceholder: 'z.B. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: 'Image Lab',
        textlab: 'Latent Text Lab',
        crossmodal: 'Crossmodal Lab'
      },
      imageLab: {
        headerTitle: 'Image Lab — Visuelle Vektorraumforschung',
        headerSubtitle: 'Fünf Werkzeuge zur Untersuchung, wie Diffusionsmodelle Bilder aus Text erzeugen: von der Entrauschung über Attention und Fusion bis zur Vektorarithmetik.',
        tabs: {
          archaeology: {
            label: 'Denoising Archaeology',
            short: 'Dem Modell beim Arbeiten zusehen'
          },
          attention: {
            label: 'Attention Cartography',
            short: 'Sehen, wohin das Modell schaut'
          },
          fusion: {
            label: 'Encoder Fusion',
            short: 'Surrealistische Verschmelzung'
          },
          probing: {
            label: 'Feature Probing',
            short: 'Dimensionsanalyse'
          },
          algebra: {
            label: 'Concept Algebra',
            short: 'Vektorarithmetik'
          }
        }
      },
      comingSoon: 'Dieses Tool wird in einer zukünftigen Version implementiert.',
      shared: {
        negativeHint: 'Begriffe, die das Modell aktiv vermeiden soll (z.B. "verschwommen, Text")',
        stepsHint: 'Mehr Schritte = höhere Qualität, aber längere Generierung',
        cfgHint: 'Classifier-Free Guidance: höher = stärker am Prompt orientiert, weniger Variation',
        seedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
        recordingActive: 'Aufzeichnung aktiv',
        recordingCount: '{count} Aufzeichnung | {count} Aufzeichnungen',
        recordingTooltip: 'Forschungsdaten werden automatisch gespeichert',
      },
      attention: {
        headerTitle: 'Attention Cartography — Welches Wort steuert welche Bildregion?',
        headerSubtitle: 'Für jedes Wort im Prompt zeigt eine Heatmap-Überlagerung auf dem generierten Bild, WO im Bild dieses Wort den größten Einfluss hatte. So wird sichtbar, wie das Modell semantische Konzepte räumlich verteilt.',
        explanationToggle: 'Ausführliche Erklärung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Wenn ein Diffusionsmodell ein Bild erzeugt, liest es den Prompt nicht Wort für Wort ab wie eine Bauanleitung. Stattdessen verteilt ein Mechanismus namens „Attention" den Einfluss jedes Wortes auf verschiedene Bildregionen. Das Wort „Haus" beeinflusst hauptsächlich die Region, in der das Haus entsteht — aber auch benachbarte Bereiche, weil das Modell den Kontext der gesamten Szene versteht. Dieses Tool macht diese Verteilung sichtbar: Klicke auf ein Wort und sieh, welche Bildregionen aufleuchten.',
        explainHowTitle: 'Wie lese ich die Heatmap?',
        explainHowText: 'Helle, intensive Farbe = starker Einfluss des Wortes auf diese Region. Dunkle oder fehlende Farbe = wenig Einfluss. Wenn du mehrere Wörter auswählst, erscheinen sie in verschiedenen Farben. Beachte: Die Karten sind NICHT perfekt scharf begrenzt — das ist kein Fehler, sondern zeigt, dass das Modell Konzepte kontextuell und nicht isoliert verarbeitet. Ein „Haus" in einer Bauernhof-Szene hat auch etwas Einfluss auf Tiere und Felder, weil das Modell die Szene als Ganzes versteht.',
        explainReadTitle: 'Was verraten die zwei Regler?',
        explainReadText: 'Der Entrauschungsschritt-Regler zeigt, WANN im 25-schrittigen Erzeugungsprozess du die Attention betrachtest. Frühe Schritte zeigen die grobe Layoutplanung, späte die Detailzuordnung. Der Netzwerktiefe-Regler zeigt, WO im Transformer die Attention gemessen wird: Flache Schichten (nahe am Eingang) zeigen globale Kompositionsplanung, mittlere die semantische Zuordnung, tiefe die Feinabstimmung. Beide Achsen sind unabhängig — es lohnt sich, systematisch verschiedene Kombinationen zu erkunden.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 verwendet einen MMDiT (Multimodal Diffusion Transformer) mit Joint Attention: Bild- und Text-Tokens bearbeiten sich gegenseitig in 24 Transformer-Blöcken. Wir ersetzen den Standard-SDPA-Prozessor durch einen manuellen Softmax(QK^T/√d)-Prozessor an 3 ausgewählten Blöcken, um die Text→Bild-Attention-Submatrix zu extrahieren. Die Maps haben 64x64 Auflösung (Patch-Grid) und werden per bilinearer Interpolation auf die Bildauflösung hochskaliert. SD3.5 nutzt zwei Text-Encoder: CLIP-L (BPE, 77 Tokens) und T5-XXL (SentencePiece, 512 Tokens). Beide können hier umgeschaltet werden, um zu sehen, wie unterschiedliche Tokenisierungen die Attention beeinflussen.',
        referencesTitle: 'Forschungsgrundlagen',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. Ein Haus steht in einer Landschaft, umgeben von landwirtschaftlichen Flächen, Natur und Tieren. Es sind einige Menschen zu sehen.',
        generate: 'Generieren + Analyse',
        generating: 'Bild wird generiert und Attention wird extrahiert...',
        emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um die Attention-Karten des Modells zu visualisieren.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Tokens',
        tokensHint: 'Klicke auf ein oder mehrere Wörter. Subwort-Tokens (z.B. "Ku"+"gel") werden automatisch zusammengefasst. Mehrere Wörter erscheinen in verschiedenen Farben.',
        timestepLabel: 'Entrauschungsschritt',
        timestepHint: 'Diffusionsmodelle erzeugen Bilder in 25 Schritten vom Rauschen zum Bild. Frühe Schritte legen die grobe Struktur fest, späte verfeinern Details. Dieser Regler zeigt, worauf das Modell bei welchem Schritt achtet.',
        step: 'Schritt',
        layerLabel: 'Netzwerktiefe',
        layerHint: 'Bei jedem Entrauschungsschritt durchläuft das Signal alle 24 Schichten des Transformers. Flache Schichten (nahe am Eingang) erfassen globale Komposition, mittlere die semantische Zuordnung, tiefe (nahe am Ausgang) feine Details. Beide Regler sind unabhängig: Schritt = wann im Prozess, Tiefe = wo im Netzwerk.',
        layerEarly: 'Flach (Komposition)',
        layerMid: 'Mittel (Semantik)',
        layerLate: 'Tief (Detail)',
        opacityLabel: 'Heatmap',
        opacityHint: 'Stärke der farbigen Überlagerung auf dem Bild.',
        baseImageLabel: 'Basisbild',
        baseColor: 'Farbe',
        baseBW: 'S/W',
        baseOff: 'Aus',
        baseImageHint: 'Farbe zeigt das Originalbild. S/W entsättigt es, damit Heatmap-Farben klar erkennbar sind. Aus blendet das Bild aus und zeigt nur die Attention-Karte.',
        encoderLabel: 'Text-Encoder',
        encoderClipL: 'CLIP-L (77 Tokens)',
        encoderT5: 'T5-XXL (512 Tokens)',
        encoderHint: 'SD3.5 nutzt zwei Text-Encoder mit unterschiedlicher Tokenisierung. CLIP-L verwendet BPE (Byte-Pair-Encoding), T5-XXL SentencePiece. Vergleiche, wie beide Encoder denselben Prompt verarbeiten und welche Bildregionen sie jeweils steuern.',
        download: 'Bild herunterladen'
      },
      probing: {
        headerTitle: 'Feature Probing — Welche Dimensionen kodieren was?',
        headerSubtitle: 'Vergleiche zwei Prompts und finde heraus, welche Embedding-Dimensionen den semantischen Unterschied kodieren. Übertrage gezielt einzelne Dimensionen, um zu sehen, wie sie das Bild verändern.',
        explanationToggle: 'Ausführliche Erklärung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Jedes Wort wird vom Text-Encoder in einen hochdimensionalen Vektor umgewandelt (z.B. 4096 Dimensionen bei T5). Wenn du ein Wort im Prompt änderst — z.B. „rotes" zu „blaues" — ändern sich bestimmte Dimensionen stärker als andere. Dieses Tool zeigt dir, WELCHE Dimensionen sich am meisten ändern und lässt dich gezielt einzelne Dimensionen von Prompt B in Prompt A übertragen.',
        explainHowTitle: 'Wie funktioniert die Übertragung?',
        explainHowText: 'Im Balkendiagramm siehst du alle Dimensionen sortiert nach Differenzgröße. Mit den Rang-Reglern (Von/Bis) wählst du einen Bereich aus — z.B. nur die Top-100 oder gezielt Rang 880–920. Beim Klick auf „Übertragen" wird das Bild mit denselben Einstellungen (gleicher Seed!) neu generiert — aber mit den ausgewählten Dimensionen aus Prompt B. So siehst du exakt, was diese Dimensionen „kodieren".',
        explainReadTitle: 'Wie lese ich das Balkendiagramm?',
        explainReadText: 'Jeder Balken repräsentiert eine Embedding-Dimension. Die Länge zeigt, wie stark sich diese Dimension zwischen Prompt A und B unterscheidet. Dimensionen mit großem Unterschied sind die wahrscheinlichsten Träger der semantischen Änderung. Aber: Embeddings sind verteilt — oft braucht es mehrere Dimensionen zusammen, um eine sichtbare Änderung zu bewirken.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 verwendet drei Text-Encoder: CLIP-L (768d), CLIP-G (1280d) und T5-XXL (4096d). Du kannst jeden einzeln proben. Die Differenz wird als mittlere absolute Abweichung über alle Token-Positionen berechnet: mean(abs(B-A), dim=tokens). Die Übertragung ersetzt die ausgewählten Dimensionen in allen Token-Positionen gleichzeitig.',
        referencesTitle: 'Forschungsgrundlagen',
        promptALabel: 'Prompt A (Original)',
        promptBLabel: 'Prompt B (Vergleich)',
        promptAPlaceholder: 'z.B. Ein rotes Haus am See',
        promptBPlaceholder: 'z.B. Ein blaues Haus am See',
        encoderLabel: 'Encoder',
        encoderAll: 'Alle (empfohlen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analysieren',
        analyzing: 'Prompts werden kodiert und verglichen...',
        transferBtn: 'Übertrage die ausgewählten Vektor-Dimensionen von Prompt B in das erzeugte Bild',
        transferring: 'Bild mit modifiziertem Embedding wird erzeugt...',
        rankFromLabel: 'Von Rang',
        rankToLabel: 'Bis Rang',
        sliderLabel: 'Dimensionen von Prompt B auswählen',
        range1Label: 'Bereich 1',
        range2Label: 'Bereich 2',
        addRange: 'Bereich hinzufügen',
        selectionDesc: '{count} Dimensionen von Prompt B ausgewählt (Rang {ranges} von {total})',
        listTitle: 'Die {count} Dimensionen von Prompt B mit der größten Differenz zu Prompt A',
        sortAsc: 'Aufsteigend sortiert',
        sortDesc: 'Absteigend sortiert',
        originalLabel: 'Original (Prompt A)',
        modifiedLabel: 'Modifiziert (Transfer von Prompt B)',
        modifiedHint: 'Wähle unten einen Rangbereich und klicke „Übertragen" — hier erscheint dann Prompt A mit den übertragenen Dimensionen aus B (gleicher Seed).',
        noDifference: 'Die Embeddings sind identisch — ändere Prompt B.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'Alle',
        selectNone: 'Keine',
        encoderHint: 'All = alle Encoder gleichzeitig. CLIP-L/CLIP-G/T5 = isoliert einen einzelnen Encoder für die Analyse.',
        sliderHint: 'Wähle einen Rangbereich der wichtigsten Embedding-Dimensionen (sortiert nach Unterschied zwischen A und B).',
        transferHint: 'Überträgt die ausgewählten Dimensionen von Prompt B auf Prompt A und generiert ein neues Bild.',
        downloadOriginal: 'Original herunterladen',
        downloadModified: 'Modifiziert herunterladen'
      },
      algebra: {
        headerTitle: 'Concept Algebra \u2014 Vektor-Arithmetik auf Bild-Embeddings',
        headerSubtitle: 'Wende die ber\u00fchmte Word2Vec-Analogie auf Bildgenerierung an: K\u00f6nig \u2212 Mann + Frau \u2248 K\u00f6nigin. Drei Prompts werden kodiert und algebraisch kombiniert.',
        explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Mikolov zeigte 2013, dass Word-Embeddings semantische Beziehungen als lineare Richtungen kodieren: Der Vektor von \u201eK\u00f6nig\u201c minus \u201eMann\u201c plus \u201eFrau\u201c ergibt einen Vektor nahe \u201eK\u00f6nigin\u201c. Dieses Tool \u00fcbertr\u00e4gt diese Idee auf die Text-Encoder von SD3.5: Statt einzelner W\u00f6rter manipulierst du ganze Prompt-Embeddings. Das Ergebnis ist ein Bild, das das Konzept A enth\u00e4lt, aber B durch C ersetzt hat.',
        explainHowTitle: 'Wie funktioniert die Algebra \u2014 und warum nicht einfach ein Negativ-Prompt?',
        explainHowText: 'Du gibst drei Prompts ein: A (Basis), B (subtrahieren) und C (addieren). Die Formel ist: Ergebnis = A \u2212 Skalierung\u2081\u00d7B + Skalierung\u2082\u00d7C. Mit den Skalierungsreglern steuerst du die Intensit\u00e4t: Bei 1.0 wird B vollst\u00e4ndig subtrahiert und C vollst\u00e4ndig addiert. Bei 0.5 nur zur H\u00e4lfte. Werte \u00fcber 1.0 verst\u00e4rken den Effekt. \u2014 Warum nicht einfach \u201eA + C\u201c als Prompt und \u201eB\u201c als Negativ-Prompt verwenden? Weil das etwas fundamental anderes tut: Ein Negativ-Prompt steuert den Entrauschungsprozess bei JEDEM der 25 Schritte weg von B \u2014 das Modell entscheidet Schritt f\u00fcr Schritt, wie es \u201enicht B\u201c interpretiert. Concept Algebra dagegen berechnet einen neuen Vektor VOR der Bildgenerierung: Die Subtraktion passiert im Embedding-Raum, nicht im Diffusionsprozess. Das Ergebnis ist ein einziger Vektor, der \u201eA ohne B-heit plus C-heit\u201c direkt kodiert. Der Negativ-Prompt sagt \u201emach das nicht\u201c. Die Algebra sagt \u201enimm dieses Konzept heraus und setze jenes ein\u201c \u2014 eine chirurgische Operation im Bedeutungsraum statt einer schrittweisen Vermeidungsstrategie.',
        explainReadTitle: 'Was bedeuten die Ergebnisse?',
        explainReadText: 'Links siehst du das Referenzbild (nur Prompt A, gleicher Seed). Rechts das Ergebnis der Algebra. Wenn die Analogie funktioniert, sollte das rechte Bild das Konzept von A zeigen, aber mit der semantischen Ver\u00e4nderung B\u2192C. Beispiel: \u201eSonnenuntergang am Strand\u201c \u2212 \u201eStrand\u201c + \u201eBerge\u201c \u2248 \u201eSonnenuntergang \u00fcber Bergen\u201c. Die L2-Distanz zeigt, wie weit sich das Ergebnis vom Original entfernt hat. \u2014 Ist die Operation kommutativ? Nein. Die Subtraktion von B und die Addition von C finden relativ zum Vektor A statt. Die Richtung B\u2192C ist nur im Kontext von A sinnvoll: \u201eK\u00f6nig \u2212 Mann\u201c entfernt die \u201em\u00e4nnlichen\u201c Richtungen aus dem K\u00f6nig-Vektor, \u201e+ Frau\u201c erg\u00e4nzt die \u201eweiblichen\u201c Richtungen \u2014 das Ergebnis liegt nahe \u201eK\u00f6nigin\u201c. C wird dabei nicht gezielt an die Stelle von B gesetzt, sondern einfach addiert. Dass das trotzdem funktioniert, zeigt, dass semantische Beziehungen im Vektorraum als konsistente lineare Richtungen kodiert sind.',
        techTitle: 'Technische Details',
        techText: 'Die Algebra wird auf den gew\u00e4hlten Encoder-Embeddings durchgef\u00fchrt: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d) oder alle zusammen (589 Tokens \u00d7 4096d). Dieselbe Operation wird auch auf die Pooled Embeddings (2048d) angewendet. Beide Bilder verwenden denselben Seed f\u00fcr faire Vergleichbarkeit.',
        referencesTitle: 'Forschungsgrundlagen',
        promptALabel: 'Prompt A (Basis)',
        promptAPlaceholder: 'z.B. Sonnenuntergang am Strand mit Palmen',
        promptBLabel: 'Prompt B (Subtrahieren)',
        promptBPlaceholder: 'z.B. Strand mit Palmen',
        promptCLabel: 'Prompt C (Addieren)',
        promptCPlaceholder: 'z.B. Schneebedeckte Berge',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Encoder',
        encoderAll: 'Alle (empfohlen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Berechnen',
        generating: 'Embeddings werden berechnet und Bilder erzeugt...',
        referenceLabel: 'Referenz (Prompt A)',
        resultLabel: 'Ergebnis (A \u2212 B + C)',
        l2Label: 'L2-Distanz zum Original',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Subtraktions-Skalierung',
        scaleAddLabel: 'Additions-Skalierung',
        encoderHint: 'All = alle Encoder gleichzeitig. CLIP-L/CLIP-G/T5 = isoliert einen einzelnen Encoder für die Arithmetik.',
        scaleSubHint: 'Gewicht der Subtraktion (B). Höher = stärkere Entfernung von Konzept B.',
        scaleAddHint: 'Gewicht der Addition (C). Höher = stärkere Hinzufügung von Konzept C.',
        l2Hint: 'Euklidischer Abstand im Embedding-Raum. Kleiner = ähnlicher, größer = unterschiedlicher.',
        downloadReference: 'Referenz herunterladen',
        downloadResult: 'Ergebnis herunterladen',
        resultHint: 'Gib drei Prompts ein und klicke auf Berechnen \u2014 hier erscheint das Ergebnis der Vektor-Arithmetik.'
      },
      archaeology: {
        headerTitle: 'Denoising Archaeology \u2014 Wie wird aus Rauschen ein Bild?',
        headerSubtitle: 'Beobachte jeden einzelnen Entrauschungsschritt. Diffusionsmodelle arbeiten nicht links-nach-rechts, sondern gleichzeitig \u00fcberall \u2014 von groben Formen zu feinen Details.',
        explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Ein Diffusionsmodell erzeugt ein Bild, indem es schrittweise Rauschen entfernt. Dabei entsteht das Bild nicht wie beim Zeichnen von links nach rechts \u2014 stattdessen arbeitet das Modell an ALLEN Bildregionen gleichzeitig. In den ersten Schritten entstehen grobe Strukturen: Wo ist oben, wo unten? Wo ist der Horizont? In den mittleren Schritten kommen semantische Inhalte: Objekte, Formen, Farben. Die letzten Schritte verfeinern Texturen und Details. Dieses Tool macht jeden einzelnen Schritt sichtbar.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Gib einen Prompt ein und klicke auf Generieren. Das Modell erzeugt 25 Zwischenbilder (eins pro Entrauschungsschritt). Diese erscheinen als Filmstreifen unten. Klicke auf ein Thumbnail oder benutze den Zeitregler, um jeden Schritt in voller Gr\u00f6\u00dfe zu betrachten. Vergleiche fr\u00fche und sp\u00e4te Schritte: Wann \u201ewei\u00df\u201c das Modell, was es zeichnet?',
        explainReadTitle: 'Was verraten die drei Phasen?',
        explainReadText: 'Fr\u00fche Schritte (1\u20138): Globale Komposition \u2014 Grundstruktur, Farbverteilung, Layoutplanung. Mittlere Schritte (9\u201317): Semantische Emergenz \u2014 Objekte werden erkennbar, Formen kristallisieren sich heraus. Sp\u00e4te Schritte (18\u201325): Detail-Verfeinerung \u2014 Texturen, Kanten, feine Muster. Die \u00dcberg\u00e4nge sind flie\u00dfend, aber die Phasen zeigen deutlich: Das Modell \u201eplant\u201c zuerst global und verfeinert dann lokal. Besonders aufschlussreich: Der allererste Schritt zeigt keine feink\u00f6rnigen Pixel, sondern farbige Flecken. Das liegt daran, dass das Rauschen im Latent-Raum (128\u00d7128 bei 16 Kan\u00e4len) erzeugt wird, nicht im Pixel-Raum. Der VAE \u00fcbersetzt jeden Latent-Pixel in einen ~8\u00d78-Pixel-Patch \u2014 selbst pures Gau\u00dfsches Rauschen wird dadurch zu zusammenh\u00e4ngenden Farbclustern. Das Modell \u201edenkt\u201c nie in einzelnen Pixeln, sondern immer in diesem komprimierten Raum.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 Large verwendet Rectified Flow als Scheduler mit 25 Standardschritten. Bei jedem Schritt werden die aktuellen Latent-Vektoren durch den VAE dekodiert (1024\u00d71024 JPEG). Der VAE (Variational Autoencoder) \u00fcbersetzt den mathematischen Latent-Raum in Pixel. Die Latent-Darstellung ist 128\u00d7128 bei 16 Kan\u00e4len \u2014 jeder Latent-Pixel entspricht einem ~8\u00d78-Pixel-Patch im Bild. Deshalb zeigt schon der erste Schritt farbige Cluster statt feines Pixelrauschen: Der VAE interpretiert zuf\u00e4llige 16-dimensionale Vektoren als koh\u00e4rente Farbfl\u00e4chen.',
        referencesTitle: 'Forschungsgrundlagen',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. Ein Marktplatz in einer mittelalterlichen Stadt mit Menschen, Geb\u00e4uden und einem Brunnen',
        generate: 'Generieren',
        generating: 'Bild wird generiert \u2014 jeder Schritt wird aufgezeichnet...',
        emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um den Entrauschungsprozess zu visualisieren.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Entrauschungs-Filmstreifen',
        timelineLabel: 'Schritt',
        phaseEarly: 'Komposition',
        phaseMid: 'Semantik',
        phaseLate: 'Detail',
        phaseEarlyDesc: 'Globale Struktur und Farbverteilung entstehen',
        phaseMidDesc: 'Objekte und Formen werden erkennbar',
        phaseLateDesc: 'Texturen und feine Details werden gesch\u00e4rft',
        finalImageLabel: 'Finales Bild (volle Aufl\u00f6sung)',
        timelineHint: 'Scrubbt durch die Entrauschungsschritte — zeigt wie das Bild vom Rauschen zur fertigen Komposition entsteht.',
        download: 'Bild herunterladen'
      },
      textLab: {
        headerTitle: 'Latent Text Lab \u2014 Wissenschaftliche LLM-Dekonstruktion',
        headerSubtitle: 'Representation Engineering, vergleichende Modell-Arch\u00e4ologie und systematische Bias-Analyse: Drei forschungsbasierte Werkzeuge zur Untersuchung von Sprachmodellen.',
        explanationToggle: 'Erkl\u00e4rung anzeigen',
        modelPanel: {
          presetLabel: 'Preset',
          presetNone: 'Kein Preset (eigene ID)',
          customModelLabel: 'HuggingFace Modell-ID',
          customModelPlaceholder: 'z.B. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Quantisierung',
          quantAuto: 'Auto',
          quantizationHint: 'bf16 = volle Qualität, int8 = halb so viel VRAM, int4 = minimal VRAM aber niedrigste Qualität',
        },
        temperatureHint: 'Zufälligkeit der Textgenerierung. Niedrig = deterministisch, hoch = kreativer.',
        maxTokensHint: 'Maximale Anzahl generierter Tokens (Wortteile).',
        textSeedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
        tabs: {
          repeng: { label: 'Representation Engineering', short: 'Steuervektoren im LLM finden' },
          compare: { label: 'Modell-Vergleich', short: 'Zwei LLMs Schicht f\u00fcr Schicht vergleichen' },
          bias: { label: 'Bias-Arch\u00e4ologie', short: 'Verborgene Vorurteile im LLM aufdecken' },
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: 'Finde Konzept-Richtungen im Aktivierungsraum und steuere die Generierung',
          explainWhatTitle: 'Was zeigt dieses Experiment?',
          explainWhatText: 'Basiert auf Zou et al. (2023) \u201cRepresentation Engineering\u201d und Li et al. (2024) \u201cInference-Time Intervention\u201d. LLMs kodieren abstrakte Konzepte (Wahrheit, Stimmung, Ethik) als Richtungen im hochdimensionalen Aktivierungsraum. Durch Kontrastpaare (wahrer vs. falscher Satz) l\u00e4sst sich die Richtung extrahieren, die ein Konzept kodiert. Addiert man diese Richtung zur Laufzeit, \u00e4ndert sich das Modellverhalten gezielt \u2014 ohne Retraining. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: 'Wie benutze ich es?',
          explainHowText: 'Dieses Experiment extrahiert eine \u201cWahrheits-Richtung\u201d aus dem Modell. Die vorausgef\u00fcllten Kontrastpaare enthalten jeweils einen wahren und einen falschen Satz. Aus den Unterschieden in den Aktivierungen berechnet das System eine Richtung im hochdimensionalen Raum. Wenn diese Richtung invertiert wird (\u03b1 = -1), sollte das Modell bei einem Fakten-Prompt eine falsche Antwort generieren \u2014 obwohl es die richtige \u201cwei\u00df\u201d. Empfehlung: Englische Prompts funktionieren deutlich besser, da die meisten Open-Source-LLMs prim\u00e4r auf englischen Daten trainiert wurden.',
          referencesTitle: 'Forschungsgrundlagen',
          expectedResults: 'Erwartete Ergebnisse: Bei \u03b1 = 0 (Baseline) generiert das Modell die korrekte Antwort. Bei \u03b1 = -1 (Invertierung) sollte eine falsche Antwort erscheinen \u2014 das ist der Kern des Experiments. Bei \u03b1 = +1 \u00e4ndert sich wenig, weil das Modell bereits korrekt antwortet. Ab |\u03b1| > 2 dominieren Artefakte (Wiederholungen, Unsinn). Der \"Sweet Spot\" liegt laut Zou et al. bei |\u03b1| zwischen 0.5 und 2.0. Erkl\u00e4rte Varianz > 50% bedeutet eine saubere Trennung \u2014 darunter sind die Kontrastpaare zu \u00e4hnlich oder zu wenige (mindestens 3 empfohlen).',
          pairsTitle: 'Kontrastpaare',
          pairsSubtitle: 'Mindestens 3 Paare empfohlen. Jedes Paar muss sich nur im Zielkonzept unterscheiden (wahr vs. falsch). Die Beispiele sind editierbar.',
          positiveLabel: 'Positiv (wahr)',
          negativeLabel: 'Negativ (falsch)',
          positivePlaceholder: 'z.B. The capital of France is Paris',
          negativePlaceholder: 'z.B. The capital of France is Berlin',
          addPair: 'Paar hinzuf\u00fcgen',
          removePair: 'Entfernen',
          targetLayerLabel: 'Ziel-Schicht',
          targetLayerHint: 'Welche Transformer-Schicht den Steuerungsvektor erhält. Verschiedene Schichten beeinflussen verschiedene Aspekte der Textgenerierung.',
          targetLayerAuto: 'Letzte Schicht',
          findDirection: 'Richtung finden',
          finding: 'Konzept-Richtung wird berechnet...',
          directionFound: 'Konzept-Richtung gefunden',
          varianceLabel: 'Erkl\u00e4rte Varianz',
          dimLabel: 'Dimensionen',
          projectionsTitle: 'Projektionen der Kontrastpaare',
          testTitle: 'Test + Manipulation',
          testSubtitle: 'Gib einen Satz ein und steuere die Generierung entlang der Konzept-Richtung',
          testPromptLabel: 'Test-Prompt',
          testPromptPlaceholder: 'z.B. The capital of Germany is',
          alphaLabel: 'Manipulationsst\u00e4rke (\u03b1)',
          alphaHint: 'Stärke des Steuerungsvektors. 0 = kein Effekt, höher = stärkerer Einfluss der Kontrastpaare.',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          seedLabel: 'Seed (-1 = zuf\u00e4llig)',
          generateBtn: 'Generieren mit Manipulation',
          generating: 'Manipulierte Generierung l\u00e4uft...',
          baselineLabel: 'Baseline (ohne Manipulation)',
          manipulatedLabel: 'Manipuliert (\u03b1 = {alpha})',
          projectionLabel: 'Projektion auf Konzept-Richtung',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        compare: {
          title: 'Vergleichende Modell-Arch\u00e4ologie',
          subtitle: 'Lade zwei Modelle und vergleiche ihre inneren Repr\u00e4sentationen systematisch',
          explainWhatTitle: 'Was zeigt dieses Experiment?',
          explainWhatText: 'Basiert auf Belinkov (2022) \u201cProbing Classifiers\u201d und Olsson et al. (2022) \u201cIn-Context Learning and Induction Heads\u201d. Die Heatmap zeigt CKA (Centered Kernel Alignment) zwischen Schichten beider Modelle. Hohe \u00c4hnlichkeit bedeutet: Diese Schichten repr\u00e4sentieren Information auf \u00e4hnliche Weise. Fr\u00fche Schichten (Syntax) sind oft \u00e4hnlich \u2014 sp\u00e4te Schichten (Semantik) divergieren st\u00e4rker. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: 'Wie benutze ich es?',
          explainHowText: 'Modell A ist das aktive Preset. W\u00e4hle ein zweites Modell (Modell B) \u2014 per Preset oder eigener HuggingFace-ID \u2014 und lade es. Gib einen Text ein und klicke \u201cVergleichen\u201d. Die CKA-Heatmap zeigt, welche Schichten \u00e4hnlich repr\u00e4sentieren. Die Generierungen beider Modelle mit identischem Seed zeigen, wie unterschiedlich sie denselben Prompt vervollst\u00e4ndigen.',
          referencesTitle: 'Forschungsgrundlagen',
          modelATitle: 'Modell A (aus Preset-Auswahl)',
          modelAHint: 'Wechsel \u00fcber das Preset-Dropdown oben',
          modelBTitle: 'Modell B (zweites Modell)',
          modelBPresetLabel: 'Preset',
          modelBCustomLabel: 'HuggingFace Modell-ID',
          modelBCustomPlaceholder: 'z.B. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Modell B laden',
          modelBLoaded: 'Modell B geladen',
          modelBNone: 'Modell B nicht geladen',
          promptLabel: 'Prompt',
          promptPlaceholder: 'z.B. Die Katze sa\u00df auf der Matte und beobachtete die V\u00f6gel',
          seedLabel: 'Seed',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          compareBtn: 'Vergleichen',
          comparing: 'Modelle werden verglichen...',
          heatmapTitle: 'Schicht-Alignment (CKA)',
          heatmapAxisA: 'Modell A \u2014 Schichten',
          heatmapAxisB: 'Modell B \u2014 Schichten',
          heatmapExplain: 'Helle Zellen = hohe Repr\u00e4sentations\u00e4hnlichkeit. Diagonale Muster zeigen, dass die Modelle Information in \u00e4hnlicher Reihenfolge verarbeiten.',
          attentionTitle: 'Attention-Vergleich (letzte Schicht)',
          modelALabel: 'Modell A',
          modelBLabel: 'Modell B',
          generationTitle: 'Generierungs-Vergleich (gleicher Seed)',
          layerStatsTitle: 'Schicht-Statistiken',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        bias: {
          title: 'Bias-Arch\u00e4ologie',
          subtitle: 'Systematische Bias-Experimente durch kontrollierte Token-Manipulation',
          explainWhatTitle: 'Was zeigt dieses Experiment?',
          explainWhatText: 'Basiert auf Zou et al. (2023) \u201cRepresentation Engineering\u201d und Bricken et al. (2023) \u201cTowards Monosemanticity\u201d. Statt freiem Manipulieren untersucht dieses Tool systematische Verzerrungen: Was passiert, wenn alle m\u00e4nnlichen Pronomen unterdr\u00fcckt werden? Welches Geschlecht w\u00e4hlt das Modell als Default? \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: 'Wie benutze ich es?',
          explainHowText: 'W\u00e4hle einen Experiment-Typ (Geschlecht, Sentiment, Dom\u00e4ne oder eigene Tokens). Gib einen Prompt ein, der das Modell zur Fortsetzung einl\u00e4dt (z.B. \u201cThe doctor said to the patient\u201d). Die Ergebnisse zeigen Baseline vs. manipulierte Generierungen \u2014 die Unterschiede offenbaren die im Modell kodierten Biases.',
          referencesTitle: 'Forschungsgrundlagen',
          presetLabel: 'Experiment-Typ',
          presetGender: 'Geschlecht \u2014 Unterdr\u00fccke gendered Pronomen',
          presetSentiment: 'Sentiment \u2014 Verst\u00e4rke positiv/negativ',
          presetDomain: 'Dom\u00e4ne \u2014 Verst\u00e4rke wissenschaftlich/poetisch',
          presetCustom: 'Eigenes Experiment',
          promptLabel: 'Prompt',
          promptPlaceholder: 'z.B. The doctor said to the patient',
          customBoostLabel: 'Verst\u00e4rkte Tokens (kommagetrennt)',
          customBoostPlaceholder: 'z.B. dark,shadow,night',
          customSuppressLabel: 'Unterdr\u00fcckte Tokens (kommagetrennt)',
          customSuppressPlaceholder: 'z.B. light,sun,bright',
          numSamplesLabel: 'Stichproben pro Bedingung',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          seedLabel: 'Basis-Seed',
          runBtn: 'Experiment starten',
          running: 'Bias-Experiment l\u00e4uft...',
          baselineTitle: 'Baseline (keine Manipulation)',
          groupTitle: 'Gruppe: {name}',
          modeSuppress: 'unterdr\u00fcckt',
          modeBoost: 'verst\u00e4rkt',
          tokensLabel: 'Tokens',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Unterdr\u00fcckt alle geschlechtsspezifischen Pronomen und beobachtet, welche Defaults das Modell w\u00e4hlt.',
          sentimentDesc: 'Verst\u00e4rkt positive oder negative W\u00f6rter und misst, wie stark der gesamte Textfluss beeinflusst wird.',
          domainDesc: 'Verst\u00e4rkt wissenschaftliches oder poetisches Vokabular und beobachtet Register-Verschiebungen.',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        error: {
          gpuUnreachable: 'GPU-Service nicht erreichbar. Ist er gestartet?',
          loadFailed: 'Modell konnte nicht geladen werden.',
          operationFailed: 'Operation fehlgeschlagen.'
        }
      },
      crossmodal: {
        headerTitle: 'Crossmodal Lab',
        headerSubtitle: 'Klang aus latenten Raeumen: T5-Embedding-Manipulation, bildgesteuerte Audiogenerierung, crossmodaler Transfer',
        explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
        generate: 'Generieren',
        generating: 'Generiere...',
        result: 'Ergebnis',
        seed: 'Seed',
        generationTime: 'Generierungszeit',
        tabs: {
          synth: {
            label: 'Latent Audio Synth',
            short: 'T5-Embedding-Manipulation',
            title: 'Latent Audio Synth',
            description: 'Direkte Manipulation des T5-Conditioning-Raums (768d) fuer Stable Audio. Interpoliere zwischen Prompts, extrapoliere ueber den Prompt hinaus, skaliere Embeddings und injiziere Rauschen. Ultra-kurze Loops, nahezu Echtzeit.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Bild/Text zu Audio (CVPR 2025)',
            title: 'MMAudio — Video/Image to Audio',
            description: 'Bild und Text fliessen als getrennte Signale in dasselbe Netzwerk ein — das Bild wird nicht in Sprache uebersetzt, sondern beide steuern die Klangerzeugung gleichzeitig. Das Modell wurde gemeinsam auf Video und Audio trainiert und lernt dadurch direkte Zusammenhaenge zwischen Sichtbarem und Hoerbarem. Bis 8s, 44.1kHz, ~1.2s Rechenzeit. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'ImageBind Guidance',
            short: 'Gradientenbasierte Bildsteuerung',
            title: 'ImageBind Gradient Guidance',
            description: 'Gradient-basierte Steuerung waehrend des Stable Audio Denoising-Prozesses. ImageBind liefert einen gemeinsamen 1024d-Raum fuer Bild und Audio — der Gradient der Cosine-Similarity lenkt die Audiogenerierung in Richtung des Bild-Embeddings.'
          }
        },
        synth: {
          explainWhatTitle: 'Was macht der Latent Audio Synth?',
          explainWhatText: 'Stable Audio erzeugt Klang aus Text. Der Text wird dabei von einem T5-Encoder in einen Zahlenvektor mit 768 Dimensionen umgewandelt \u2014 diesen Vektor kannst du hier direkt manipulieren. Statt nur \u201ewas\u201c das Modell erzeugt zu \u00e4ndern (durch den Prompt), \u00e4nderst du \u201ewie\u201c das Modell den Text intern versteht. Zwei Prompts, die \u00e4hnlich klingen, k\u00f6nnen in diesem Raum weit auseinander liegen \u2014 und umgekehrt.',
          explainHowTitle: 'Wie benutze ich das Tool?',
          explainHowText: 'Gib in Prompt A einen Text ein \u2014 das bestimmt den Grundklang. Optional: Prompt B als Zielpunkt. Der Alpha-Regler steuert die Mischung: Bei 0 h\u00f6rst du nur A, bei 1 nur B, bei 0.5 eine Mischung. Werte \u00fcber 1 extrapolieren \u00fcber B hinaus (der Klang wird extremer), Werte unter 0 gehen in die entgegengesetzte Richtung. Magnitude skaliert das gesamte Embedding \u2014 h\u00f6here Werte erzeugen intensivere Kl\u00e4nge. Noise injiziert Zufall und erzeugt unvorhersehbare Variationen. Die Spectral Strip (unter dem Generate-Button) zeigt alle 768 Dimensionen als Balken. Du kannst einzelne Dimensionen per Mausklick verschieben und so gezielt den Klang manipulieren. Rechtsklick setzt eine Dimension zur\u00fcck.',
          promptA: 'Prompt A (Basis)',
          promptAPlaceholder: 'z.B. Meereswellen',
          promptB: 'Prompt B (Optional, fuer Interpolation)',
          promptBPlaceholder: 'z.B. Klaviermelodie',
          alpha: 'Alpha (Interpolation)',
          alphaHint: '0 = nur A, 1 = nur B, dazwischen = Mischung, >1 oder <0 = Extrapolation',
          magnitude: 'Magnitude (Skalierung)',
          magnitudeHint: 'Globale Skalierung des Embeddings (1.0 = unveraendert)',
          noise: 'Rauschen',
          noiseHint: 'Gauss-Rauschen auf dem Embedding (0 = kein Rauschen)',
          duration: 'Dauer (s)',
          steps: 'Schritte',
          cfg: 'CFG',
          durationHint: 'Länge des generierten Audio-Clips in Sekunden',
          stepsHint: 'Entrauschungsschritte. Mehr = höhere Qualität.',
          cfgHint: 'Classifier-Free Guidance für die Audiogenerierung',
          seedHint: '-1 = zufällig, fester Wert = reproduzierbares Ergebnis',
          loop: 'Loop-Wiedergabe',
          loopOn: 'Loop An',
          loopOff: 'Loop Aus',
          stop: 'Stop',
          looping: 'Loopt',
          playing: 'Spielt',
          stopped: 'Gestoppt',
          transpose: 'Transposition (Halbtoene)',
          midiSection: 'MIDI-Steuerung',
          midiUnsupported: 'Web MIDI wird von diesem Browser nicht unterstuetzt.',
          midiInput: 'MIDI-Eingang',
          midiNone: '(keiner)',
          midiMappings: 'CC-Zuordnungen',
          midiNoteC3: 'Note (C3 = Ref)',
          midiGenerate: 'Generieren + Transposition',
          midiPitch: 'Tonhoehe rel. C3',
          loopInterval: 'Loop-Intervall',
          loopOptimize: 'Auto-Optimierung',
          loopPingPong: 'Ping-Pong',
          loopIntervalHint: 'Start/Ende des Loop-Bereichs — verkuerze das Ende, um Stable Audios Fade-Out abzuschneiden',
          modeLoop: 'Loop',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Wavetable',
          modeRate: 'Tempo (schnell)',
          modePitch: 'Tonhoehe (OLA)',
          wavetableScan: 'Scan-Position',
          wavetableScanHint: 'Morpht zwischen Frames (0 = Anfang, 1 = Ende)',
          wavetableFrames: '{count} Frames',
          midiScan: 'Scan-Position',
          adsrTitle: 'ADSR-Huellkurve',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Huellkurve fuer MIDI-Noten (Attack/Decay/Sustain/Release)',
          play: 'Abspielen',
          normalize: 'Lautheit normalisieren',
          peak: 'Peak',
          crossfade: 'Crossfade',
          transposeHint: 'Verschiebt die Tonhöhe in Halbtönen',
          crossfadeHint: 'Überblendzeit an der Loop-Grenze (ms)',
          normalizeHint: 'Normalisiert die Lautstärke auf maximale Amplitude',
          saveRaw: 'Raw speichern',
          saveLoop: 'Loop speichern',
          embeddingStats: 'Embedding-Statistiken',
          dimensions: {
            section: 'Dimensions-Explorer',
            hint: 'Auf Balken ziehen = Offset setzen. Horizontal malen = mehrere Dimensionen.',
            resetAll: 'Alle zuruecksetzen',
            hoverActivation: 'Aktivierung',
            hoverOffset: 'Offset',
            rightClickReset: 'Rechtsklick = zuruecksetzen',
            sortDiff: 'Sortiert nach Prompt-Differenz',
            sortMagnitude: 'Sortiert nach Aktivierung',
            activeOffsets: '{count} Offsets aktiv',
            applyAndGenerate: 'Anwenden und neu generieren',
            undo: 'Rueckgaengig',
            redo: 'Wiederholen'
          }
        },
        mmaudio: {
          explainWhatTitle: 'Was macht MMAudio?',
          explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) wurde gemeinsam auf Video und Audio trainiert. Es \u00fcbersetzt ein Bild nicht erst in Text und dann in Klang, sondern verarbeitet Bild und Text als parallele Signale im selben Netzwerk. Das Modell hat gelernt, welche Kl\u00e4nge zu welchen visuellen Szenen geh\u00f6ren \u2014 ein Wald erzeugt Vogelgezwitscher, eine Stra\u00dfe Verkehrsl\u00e4rm, eine Gitarre Zupfkl\u00e4nge.',
          explainHowTitle: 'Wie benutze ich das Tool?',
          explainHowText: 'Lade ein Bild hoch und/oder gib einen Text-Prompt ein \u2014 beides zusammen ergibt die reichsten Ergebnisse. Das Bild allein erzeugt Kl\u00e4nge passend zum visuellen Inhalt. Der Text-Prompt kann den Klang zus\u00e4tzlich lenken oder ohne Bild allein verwendet werden. Im Negativ-Prompt beschreibst du Kl\u00e4nge, die du NICHT h\u00f6ren willst (z.B. \u201eSprache, Musik\u201c). Duration bestimmt die L\u00e4nge (1\u20138 Sekunden). CFG Strength steuert, wie streng das Modell dem Prompt folgt \u2014 niedrige Werte (2\u20133) erzeugen vielf\u00e4ltigere, h\u00f6here Werte (6\u20138) prompt-treuere Ergebnisse.',
          imageUpload: 'Bild hochladen (optional)',
          prompt: 'Text-Prompt (optional)',
          promptPlaceholder: 'z.B. Knisterndes Lagerfeuer',
          negativePrompt: 'Negativ-Prompt',
          duration: 'Dauer (s)',
          maxDuration: 'Max 8s (Modell-Limit)',
          cfg: 'CFG',
          steps: 'Schritte',
          compareHint: 'Vergleiche: Nur Text vs. Bild + Text'
        },
        guidance: {
          explainWhatTitle: 'Was macht ImageBind Guidance?',
          explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) bringt sechs Sinne \u2014 Bild, Klang, Text, Tiefe, W\u00e4rme, Bewegung \u2014 in eine gemeinsame \u201eSprache\u201c. Dieses Tool nutzt diese Gemeinsamkeit: W\u00e4hrend der Klang Schritt f\u00fcr Schritt entsteht, vergleicht es st\u00e4ndig \u201eKlingt das schon nach dem Bild?\u201c und korrigiert die Richtung. Im Ergebnis zeigt die Cosine-Similarity, wie nah der erzeugte Klang dem Bild-Inhalt kam.',
          explainHowTitle: 'Wie benutze ich das Tool?',
          explainHowText: 'Lade ein Bild hoch \u2014 das ist die Zielrichtung f\u00fcr den Klang. Optional: Ein Text-Prompt als zus\u00e4tzliche Lenkung. Der Regler \u201e\u03bb Guidance Strength\u201c ist der wichtigste Parameter: Niedrige Werte (0.01\u20130.05) lassen dem Klang viel Freiheit, hohe Werte (0.3\u20131.0) binden ihn eng an das Bild. \u201eWarmup Steps\u201c bestimmt, ab welchem Schritt die Bildlenkung einsetzt \u2014 niedrige Werte starten sofort, h\u00f6here lassen die Grundstruktur erst ungesteuert entstehen. Total Steps und Duration steuern Qualit\u00e4t und L\u00e4nge.',
          referencesTitle: 'Forschungsgrundlagen',
          imageUpload: 'Bild hochladen',
          prompt: 'Basis-Prompt (optional)',
          promptPlaceholder: 'z.B. Umgebungsklanglandschaft',
          lambda: 'Guidance-Staerke',
          lambdaHint: 'Wie stark das Bild die Audiogenerierung steuert',
          warmupSteps: 'Warmup-Schritte',
          warmupHint: 'Gradient-Guidance nur in den ersten N Schritten',
          totalSteps: 'Gesamt-Schritte',
          duration: 'Dauer (s)',
          cfg: 'CFG',
          totalStepsHint: 'Gesamte Entrauschungsschritte. Mehr = höhere Qualität.',
          durationHint: 'Länge des generierten Audio-Clips in Sekunden',
          cosineSimilarity: 'Cosine-Similarity (Bild-Audio-Naehe)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Wusstest du?',
        learnMore: '📚 Mehr erfahren',
        currentlyHappening: '⚡ Gerade passiert:',
        energyUsed: 'Verbrauchte Energie',
        co2Produced: 'CO₂ produziert'
      },
      energy: {
        kids_1: '💡 KI-Bilder brauchen Strom – so viel wie dein Handy 3 Stunden laden!',
        kids_2: '🔌 Die GPU ist wie ein Super-Taschenrechner der sehr viel Strom frisst!',
        kids_3: '⚡ Jedes Bild braucht so viel Energie wie eine LED-Lampe 10 Minuten an!',
        youth_1: '⚡ Eine GPU braucht beim Generieren {watts}W – wie ein kleiner Heizlüfter!',
        youth_2: '🔋 Ein Bild verbraucht etwa 0.01-0.02 kWh – klingt wenig, summiert sich aber!',
        youth_3: '🌡️ Die GPU wird gerade {temp}°C heiß – deshalb braucht sie Kühlung!',
        expert_1: '📊 Echtzeit: {watts}W bei {util}% Auslastung = {kwh} kWh bisher',
        expert_2: '🔥 TDP-Limit: {tdp}W | Aktuell: {watts}W ({percent}% des Limits)',
        expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – Modell + Aktivierungen'
      },
      data: {
        kids_1: '🧮 Die GPU rechnet gerade 10 Milliarden mal – schneller als du zählen kannst!',
        kids_2: '🎨 Das Bild entsteht in 50 kleinen Schritten – wie ein Puzzle das sich selbst löst!',
        kids_3: '🧩 Millionen von Zahlen fliegen gerade durch die GPU!',
        youth_1: '🔄 Jedes Bild durchläuft ~50 "Denoising Steps" – 50 Runden Rauschen entfernen!',
        youth_2: '📐 8 Milliarden Parameter werden gerade abgefragt – pro Bild!',
        youth_3: '🧠 Die KI "denkt" in Vektoren mit tausenden Dimensionen – wie Koordinaten in einem Raum.',
        expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – Text + Bild in gemeinsamen Attention-Layern',
        expert_2: '📈 Self-Attention: O(n²) Komplexität – jedes Token "sieht" alle anderen',
        expert_3: '⚙️ Classifier-Free Guidance: Prompt-Einfluss vs. Kreativität-Balance'
      },
      model: {
        kids_1: '🎓 Das KI-Modell hat sich Millionen Bilder angeschaut um malen zu lernen!',
        kids_2: '🤖 Die KI ist wie ein Künstler der nie vergisst was er gesehen hat!',
        kids_3: '✨ 8 Milliarden Verbindungen im Modell – mehr als Sterne am Himmel die du sehen kannst!',
        youth_1: '🧠 SD3.5 Large hat 8 Milliarden Parameter – wie 8 Milliarden Entscheidungsknoten.',
        youth_2: '📚 3 Text-Encoder arbeiten zusammen: CLIP-L, CLIP-G und T5-XXL',
        youth_3: '🔢 Das Modell braucht {vram} GB VRAM nur um geladen zu werden!',
        expert_1: '🏗️ Architektur: Rectified Flow + MMDiT mit 38 Transformer-Blöcken',
        expert_2: '📊 FP16/FP8 Quantisierung: Präzision vs. VRAM-Trade-off',
        expert_3: '🔗 LoRA: Low-Rank Adaptation – nur 0.1% der Parameter neu trainiert'
      },
      ethics: {
        kids_1: '🌍 KI lernt von Bildern im Internet – deshalb ist es wichtig, fair mit Kunst anderer zu sein!',
        kids_2: '⚖️ Nicht alle Künstler wurden gefragt ob die KI von ihnen lernen darf.',
        kids_3: '🤝 Gute KI respektiert die Arbeit von Menschen!',
        youth_1: '📜 Trainingsdaten stammen oft aus dem Internet. Künstler diskutieren: Fair Use oder Kopieren?',
        youth_2: '🏛️ Der EU AI Act fordert Transparenz: Woher kommen die Trainingsdaten?',
        youth_3: '💭 Frage: Wem gehört ein KI-generiertes Bild eigentlich?',
        expert_1: '⚠️ LAION-5B wurde teils ohne Urheber-Zustimmung erstellt – rechtliche Grauzone.',
        expert_2: '📋 EU AI Act Art. 52: Kennzeichnungspflicht für KI-generierte Inhalte',
        expert_3: '🔍 Model Cards & Datasheets: Best Practice für ML-Transparenz'
      },
      environment: {
        kids_1: '☁️ Jedes KI-Bild produziert ein bisschen CO₂ – wie Autofahren, nur weniger!',
        kids_2: '🌱 Überlege: Ist dieses Bild den Strom wert?',
        kids_3: '🌞 Die Energie für KI kommt oft aus Kraftwerken – manche sauber, manche nicht.',
        youth_1: '🏭 Deutscher Strommix: ~400g CO₂ pro kWh – das addiert sich!',
        youth_2: '📈 {co2}g CO₂ für dieses Bild – bei 1000 Bildern wären das {totalKg} kg!',
        youth_3: '💡 Tipp: Weniger Bilder generieren, dafür bewusster – spart Energie und CO₂.',
        expert_1: '📊 Berechnung: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Scope 2 Emissionen: Standort des Rechenzentrums entscheidend',
        expert_3: '⚡ PUE (Power Usage Effectiveness): Zusätzlicher Energie-Overhead für Kühlung'
      },
      iceberg: {
        drawPrompt: 'KI-Generierung verbraucht viel Energie. Zeichne Eisberge und schau was geschieht...',
        redraw: 'Neu zeichnen',
        startMelting: 'Schmelzen starten',
        melting: 'Eisberg schmilzt...',
        melted: 'Geschmolzen!',
        meltedMessage: '{co2}g CO₂ produziert',
        comparison: 'Diese CO₂-Menge lässt etwa {volume} cm³ Arktis-Eis schmelzen.',
        comparisonInfo: '(Jede Tonne CO₂ = ca. 6m³ Meereis-Verlust)',
        gpuPower: 'Stromverbrauch der Grafikkarte',
        gpuTemp: 'Temperatur der Grafikkarte',
        co2Info: 'CO₂-Emissionen durch Stromverbrauch (basierend auf deutschem Strommix)',
        drawAgain: 'Zeichne weitere Eisberge...'
      },
      pixel: {
        grafikkarte: 'Grafikkarte',
        energieverbrauch: 'Energieverbrauch',
        co2Menge: 'CO₂-Menge',
        smartphoneComparison: 'Du müsstest Dein Handy {minutes} Minuten ausgeschaltet lassen, um den CO₂-Verbrauch wieder auszugleichen!',
        clickToProcess: 'Klicke auf die Daten-Pixel um ein Minibild zu generieren!'
      },
      forest: {
        trees: 'Bäume',
        clickToPlant: 'Klicke um Bäume zu pflanzen! Wo Du einen Baum pflanzt, wird die Fabrik verschwinden.',
        gameOver: 'Der Wald ist verloren!',
        treesPlanted: 'Du hast {count} Bäume gepflanzt.',
        complete: 'Generation abgeschlossen',
        comparison: 'Ein durchschnittlicher Baum braucht {minutes} Minuten, um diese CO₂-Menge zu absorbieren.'
      },
      rareearth: {
        clickToClean: 'Klicke auf den See um Giftschlamm zu entfernen!',
        sludgeRemoved: 'Schlamm entfernt',
        environmentHealth: 'Umwelt',
        gameOverInactive: 'Du hast aufgegeben... der Abbau geht weiter',
        infoBanner: 'Seltene Erden für GPU-Chips: Der Abbau hinterlässt Giftschlamm und zerstört Ökosysteme. Deine Aufräum-Arbeit kann die Geschwindigkeit des Abbaus nicht aufhalten.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Umwelt',
        statsSludge: 'Schlamm entfernt'
      }
    }
  },
  en: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Creative AI Transformations'
    },
    form: {
      inputLabel: 'Your Text',
      inputPlaceholder: 'e.g. A flower in the meadow',
      schemaLabel: 'Transformation Style',
      executeModeLabel: 'Execution Mode',
      safetyLabel: 'Safety Level',
      generateButton: 'Generate'
    },
    schemas: {
      dada: 'Dada (Random & Absurd)',
      bauhaus: 'Bauhaus (Geometric)',
      stillepost: 'Stille Post (Iterative)'
    },
    executionModes: {
      eco: 'Eco (Fast)',
      fast: 'Fast (Balanced)',
      best: 'Best (Quality)'
    },
    safetyLevels: {
      kids: 'Kids',
      youth: 'Youth',
      adult: 'Adults',
      research: 'Research'
    },
    stages: {
      pipeline_starting: 'Pipeline Starting',
      translation_and_safety: 'Translation & Safety',
      interception: 'Transformation',
      pre_output_safety: 'Output Safety',
      media_generation: 'Image Generation',
      completed: 'Completed'
    },
    status: {
      idle: 'Ready',
      executing: 'Pipeline running...',
      connectionSlow: 'Connection slow, retrying...',
      completed: 'Pipeline completed!',
      error: 'Error occurred'
    },
    entities: {
      input: 'Input',
      translation: 'Translation',
      safety: 'Safety Check',
      interception: 'Transformation',
      safety_pre_output: 'Output Safety',
      media: 'Generated Image'
    },
    properties: {
      chill: 'chill',
      chaotic: 'wild',
      narrative: 'tell stories',
      algorithmic: 'follow rules',
      historical: 'history',
      contemporary: 'present',
      explore: 'test AI',
      create: 'make art',
      playful: 'playful',
      serious: 'serious'
    },
    phase2: {
      title: 'Prompt Input',
      userInput: 'Your Input',
      yourInput: 'Your Input',
      yourIdea: 'Your Idea: WHAT should this be about?',
      rules: 'Your Rules: HOW should your idea be implemented?',
      yourInstructions: 'Your Instructions',
      what: 'WHAT',
      how: 'HOW',
      userInputPlaceholder: 'e.g. A flower in the meadow',
      inputPlaceholder: 'Your text appears here...',
      metaPrompt: 'Artistic Instruction',
      instruction: 'Instruction',
      transformation: 'Artistic Transformation',
      metaPromptPlaceholder: 'Describe the transformation...',
      result: 'Result',
      expectedResult: 'Expected Result',
      execute: 'Execute Pipeline',
      executing: 'Running...',
      transforming: 'LLM transforming...',
      startTransformation: 'Start Transformation',
      letsGo: 'Ok, let\'s go!',
      modified: 'Modified',
      reset: 'Reset',
      loadingConfig: 'Loading configuration...',
      loadingMetaPrompt: 'Loading meta-prompt...',
      errorLoadingConfig: 'Error loading configuration',
      errorLoadingMetaPrompt: 'Error loading meta-prompt',
      threeForces: '3 Forces Working Together',
      twoForces: 'WHAT + HOW → LLM → Result',
      yourPrompt: 'Your Prompt:',
      writeYourText: 'Write your text...',
      examples: 'Examples',
      estimatedTime: '~12 seconds',
      stage12Time: '~5-10 seconds',
      willAppearAfterExecution: 'Will appear after execution...',
      back: 'Back',
      retry: 'Retry',
      transformedPrompt: 'Transformed Prompt',
      notYetTransformed: 'Not yet transformed...',
      transform: 'Transform',
      reTransform: 'Try again differently',
      startAI: 'AI, process my input',
      aiWorking: 'AI is working...',
      continueToMedia: 'Continue to Image Generation',
      readyForMedia: 'Ready for Image Generation',
      stage1: 'Stage 1: Translation + Safety...',
      stage2: 'Stage 2: Transformation...',
      selectMedia: 'Choose your medium:',
      mediaImage: 'Image',
      mediaAudio: 'Audio',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Coming soon',
      generateMedia: 'Start!'
    },
    phase3: {
      generating: 'Image is being generated...',
      generatingHint: '~30 seconds'
    },
    common: {
      back: 'Back',
      loading: 'Loading...',
      error: 'Error',
      retry: 'Retry',
      cancel: 'Cancel',
      checkingSafety: 'Checking...'
    },
    gallery: {
      title: 'Favorites',  // Session 145: "My" redundant with switch
      empty: 'No favorites yet',
      favorite: 'Add to favorites',
      unfavorite: 'Remove from favorites',
      continue: 'Continue editing',
      restore: 'Restore session',
      viewMine: 'My favorites',  // Session 145
      viewAll: 'All favorites'  // Session 145
    },
    settings: {
      authRequired: 'Authentication Required',
      authPrompt: 'Please enter the password to access settings:',
      passwordPlaceholder: 'Enter password...',
      authenticate: 'Sign In',
      authenticating: 'Authenticating...',
      // Admin page
      title: 'Administration',
      tabs: {
        export: 'Research Data',
        config: 'Configuration',
        demos: 'Minigame Demo',
        matrix: 'Model Matrix'
      },
      loading: 'Loading settings...',
      presets: {
        title: 'Model Presets',
        help: 'Use the <strong>Model Matrix</strong> tab to see all available presets and apply them with one click.',
        openMatrix: 'Open Model Matrix'
      },
      testingTools: {
        title: 'Testing Tools for Educators',
        help: 'Test and explore the pedagogical minigames and animations before using them with learners.',
        openPreview: 'Open Minigame Preview',
        pixelEditor: 'Pixel Template Editor',
        includes: 'Includes: Pixel Animation, Iceberg Melting, Forest Game, Rare Earths'
      },
      general: {
        title: 'General Configuration',
        uiMode: 'UI Mode',
        uiModeHelp: 'Interface complexity level',
        kids: 'Kids (8–12)',
        youth: 'Youth (13–17)',
        expert: 'Expert',
        safetyLevel: 'Safety Level',
        defaultLanguage: 'Default Language',
        germanDe: 'German (de)',
        englishEn: 'English (en)',
        turkishTr: 'Turkish (tr)',
        koreanKo: 'Korean (ko)',
        ukrainianUk: 'Ukrainian (uk)',
        frenchFr: 'French (fr)'
      },
      safety: {
        kidsTitle: 'Children (8–12)',
        kidsDesc: 'All filters active: §86a, DSGVO, Youth Protection (age-appropriate parameters), VLM image check',
        youthTitle: 'Youth (13–17)',
        youthDesc: 'All filters active: §86a, DSGVO, Youth Protection (youth parameters), VLM image check',
        adultTitle: 'Adults',
        adultDesc: '§86a + DSGVO active. No youth protection, no VLM image check.',
        researchTitle: 'Research Mode',
        researchDesc: 'NO safety filters active. Only permitted for use by research institutions in the context of scientific research projects.'
      },
      safetyModels: {
        title: 'Local Safety Models',
        help: 'Local via Ollama — person names and safety checks never leave the system',
        safetyModel: 'Safety Model',
        safetyModelHelp: 'Guard model for content safety (§86a, youth protection)',
        dsgvoModel: 'DSGVO-Verify Model',
        dsgvoModelHelp: 'General-purpose model for DSGVO NER verification (not a guard model)',
        vlmModel: 'VLM Safety Model',
        vlmModelHelp: 'Vision model for post-generation image safety check (kids/youth)',
        fast: 'fast, minimal',
        recommended: 'recommended'
      },
      dsgvo: {
        title: 'DSGVO Warning',
        notCompliant: 'The following models are <strong>NOT DSGVO-compliant</strong> (data processed outside EU):',
        compliantHint: 'DSGVO-compliant options:'
      },
      models: {
        title: 'Model Configuration',
        help: 'Model identifiers with provider prefix: local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: 'Use of Model Matrix is advised. However, you may configure your settings here freely.',
        ollamaAvailable: '{count} Ollama models available (type or select from dropdown)',
        stage1Text: 'Stage 1 - Text Model',
        stage1Vision: 'Stage 1 - Vision Model',
        stage2Interception: 'Stage 2 - Interception Model',
        stage2Optimization: 'Stage 2 - Optimization Model',
        stage3: 'Stage 3 - Translation/Safety Model',
        stage4Legacy: 'Stage 4 - Legacy Model',
        chatHelper: 'Chat Helper Model',
        imageAnalysis: 'Image Analysis Model',
        coding: 'Code Generation (Tone.js, p5.js)'
      },
      api: {
        title: 'API Configuration',
        llmProvider: 'LLM Provider',
        localFramework: 'Local LLM framework',
        externalProvider: 'External LLM Provider',
        cloudProvider: 'Cloud LLM provider - requires API key',
        noneLocal: 'None (Local only, DSGVO)',
        mistralEu: 'Mistral AI (EU-based, DSGVO)',
        anthropicDirect: 'Anthropic Direct API (NOT DSGVO)',
        openaiDirect: 'OpenAI Direct API (NOT DSGVO)',
        openrouterDirect: 'OpenRouter (NOT DSGVO, EU routing available)',
        mistralInfo: 'Mistral AI (EU-based)',
        mistralDsgvo: 'DSGVO-compliant (EU infrastructure)',
        anthropicInfo: 'Anthropic Direct API',
        anthropicNotDsgvo: 'NOT DSGVO-compliant',
        anthropicWarning: 'Data processed outside EU. Use only for non-educational contexts.',
        openaiInfo: 'OpenAI Direct API',
        openaiNotDsgvo: 'NOT DSGVO-compliant (US-based)',
        openaiWarning: 'Data processed in United States. Use only for non-educational contexts.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: 'NOT DSGVO-compliant (US company)',
        openrouterWarning: 'EU server routing configurable in OpenRouter settings, but company is US-based.',
        storedIn: 'Stored in',
        currentKey: 'Current'
      },
      save: {
        saveApply: 'Save & Apply',
        saving: 'Saving...',
        applying: 'Applying...',
        success: 'Settings saved and applied',
        presetApplied: 'Applied preset: {preset}'
      }
    },
    pipeline: {
      yourInput: 'Your input',
      result: 'Result',
      generatedMedia: 'Generated image'
    },
    landing: {
      subtitlePrefix: 'Pedagogical-artistic experimentation platform of the',
      subtitleSuffix: 'for the explorative use of generative AI in cultural-aesthetic media education',
      research: '',
      features: {
        textTransformation: {
          title: 'Text Transformation',
          description: 'Perspective shift through AI — your prompt is transformed through artistic-pedagogical lenses into image, video, and sound.'
        },
        imageTransformation: {
          title: 'Image Transformation',
          description: 'Transform images through different models and perspectives into new images and videos.'
        },
        multiImage: {
          title: 'Image Fusion',
          description: 'Combine multiple images and merge them into new image compositions through AI models.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Visual workflow composition — connect modules via drag & drop into custom AI pipelines.'
        },
        music: {
          title: 'Music Generation',
          description: 'AI-powered music creation with lyrics, tags, and stylistic control.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vector space research — surrealization, dimension elimination, embedding interpolation.'
        }
      }
    },
    research: {
      locked: 'Only available in research mode',
      lockedHint: 'Requires safety level "Adult" or "Research" (config.py)',
      complianceTitle: 'Research Mode Notice',
      complianceWarning: 'In research mode, no safety filters are active for prompts or generated images. Unexpected or inappropriate results may occur.',
      complianceAge: 'This mode is not recommended for persons under 16 years of age.',
      complianceConfirm: 'I confirm that I have understood the notices',
      complianceCancel: 'Cancel',
      complianceProceed: 'Proceed'
    },
    presetOverlay: {
      title: 'Choose Perspective',
      close: 'Close'
    },
    imageUpload: {
      clickHere: 'Click here',
      orDragImage: 'or drag an image here',
      formatHint: 'PNG, JPG, WEBP (max 10MB)',
      invalidFormat: 'Invalid file format. Only PNG, JPG, and WEBP allowed.',
      fileTooLarge: 'File too large. Maximum: {max}MB',
      uploadFailed: 'Upload failed',
      infoOriginal: 'Original:',
      infoSize: 'Size:'
    },
    mediaInput: {
      choosePreset: 'Choose Perspective',
      translateToEnglish: 'Translate to English',
      copy: 'Copy',
      paste: 'Paste',
      delete: 'Delete',
      loading: 'Loading...',
      contentBlocked: 'Content blocked'
    },
    nav: {
      about: 'About',
      impressum: 'Imprint',
      privacy: 'Privacy',
      docs: 'Documentation',
      language: 'Switch language',
      settings: 'Settings',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'New Workflow',
      importWorkflow: 'Import',
      exportWorkflow: 'Export',
      execute: 'Execute',
      ready: 'Ready',
      errors: 'errors',
      discardWorkflow: 'Discard current workflow?',
      importError: 'Failed to import file',
      selectTransformation: 'Select Transformation',
      selectOutput: 'Select Output Model',
      search: 'Search...',
      noResults: 'No results found',
      dragHint: 'Click or drag modules onto the canvas',
      editNameHint: '(double-click to edit)',
      modules: 'Modules',
      toggleSidebar: 'Toggle sidebar',
      dsgvoTooltip: 'Canvas workflows may use external LLM APIs. GDPR compliance is the responsibility of the user.',
      batchExecute: 'Batch Execute',
      batchExecution: 'Batch Execution',
      batchAbort: 'Abort Batch',
      abort: 'Abort',
      cancel: 'Cancel',
      loading: 'Loading...',
      executingWorkflow: 'Executing Workflow...',
      starting: 'Starting...',
      nodes: 'nodes',
      batchRunCount: 'Number of Runs',
      batchUseSeed: 'Use Base Seed',
      batchBaseSeed: 'Base Seed',
      batchSeedHint: 'Each run: seed + index',
      batchStart: 'Start Batch',
      stage: {
        configSelectPlaceholder: 'Select...',
        evaluationCriteriaFallback: 'Evaluation criteria...',
        feedbackInputTitle: 'Feedback Input',
        deleteTitle: 'Delete',
        selectLlmPlaceholder: 'Select LLM...',
        resizeTitle: 'Resize',
        input: {
          promptPlaceholder: 'Your prompt...'
        },
        imageInput: {
          uploadLabel: 'Upload Image'
        },
        interception: {
          contextPromptLabel: 'Context Prompt',
          contextPromptPlaceholder: 'Transformation instructions...'
        },
        translation: {
          translationPromptLabel: 'Translation Prompt',
          translationPromptPlaceholder: 'Translation instructions...'
        },
        modelAdaption: {
          targetModelLabel: 'Target Model',
          noAdaptionOption: 'No Adaption',
          videoModelsOption: 'Video Models',
          audioModelsOption: 'Audio Models'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Comparison Criteria',
          criteriaPlaceholder: 'e.g. Compare by originality, clarity, detail...',
          infoText: 'Connect up to 3 text outputs'
        },
        seed: {
          modeLabel: 'Mode',
          modeFixed: 'Fixed',
          modeRandom: 'Random',
          valueLabel: 'Value',
          baseLabel: 'Base'
        },
        resolution: {
          customOption: 'Custom',
          widthLabel: 'Width',
          heightLabel: 'Height'
        },
        collector: {
          emptyText: 'Waiting for execution...'
        },
        evaluation: {
          typeLabel: 'Evaluation Type',
          typeCreativity: 'Creativity',
          typeQuality: 'Quality',
          typeCustom: 'Custom',
          criteriaLabel: 'Evaluation Criteria',
          outputTypeLabel: 'Output Type',
          outputCommentary: 'Commentary + Binary',
          outputScore: 'Commentary + Score + Binary',
          outputAll: 'All',
          evalPassTitle: 'Pass (forward)',
          evalFailTitle: 'Feedback (backward)',
          evalCommentaryTitle: 'Commentary (forward)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Select Vision Model...',
          frameworkLabel: 'Analysis Framework',
          frameworkPanofsky: 'Art Historical (Panofsky)',
          frameworkEducational: 'Educational Theory',
          frameworkEthical: 'Ethical',
          frameworkCritical: 'Critical/Decolonial',
          frameworkCustom: 'Custom',
          customPromptLabel: 'Analysis Prompt',
          customPromptPlaceholder: 'Describe how the image should be analyzed...'
        },
        display: {
          imageAlt: 'Preview',
          emptyText: 'Preview (after execution)'
        }
      }
    },
    about: {
      title: 'About the UCDCAE AI LAB',
      intro: 'The UCDCAE AI LAB is a pedagogical-artistic experimentation platform of the UNESCO Chair in Digital Culture and Arts in Education for the explorative use of generative artificial intelligence in cultural-aesthetic media education. It was developed within the AI4ArtsEd and COMeARTS projects.',
      project: {
        title: 'The Project',
        description: 'AI is transforming society and the world of work; it is increasingly becoming a subject of education. The project explores opportunities, conditions, and limits of the pedagogical use of artificial intelligence (AI) in culturally diversity-sensitive settings of cultural education.',
        paragraph2: 'In three sub-projects – General Pedagogy (TPap), Computer Science (TPinf), and Art Education (TPkp) – creativity-oriented pedagogical AI practice research and computer science AI conception and programming interlock in close cooperation. From the outset, the project systematically involves artistic-pedagogical practitioners in the design process; it acts as a bridge between professional (quality-related, aesthetic, ethical, and value-based) pedagogical-practical implementation on the one hand and the implementation and training process of the computer science sub-project on the other.',
        paragraph3: 'A participatory design process spanning approximately two years aims to produce an open-source AI technology that explores the extent to which AI systems can already incorporate artistic-pedagogical principles at their structural level under favorable real-world conditions.',
        paragraph4: 'The focus is on a) the future applicability and added value of highly innovative technologies for cultural education, b) the scope and limits of AI literacy among teachers and learners, and c) the overarching question of the assessability and evaluation of the transformation of pedagogical settings by complex non-human actors in terms of pedagogical ethics and technology assessment.',
        moreInfo: 'More information:'
      },
      subproject: {
        title: 'Sub-project "General Pedagogy"',
        description: 'The sub-project "General Pedagogy" researches possibilities and limits of an artistic-pedagogical AI design process based on participatory practice research within the framework of the joint research question of the collaborative project. For this purpose, it conducts a series of research, analyses, expert workshops, and open spaces in the first project year. The subsequent project phase, designed as a feedback loop in several cycles, explores the use of a prototype with pedagogical practitioners and artist-educators, particularly in non-formal cultural education, as a relational and collective transformative educational process.'
      },
      team: {
        title: 'Team',
        projectLead: 'Project Lead',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Institute of Education',
        leadChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
        leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
        researcher: 'Research Associate',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Institute of Education',
        researcherChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
        researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
      },
      funding: {
        title: 'Funded by'
      }
    },
    legal: {
      impressum: {
        title: 'Imprint',
        publisher: 'Publisher',
        represented: 'Represented by the President',
        responsible: 'Responsible for content',
        authority: 'Supervisory Authority',
        moreInfo: 'Additional Information',
        moreInfoText: 'Complete imprint of FAU:',
        funding: 'Funded by'
      },
      privacy: {
        title: 'Privacy Policy',
        notice: 'Notice: Generated content is stored on the server for research purposes. No user or IP data is collected. Uploaded images are not stored.',
        usage: 'Use of this platform is exclusively permitted for registered cooperation partners of the UCDCAE AI LAB. The data protection agreements made in this context apply. If you have any questions, please contact vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Documentation & Guide',
      intro: {
        title: 'Welcome',
        content: 'Creative experiments with AI transformations.'
      },
      gettingStarted: {
        title: 'Getting Started',
        step1: 'Select properties from quadrants',
        step2: 'Enter text or image',
        step3: 'Start transformation'
      },
      modes: {
        title: 'Modes',
        mode1: { name: 'Direct', desc: 'Quick experiments' },
        mode2: { name: 'Text', desc: 'Text-based transformations' },
        mode3: { name: 'Image', desc: 'Image-based procedures' }
      },
      support: {
        title: 'Support',
        content: 'For questions:'
      },
      wikipedia: {
        title: 'Wikipedia Research',
        subtitle: 'Knowledge about the world as part of artistic processes',
        feature: 'Artistic processes require not only aesthetic knowledge, but also knowledge about facts in the world. The AI researches Wikipedia during transformation to find factual information.',
        languages: 'Over 70 languages are supported',
        languagesDesc: 'The AI automatically chooses the appropriate language Wikipedia for each topic:',
        examples: {
          nigeria: 'Topic about Nigeria → Hausa, Yoruba, Igbo, or English',
          india: 'Topic about India → Hindi, Tamil, Bengali, or other regional languages',
          indigenous: 'Indigenous cultures → Quechua, Māori, Inuktitut, etc.'
        },
        why: 'Transparency: What does the AI know?',
        whyDesc: 'The system shows all research attempts: Both found articles (as clickable links) and terms for which nothing was found. This makes visible what the AI thinks it knows – and what it does not.',
        culturalRespect: 'Invitation to research yourself',
        culturalRespectDesc: 'The displayed Wikipedia links are an invitation to learn more yourself. Click on the links to check the sources and expand your own knowledge.',
        limitations: 'AI research is an aid, not a substitute for your own engagement with the topic.'
      }
    },
    multiImage: {
      image1Label: 'Image 1',
      image2Label: 'Image 2 (optional)',
      image3Label: 'Image 3 (optional)',
      contextLabel: 'Describe what you want to do with the images',
      contextPlaceholder: 'e.g. Insert the house from image 2 and the horse from image 3 into image 1. Preserve colors and style from image 1.',
      modeTitle: 'Multiple Images → Image',
      selectConfig: 'Choose your model:',
      generating: 'Images are being fused...'
    },
    imageTransform: {
      imageLabel: 'Your Image',
      contextLabel: 'Describe what you want to change in the image',
      contextPlaceholder: 'e.g. Transform it into an oil painting... Make it more colorful... Add a sunset...'
    },
    videoGeneration: {
      promptLabel: 'Your Video Idea',
      promptPlaceholder: 'e.g. A hot air balloon floating over a mountain landscape at sunset...',
      modelLabel: 'Choose a video model:',
      generating: 'Generating video...'
    },
    textTransform: {
      inputLabel: 'Your Idea = WHAT?',
      inputTooltip: 'Enter what your creation should be about.',
      inputPlaceholder: 'e.g. A festival in my street: ...',
      contextLabel: 'Your Rules = HOW?',
      contextTooltip: 'Enter how your idea should be presented, or click the circle icon!',
      contextPlaceholder: 'e.g. Describe everything as the birds in the trees perceive it!',
      resultLabel: 'Idea + Rules = Prompt',
      resultPlaceholder: 'Prompt will appear after clicking start (or enter your own text)',
      optimizedLabel: 'Model-Optimized Prompt',
      optimizedPlaceholder: 'The optimized prompt will appear after model selection.'
    },
    training: {
      info: {
        title: 'About LoRA Training',
        studioDescription: 'Train custom LoRA models for Stable Diffusion 3.5 Large with your own images.',
        description: 'This built-in training is designed for quick tests.',
        limitations: 'Limitations',
        limitationDuration: 'Training takes 1-3 hours',
        limitationBlocking: 'Blocks image generation during training',
        limitationConfig: 'Limited configuration options',
        showMore: 'Learn more',
        showLess: 'Show less'
      },
      placeholders: {
        projectName: 'e.g. Our School Building',
        triggerWords: 'e.g. our_school_building, schoolyard, classroom'
      },
      labels: {
        projectName: 'Project Name',
        triggerWords: 'Trigger Words',
        triggerHelp: 'Comma-separated tags. First = primary trigger, rest = additional tags per image.',
        images: 'Training Images (10–50 recommended)',
        dropZone: 'Click or drop images here',
        imagesSelected: '{count} images selected',
        logs: 'Training Logs',
        waiting: 'Waiting for training to start...'
      },
      buttons: {
        start: 'Start Training',
        stop: 'Stop',
        inProgress: 'Training in Progress...',
        delete: 'Delete Project Files (GDPR)',
        cancel: 'Cancel'
      },
      vram: {
        title: 'GPU VRAM Check',
        checking: 'Checking VRAM...',
        used: 'used',
        free: 'free',
        notEnough: 'Not enough free VRAM for training (need {gb} GB).',
        clearQuestion: 'Clear VRAM to continue?',
        enough: 'Enough VRAM available for training.',
        clearing: 'Clearing VRAM...',
        newFree: 'New free',
        clearBtn: 'Clear ComfyUI + Ollama VRAM'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Age Filter',
      dsgvo_ner: 'GDPR',
      dsgvo_llm: 'GDPR',
      translation: '\u2192 EN',
      fast_filter: 'Content',
      llm_context_check: 'Content (LLM)',
      llm_safety_check: 'Youth Protection',
      llm_check_failed: 'Check Failed',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Your prompt was fine, but the generated image was flagged as unsuitable by an image analysis AI. This can happen \u2014 image generation is not always predictable. Just try again, every generation is different!',
      para86a: 'Your prompt was blocked because it contains symbols or terms that are prohibited under German law (\u00A786a StGB). This rule protects us all from hate and violence. Try a different topic!',
      dsgvo: 'Your prompt was blocked because it contains something that looks like a person\'s name. This is protected by the General Data Protection Regulation (GDPR). Use descriptions like \"a girl\" or \"an old man\" instead of names.',
      kids: 'Your prompt was blocked by the child safety filter. Some terms are not suitable for children because they can be scary or disturbing. Try describing your idea with friendlier words!',
      youth: 'Your prompt was blocked by the youth protection filter. Some content is not suitable for teenagers either. Try rephrasing your idea!',
      generic: 'Your prompt was blocked by the safety system. The system protects you from unsuitable content. Try a different wording!',
      inputImage: 'The uploaded image was flagged as unsuitable by an image analysis AI. Please use a different image.',
      vlmSaw: 'The image AI saw',
      systemUnavailable: 'The safety system (Ollama) is not responding, so no further processing is possible. Please contact the system administrator.',
      suggestionLoading: 'Hang on, I have an idea...',
      suggestionError: 'I couldn\'t generate a suggestion right now. Just try again with different words!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine - Semantic Vector Fusion',
      infoDescription: 'This workflow fuses two prompts at the semantic vector level. The result is not a simple blend, but a deeper mathematical connection of meaning spaces.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Explore how AI models represent meaning as numerical spaces. What happens when we mathematically merge different concepts?',
      techTitle: 'Technical Details',
      techText: 'Model: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Partial Elimination - Vector Deconstruction',
      infoDescription: 'This workflow specifically manipulates parts of the semantic vector. By eliminating certain dimensions, we can observe which aspects of meaning are lost.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Understand how meaning is encoded across different dimensions of the vector space. What remains when we "switch off" parts?',
      techTitle: 'Technical Details',
      techText: 'Model: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Text Encoder',
      modeLabel: 'Elimination Mode',
      dimensionRange: 'Dimension Range',
      selected: 'Selected',
      dimensions: 'Dimensions',
      emptyTitle: 'Waiting for generation...',
      emptySubtitle: 'Results will appear here',
      referenceLabel: 'Reference Image',
      referenceDesc: 'Unmanipulated output (original)',
      innerLabel: 'Inner range eliminated',
      outerLabel: 'Outer range eliminated'
    },
    surrealizer: {
      infoTitle: 'Surrealizer — Extrapolation Beyond the Known',
      infoDescription: 'Two AI "brains" read your text: CLIP-L understands language through images, T5 understands it purely linguistically. The slider doesn\'t simply blend between them — it pushes the image far beyond what T5 alone would produce. The AI must then interpret vectors it has never encountered during training. The result: AI hallucinations — images that no prompt could directly produce.',
      purposeTitle: 'The Slider',
      purposeText: 'α < 0: CLIP-L amplified, T5 negated — the upper 3328 dimensions (where CLIP-L is zero-padded) receive inverted T5 vectors. Cross-attention patterns in the transformer flip: visually driven hallucinations. ◆ α = 0: pure CLIP-L — normal image. ◆ α = 1: pure T5-XXL — still normal, but different quality. ◆ α > 1: extrapolation past T5. At α = 20 the formula pushes the embedding 19× beyond T5 into unexplored vector space — linguistically driven hallucinations. ◆ Sweet spot: α = 15–35.',
      techTitle: 'How It Works',
      techText: 'Your prompt is sent through two encoders separately: CLIP-L (visually trained, 77 tokens, 768 dims → padded to 4096) and T5-XXL (linguistically trained, 512 tokens, 4096 dims). The first 77 token positions are fused: (1-α)·CLIP-L + α·T5. The remaining T5 tokens (78–512) stay unchanged as a semantic anchor — they keep the image tied to your text no matter how extreme α gets. At α > 1 this is not blending but extrapolation: vectors no training ever produced. At α < 0, T5 is negated and CLIP-L amplified — qualitatively different hallucinations because cross-attention patterns in the transformer are inverted.',
      sliderLabel: 'Extrapolation (α)',
      sliderNormal: 'normal',
      sliderWeird: 'weird',
      sliderCrazy: 'crazy',
      sliderExtremeWeird: 'super weird',
      sliderExtremeCrazy: 'super crazy',
      sliderHint: "α<0: past CLIP {'|'} α=0: pure CLIP {'|'} α=1: pure T5 {'|'} α>1: past T5",
      expandLabel: 'Expand prompt for T5',
      expandSuggest: 'Short prompt detected — T5 expansion significantly improves results with few words.',
      expandHint: 'Your prompt has few words (~{count} CLIP tokens). For optimal hallucinations, the AI can narratively expand the T5 context.',
      expandActive: 'Expanding prompt...',
      expandResultLabel: 'T5 expansion (T5 encoder only)',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      negativeHint: 'Extrapolated with the same α. Determines what the image extrapolates AWAY from — different negatives produce fundamentally different aesthetics.',
      cfgLabel: 'CFG Scale',
      cfgHint: 'Classifier-Free Guidance: strength of prompt influence. Higher = stronger effect, less variation.'
    },
    musicGeneration: {
      infoTitle: 'Music Generation',
      infoDescription: 'Create music from text and style tags. The AI generates melodies, rhythms, and harmonies based on your lyrics and genre specifications.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Explore how AI interprets musical concepts. How does word choice in lyrics affect the melody?',
      lyricsLabel: 'Lyrics (Text)',
      lyricsPlaceholder: '[Verse]\nYour lyrics here...\n\n[Chorus]\nChorus...',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: 'Choose a music model:',
      generate: 'Generate Music',
      generating: 'Generating music...'
    },
    musicGen: {
      simpleMode: 'Simple',
      advancedMode: 'Advanced',
      lyricsLabel: 'Lyrics',
      lyricsPlaceholder: 'Write your song lyrics with structure markers like [Verse], [Chorus], [Bridge]...\n\nExample:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'Genre, mood, instruments...\n\nExample: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: 'Refine Lyrics & Tags',
      refinedLyricsLabel: 'Refined Lyrics',
      refinedLyricsPlaceholder: 'Your refined lyrics will appear here...',
      refiningLyricsMessage: 'AI is refining your lyrics...',
      refinedTagsLabel: 'Refined Tags',
      refinedTagsPlaceholder: 'Refined style tags will appear here...',
      refiningTagsMessage: 'AI is generating matching style tags...',
      selectModel: 'Choose a Music Model',
      generateButton: 'Generate Music',
      quality: 'Quality'
    },
    musicGenV2: {
      lyricsWorkshop: 'Lyrics Workshop',
      lyricsInput: 'Your Text',
      lyricsPlaceholder: 'Write lyrics, a theme, keywords, or a mood...',
      themeToLyrics: 'Keywords to Song Lyrics',
      refineLyrics: 'Structure Song Lyrics',
      resultLabel: 'Result',
      resultPlaceholder: 'Your lyrics will appear here...',
      expandingTheme: 'AI is writing song lyrics from your keywords...',
      refiningLyrics: 'AI is structuring your song lyrics...',
      soundExplorer: 'Sound Explorer',
      suggestFromLyrics: 'Suggest from Lyrics',
      suggestingTags: 'AI is analyzing your lyrics...',
      mostImportant: 'most important',
      dimGenre: 'Genre',
      dimTimbre: 'Timbre',
      dimGender: 'Voice',
      dimMood: 'Mood',
      dimInstrument: 'Instruments',
      dimScene: 'Scene',
      dimRegion: 'Region (UNESCO)',
      dimTopic: 'Topic',
      audioLength: 'Audio Length',
      generateButton: 'Generate Music',
      selectModel: 'Model',
      customTags: 'Custom Tags',
      customTagsPlaceholder: 'e.g. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: 'Image Lab',
        textlab: 'Latent Text Lab',
        crossmodal: 'Crossmodal Lab'
      },
      imageLab: {
        headerTitle: 'Image Lab — Visual Vector Space Research',
        headerSubtitle: 'Five tools for investigating how diffusion models generate images from text: from denoising through attention and fusion to vector arithmetic.',
        tabs: {
          archaeology: {
            label: 'Denoising Archaeology',
            short: 'Watch the model work'
          },
          attention: {
            label: 'Attention Cartography',
            short: 'See where the model looks'
          },
          fusion: {
            label: 'Encoder Fusion',
            short: 'Surrealistic blending'
          },
          probing: {
            label: 'Feature Probing',
            short: 'Dimension-level analysis'
          },
          algebra: {
            label: 'Concept Algebra',
            short: 'Vector arithmetic'
          }
        }
      },
      comingSoon: 'This tool will be implemented in a future version.',
      shared: {
        negativeHint: 'Terms the model should actively avoid (e.g. "blurry, text")',
        stepsHint: 'More steps = higher quality but longer generation time',
        cfgHint: 'Classifier-Free Guidance: higher = stronger prompt adherence, less variation',
        seedHint: '-1 = random, fixed value = reproducible result',
        recordingActive: 'Recording active',
        recordingCount: '{count} recording | {count} recordings',
        recordingTooltip: 'Research data is saved automatically',
      },
      attention: {
        headerTitle: 'Attention Cartography — Which word steers which image region?',
        headerSubtitle: 'For each word in the prompt, a heatmap overlay on the generated image shows WHERE in the image that word had the most influence. This reveals how the model spatially distributes semantic concepts.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'When a diffusion model generates an image, it does not read the prompt word by word like a set of instructions. Instead, a mechanism called "attention" distributes the influence of each word across different image regions. The word "house" mainly influences the region where the house appears — but also neighboring areas, because the model understands the context of the entire scene. This tool makes that distribution visible: click on a word and see which image regions light up.',
        explainHowTitle: 'How do I read the heatmap?',
        explainHowText: 'Bright, intense color = strong influence of the word on that region. Dark or absent color = little influence. If you select multiple words, they appear in different colors. Note: the maps are NOT perfectly sharp-edged — this is not a bug, but shows that the model processes concepts contextually, not in isolation. A "house" in a farm scene also has some influence on animals and fields, because the model understands the scene as a whole.',
        explainReadTitle: 'What do the two sliders reveal?',
        explainReadText: 'The denoising step slider shows WHEN in the 25-step generation process you are viewing attention. Early steps show rough layout planning, late steps show detail assignment. The network depth selector shows WHERE in the transformer attention is measured: shallow layers (near input) show global composition planning, middle layers semantic assignment, deep layers fine-tuning. Both axes are independent — it is worth systematically exploring different combinations.',
        techTitle: 'Technical details',
        techText: 'SD3.5 uses an MMDiT (Multimodal Diffusion Transformer) with joint attention: image and text tokens attend to each other across 24 transformer blocks. We replace the default SDPA processor with a manual softmax(QK^T/√d) processor at 3 selected blocks to extract the text→image attention submatrix. Maps are 64x64 resolution (patch grid), upscaled to image resolution via bilinear interpolation. SD3.5 uses two text encoders: CLIP-L (BPE, 77 tokens) and T5-XXL (SentencePiece, 512 tokens). Both can be toggled here to see how different tokenization strategies affect attention.',
        referencesTitle: 'Research References',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. A house stands in a landscape, surrounded by farmland, nature and animals. Some people can be seen.',
        generate: 'Generate + Analyze',
        generating: 'Generating image and extracting attention...',
        emptyHint: 'Enter a prompt and click Generate to visualize the model\'s attention maps.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Tokens',
        tokensHint: 'Click one or more words. Subword tokens (e.g. "Ku"+"gel") are automatically combined. Multiple words appear in different colors.',
        timestepLabel: 'Denoising step',
        timestepHint: 'Diffusion models generate images in 25 steps from noise to image. Early steps establish rough structure, late steps refine details. This slider shows what the model attends to at each step.',
        step: 'Step',
        layerLabel: 'Network depth',
        layerHint: 'At every denoising step, the signal passes through all 24 transformer layers. Shallow layers (near input) capture global composition, middle layers semantic assignment, deep layers (near output) fine details. Both controls are independent: step = when in the process, depth = where in the network.',
        layerEarly: 'Shallow (Composition)',
        layerMid: 'Middle (Semantics)',
        layerLate: 'Deep (Detail)',
        opacityLabel: 'Heatmap',
        opacityHint: 'Strength of the colored overlay on the image.',
        baseImageLabel: 'Base image',
        baseColor: 'Color',
        baseBW: 'B/W',
        baseOff: 'Off',
        baseImageHint: 'Color shows the original image. B/W desaturates it so heatmap colors stand out. Off hides the image entirely and shows only the attention map.',
        encoderLabel: 'Text Encoder',
        encoderClipL: 'CLIP-L (77 Tokens)',
        encoderT5: 'T5-XXL (512 Tokens)',
        encoderHint: 'SD3.5 uses two text encoders with different tokenization. CLIP-L uses BPE (Byte-Pair Encoding), T5-XXL uses SentencePiece. Compare how both encoders process the same prompt and which image regions each one steers.',
        download: 'Download Image'
      },
      probing: {
        headerTitle: 'Feature Probing — Which dimensions encode what?',
        headerSubtitle: 'Compare two prompts and discover which embedding dimensions encode the semantic difference. Selectively transfer individual dimensions to see how they affect the image.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'Every word is converted by the text encoder into a high-dimensional vector (e.g. 4096 dimensions for T5). When you change a word in the prompt — e.g. "red" to "blue" — certain dimensions change more than others. This tool shows you WHICH dimensions change most and lets you selectively transfer individual dimensions from prompt B into prompt A.',
        explainHowTitle: 'How does the transfer work?',
        explainHowText: 'The bar chart shows all dimensions sorted by difference magnitude. Use the rank range controls (From/To) to select a window — e.g. just the top 100 or specifically ranks 880–920. Clicking "Transfer" regenerates the image with the same settings (same seed!) — but with selected dimensions from prompt B. This lets you see exactly what those dimensions "encode".',
        explainReadTitle: 'How do I read the bar chart?',
        explainReadText: 'Each bar represents one embedding dimension. The length shows how much that dimension differs between prompt A and B. Dimensions with large differences are the most likely carriers of the semantic change. But note: embeddings are distributed — often multiple dimensions together are needed to produce a visible change.',
        techTitle: 'Technical details',
        techText: 'SD3.5 uses three text encoders: CLIP-L (768d), CLIP-G (1280d) and T5-XXL (4096d). You can probe each individually. The difference is computed as mean absolute deviation across all token positions: mean(abs(B-A), dim=tokens). The transfer replaces selected dimensions across all token positions simultaneously.',
        referencesTitle: 'Research References',
        promptALabel: 'Prompt A (Original)',
        promptBLabel: 'Prompt B (Comparison)',
        promptAPlaceholder: 'e.g. A red house by the lake',
        promptBPlaceholder: 'e.g. A blue house by the lake',
        encoderLabel: 'Encoder',
        encoderAll: 'All (recommended)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analyze',
        analyzing: 'Encoding and comparing prompts...',
        transferBtn: 'Transfer selected vector dimensions from Prompt B into the generated image',
        transferring: 'Generating image with modified embedding...',
        rankFromLabel: 'From rank',
        rankToLabel: 'To rank',
        sliderLabel: 'Select dimensions from Prompt B',
        range1Label: 'Range 1',
        range2Label: 'Range 2',
        addRange: 'Add range',
        selectionDesc: '{count} dimensions from Prompt B selected (rank {ranges} of {total})',
        listTitle: 'The {count} dimensions from Prompt B with the largest difference to Prompt A',
        sortAsc: 'Ascending',
        sortDesc: 'Descending',
        originalLabel: 'Original (Prompt A)',
        modifiedLabel: 'Modified (Transfer from Prompt B)',
        modifiedHint: 'Select a rank range below and click "Transfer" — this will show prompt A with the transferred dimensions from B (same seed).',
        noDifference: 'The embeddings are identical — change prompt B.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'All',
        selectNone: 'None',
        encoderHint: 'All = all encoders combined. CLIP-L/CLIP-G/T5 = isolates one encoder for the analysis.',
        sliderHint: 'Select a rank range of the most important embedding dimensions (sorted by difference between A and B).',
        transferHint: 'Transfers the selected dimensions from Prompt B onto Prompt A and generates a new image.',
        downloadOriginal: 'Download Original',
        downloadModified: 'Download Modified'
      },
      algebra: {
        headerTitle: 'Concept Algebra \u2014 Vector arithmetic on image embeddings',
        headerSubtitle: 'Apply the famous word2vec analogy to image generation: King \u2212 Man + Woman \u2248 Queen. Three prompts are encoded and algebraically combined.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'In 2013, Mikolov showed that word embeddings encode semantic relationships as linear directions: the vector for "King" minus "Man" plus "Woman" yields a vector close to "Queen". This tool applies that idea to SD3.5\'s text encoders: instead of single words, you manipulate entire prompt embeddings. The result is an image that contains concept A but with B replaced by C.',
        explainHowTitle: 'How does the algebra work \u2014 and why not just use a negative prompt?',
        explainHowText: 'You enter three prompts: A (base), B (subtract), and C (add). The formula is: Result = A \u2212 Scale\u2081\u00d7B + Scale\u2082\u00d7C. The scale sliders control intensity: at 1.0, B is fully subtracted and C fully added. At 0.5, only half. Values above 1.0 amplify the effect. \u2014 Why not just use "A + C" as the prompt and "B" as the negative prompt? Because that does something fundamentally different: A negative prompt steers the denoising process away from B at EVERY one of the 25 steps \u2014 the model decides step by step how to interpret "not B". Concept Algebra instead computes a new vector BEFORE image generation: the subtraction happens in embedding space, not in the diffusion process. The result is a single vector that directly encodes "A without B-ness plus C-ness". The negative prompt says "don\'t do this". The algebra says "take this concept out and put that one in" \u2014 a surgical operation in meaning-space rather than a step-by-step avoidance strategy.',
        explainReadTitle: 'What do the results mean?',
        explainReadText: 'On the left you see the reference image (prompt A only, same seed). On the right, the algebra result. If the analogy works, the right image should show concept A but with the semantic change B\u2192C. Example: "Sunset at the beach" \u2212 "Beach" + "Mountains" \u2248 "Sunset over mountains". The L2 distance shows how far the result has moved from the original. \u2014 Is the operation commutative? No. Subtraction of B and addition of C happen relative to vector A. The direction B\u2192C only makes sense in the context of A: "King \u2212 Man" removes the "male" directions from the King vector, "+ Woman" adds the "female" directions \u2014 the result lands near "Queen". C is not surgically placed where B was removed; it is simply added. That this still works shows that semantic relationships are encoded as consistent linear directions in the vector space.',
        techTitle: 'Technical details',
        techText: 'The algebra is performed on the selected encoder embeddings: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), or all combined (589 tokens \u00d7 4096d). The same operation is also applied to pooled embeddings (2048d). Both images use the same seed for fair comparison.',
        referencesTitle: 'Research References',
        promptALabel: 'Prompt A (Base)',
        promptAPlaceholder: 'e.g. Sunset at the beach with palm trees',
        promptBLabel: 'Prompt B (Subtract)',
        promptBPlaceholder: 'e.g. Beach with palm trees',
        promptCLabel: 'Prompt C (Add)',
        promptCPlaceholder: 'e.g. Snow-covered mountains',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Encoder',
        encoderAll: 'All (recommended)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Compute',
        generating: 'Computing embeddings and generating images...',
        referenceLabel: 'Reference (Prompt A)',
        resultLabel: 'Result (A \u2212 B + C)',
        l2Label: 'L2 distance from original',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Subtraction scale',
        scaleAddLabel: 'Addition scale',
        encoderHint: 'All = all encoders combined. CLIP-L/CLIP-G/T5 = isolates one encoder for the arithmetic.',
        scaleSubHint: 'Weight of subtraction (B). Higher = stronger removal of concept B.',
        scaleAddHint: 'Weight of addition (C). Higher = stronger injection of concept C.',
        l2Hint: 'Euclidean distance in embedding space. Smaller = more similar, larger = more different.',
        downloadReference: 'Download Reference',
        downloadResult: 'Download Result',
        resultHint: 'Enter three prompts and click Compute \u2014 the result of the vector arithmetic will appear here.'
      },
      archaeology: {
        headerTitle: 'Denoising Archaeology \u2014 How does noise become an image?',
        headerSubtitle: 'Observe every single denoising step. Diffusion models don\'t draw left-to-right \u2014 they work everywhere simultaneously, from rough shapes to fine detail.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'A diffusion model creates an image by progressively removing noise. Unlike drawing from left to right, the model works on ALL image regions simultaneously. In the first steps, rough structures emerge: Where is up, where is down? Where is the horizon? In the middle steps, semantic content appears: objects, shapes, colors. The final steps refine textures and details. This tool makes every single step visible.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'Enter a prompt and click Generate. The model produces 25 intermediate images (one per denoising step). These appear as a filmstrip below. Click a thumbnail or use the timeline slider to view each step at full size. Compare early and late steps: When does the model "know" what it is drawing?',
        explainReadTitle: 'What do the three phases reveal?',
        explainReadText: 'Early steps (1\u20138): Global composition \u2014 basic structure, color distribution, layout planning. Middle steps (9\u201317): Semantic emergence \u2014 objects become recognizable, shapes crystallize. Late steps (18\u201325): Detail refinement \u2014 textures, edges, fine patterns. The transitions are gradual, but the phases clearly show: the model first "plans" globally, then refines locally. Particularly revealing: The very first step does not show fine-grained pixels, but colorful patches. This is because the noise is generated in latent space (128\u00d7128 at 16 channels), not in pixel space. The VAE translates each latent pixel into an ~8\u00d78 pixel patch \u2014 even pure Gaussian noise becomes coherent color clusters. The model never "thinks" in individual pixels, but always in this compressed space.',
        techTitle: 'Technical details',
        techText: 'SD3.5 Large uses Rectified Flow as scheduler with 25 default steps. At each step, the current latent vectors are decoded through the VAE (1024\u00d71024 JPEG). The VAE (Variational Autoencoder) translates the mathematical latent space into pixels. The latent representation is 128\u00d7128 at 16 channels \u2014 each latent pixel corresponds to an ~8\u00d78 pixel patch in the image. This is why even the first step shows colorful clusters instead of fine pixel noise: the VAE interprets random 16-dimensional vectors as coherent color patches.',
        referencesTitle: 'Research References',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. A marketplace in a medieval town with people, buildings and a fountain',
        generate: 'Generate',
        generating: 'Generating image \u2014 recording every step...',
        emptyHint: 'Enter a prompt and click Generate to visualize the denoising process.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Denoising Filmstrip',
        timelineLabel: 'Step',
        phaseEarly: 'Composition',
        phaseMid: 'Semantics',
        phaseLate: 'Detail',
        phaseEarlyDesc: 'Global structure and color distribution emerge',
        phaseMidDesc: 'Objects and shapes become recognizable',
        phaseLateDesc: 'Textures and fine details are sharpened',
        finalImageLabel: 'Final image (full resolution)',
        timelineHint: 'Scrubs through denoising steps — shows how the image emerges from noise to final composition.',
        download: 'Download Image'
      },
      textLab: {
        headerTitle: 'Latent Text Lab \u2014 Scientific LLM Deconstruction',
        headerSubtitle: 'Representation Engineering, comparative model archaeology, and systematic bias analysis: Three research-based tools for investigating language models.',
        explanationToggle: 'Show explanation',
        modelPanel: {
          presetLabel: 'Preset',
          presetNone: 'No preset (custom ID)',
          customModelLabel: 'HuggingFace Model ID',
          customModelPlaceholder: 'e.g. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Quantization',
          quantAuto: 'Auto',
          quantizationHint: 'bf16 = full quality, int8 = half VRAM, int4 = minimal VRAM but lowest quality',
        },
        temperatureHint: 'Randomness of text generation. Low = deterministic, high = more creative.',
        maxTokensHint: 'Maximum number of generated tokens (word pieces).',
        textSeedHint: '-1 = random, fixed value = reproducible result',
        tabs: {
          repeng: { label: 'Representation Engineering', short: 'Find steering vectors in LLMs' },
          compare: { label: 'Model Comparison', short: 'Compare two LLMs layer by layer' },
          bias: { label: 'Bias Archaeology', short: 'Uncover hidden biases in LLMs' },
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: 'Find concept directions in activation space and steer generation',
          explainWhatTitle: 'What does this experiment show?',
          explainWhatText: 'Based on Zou et al. (2023) \u201cRepresentation Engineering\u201d and Li et al. (2024) \u201cInference-Time Intervention\u201d. LLMs encode abstract concepts (truth, sentiment, ethics) as directions in high-dimensional activation space. Through contrast pairs (true vs. false sentence), the direction encoding a concept can be extracted. Adding this direction at runtime changes model behavior in a targeted way \u2014 without retraining. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: 'How do I use it?',
          explainHowText: 'This experiment extracts a \u201ctruth direction\u201d from the model. The pre-filled contrast pairs each contain a true and a false statement. From the differences in activations, the system computes a direction in high-dimensional space. When this direction is inverted (\u03b1 = -1), the model should generate a wrong answer for a factual prompt \u2014 even though it \u201cknows\u201d the correct one. Recommendation: English prompts work significantly better since most open-source LLMs were primarily trained on English data.',
          referencesTitle: 'Research References',
          expectedResults: 'Expected results: At \u03b1 = 0 (baseline), the model generates the correct answer. At \u03b1 = -1 (inversion), a wrong answer should appear \u2014 this is the core of the experiment. At \u03b1 = +1, little changes because the model already answers correctly. Beyond |\u03b1| > 2, artifacts dominate (repetitions, nonsense). The "sweet spot" according to Zou et al. is |\u03b1| between 0.5 and 2.0. Explained variance > 50% indicates clean separation \u2014 below that, contrast pairs are too similar or too few (at least 3 recommended).',
          pairsTitle: 'Contrast Pairs',
          pairsSubtitle: 'At least 3 pairs recommended. Each pair should differ only in the target concept (true vs. false). The examples are editable.',
          positiveLabel: 'Positive (true)',
          negativeLabel: 'Negative (false)',
          positivePlaceholder: 'e.g. The capital of France is Paris',
          negativePlaceholder: 'e.g. The capital of France is Berlin',
          addPair: 'Add pair',
          removePair: 'Remove',
          targetLayerLabel: 'Target layer',
          targetLayerHint: 'Which transformer layer receives the steering vector. Different layers influence different aspects of text generation.',
          targetLayerAuto: 'Last layer',
          findDirection: 'Find direction',
          finding: 'Computing concept direction...',
          directionFound: 'Concept direction found',
          varianceLabel: 'Explained variance',
          dimLabel: 'Dimensions',
          projectionsTitle: 'Contrast pair projections',
          testTitle: 'Test + Manipulation',
          testSubtitle: 'Enter a sentence and steer generation along the concept direction',
          testPromptLabel: 'Test prompt',
          testPromptPlaceholder: 'e.g. The capital of Germany is',
          alphaLabel: 'Manipulation strength (\u03b1)',
          alphaHint: 'Strength of the steering vector. 0 = no effect, higher = stronger influence of contrast pairs.',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          seedLabel: 'Seed (-1 = random)',
          generateBtn: 'Generate with manipulation',
          generating: 'Running manipulated generation...',
          baselineLabel: 'Baseline (no manipulation)',
          manipulatedLabel: 'Manipulated (\u03b1 = {alpha})',
          projectionLabel: 'Projection onto concept direction',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        compare: {
          title: 'Comparative Model Archaeology',
          subtitle: 'Load two models and systematically compare their internal representations',
          explainWhatTitle: 'What does this experiment show?',
          explainWhatText: 'Based on Belinkov (2022) \u201cProbing Classifiers\u201d and Olsson et al. (2022) \u201cIn-Context Learning and Induction Heads\u201d. The heatmap shows CKA (Centered Kernel Alignment) between layers of both models. High similarity means these layers represent information in a similar way. Early layers (syntax) are often similar \u2014 late layers (semantics) diverge more strongly. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: 'How do I use it?',
          explainHowText: 'Model A is the active preset. Choose a second model (Model B) \u2014 via preset or custom HuggingFace ID \u2014 and load it. Enter text and click \u201cCompare\u201d. The CKA heatmap shows which layers represent information similarly. The generations from both models with the same seed show how differently they complete the same prompt.',
          referencesTitle: 'Research References',
          modelATitle: 'Model A (from preset selector)',
          modelAHint: 'Change via the preset dropdown above',
          modelBTitle: 'Model B (second model)',
          modelBPresetLabel: 'Preset',
          modelBCustomLabel: 'HuggingFace Model ID',
          modelBCustomPlaceholder: 'e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Load Model B',
          modelBLoaded: 'Model B loaded',
          modelBNone: 'Model B not loaded',
          promptLabel: 'Prompt',
          promptPlaceholder: 'e.g. The cat sat on the mat and watched the birds',
          seedLabel: 'Seed',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          compareBtn: 'Compare',
          comparing: 'Comparing models...',
          heatmapTitle: 'Layer Alignment (CKA)',
          heatmapAxisA: 'Model A \u2014 Layers',
          heatmapAxisB: 'Model B \u2014 Layers',
          heatmapExplain: 'Bright cells = high representation similarity. Diagonal patterns show the models process information in a similar order.',
          attentionTitle: 'Attention Comparison (last layer)',
          modelALabel: 'Model A',
          modelBLabel: 'Model B',
          generationTitle: 'Generation Comparison (same seed)',
          layerStatsTitle: 'Layer Statistics',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        bias: {
          title: 'Bias Archaeology',
          subtitle: 'Systematic bias experiments through controlled token manipulation',
          explainWhatTitle: 'What does this experiment show?',
          explainWhatText: 'Based on Zou et al. (2023) \u201cRepresentation Engineering\u201d and Bricken et al. (2023) \u201cTowards Monosemanticity\u201d. Instead of free manipulation, this tool investigates systematic biases: What happens when all masculine pronouns are suppressed? Which gender does the model choose as default? \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: 'How do I use it?',
          explainHowText: 'Choose an experiment type (gender, sentiment, domain, or custom tokens). Enter a prompt that invites the model to continue (e.g. \u201cThe doctor said to the patient\u201d). The results show baseline vs. manipulated generations \u2014 the differences reveal which biases are encoded in the model.',
          referencesTitle: 'Research References',
          presetLabel: 'Experiment type',
          presetGender: 'Gender \u2014 Suppress gendered pronouns',
          presetSentiment: 'Sentiment \u2014 Boost positive/negative',
          presetDomain: 'Domain \u2014 Boost scientific/poetic',
          presetCustom: 'Custom experiment',
          promptLabel: 'Prompt',
          promptPlaceholder: 'e.g. The doctor said to the patient',
          customBoostLabel: 'Boost tokens (comma-separated)',
          customBoostPlaceholder: 'e.g. dark,shadow,night',
          customSuppressLabel: 'Suppress tokens (comma-separated)',
          customSuppressPlaceholder: 'e.g. light,sun,bright',
          numSamplesLabel: 'Samples per condition',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          seedLabel: 'Base seed',
          runBtn: 'Run experiment',
          running: 'Running bias experiment...',
          baselineTitle: 'Baseline (no manipulation)',
          groupTitle: 'Group: {name}',
          modeSuppress: 'suppressed',
          modeBoost: 'boosted',
          tokensLabel: 'Tokens',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Suppresses all gendered pronouns and observes which defaults the model chooses.',
          sentimentDesc: 'Boosts positive or negative words and measures how strongly the entire text flow is affected.',
          domainDesc: 'Boosts scientific or poetic vocabulary and observes register shifts.',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        error: {
          gpuUnreachable: 'GPU service unreachable. Is it running?',
          loadFailed: 'Failed to load model.',
          operationFailed: 'Operation failed.'
        }
      },
      crossmodal: {
        headerTitle: 'Crossmodal Lab',
        headerSubtitle: 'Sound from latent spaces: T5 embedding manipulation, image-guided audio generation, crossmodal transfer',
        explanationToggle: 'Show detailed explanation',
        generate: 'Generate',
        generating: 'Generating...',
        result: 'Result',
        seed: 'Seed',
        generationTime: 'Generation time',
        tabs: {
          synth: {
            label: 'Latent Audio Synth',
            short: 'T5 embedding manipulation',
            title: 'Latent Audio Synth',
            description: 'Direct manipulation of Stable Audio\'s T5 conditioning space (768d). Interpolate between prompts, extrapolate beyond the prompt, scale embeddings and inject noise. Ultra-short loops, near real-time.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Image/text to audio (CVPR 2025)',
            title: 'MMAudio — Video/Image to Audio',
            description: 'Image and text enter the same network as separate signals — the image is not translated into language, both guide sound generation simultaneously. The model was jointly trained on video and audio, learning direct associations between what is seen and what is heard. Up to 8s, 44.1kHz, ~1.2s compute time. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'ImageBind Guidance',
            short: 'Gradient-based image guidance',
            title: 'ImageBind Gradient Guidance',
            description: 'Gradient-based steering during the Stable Audio denoising process. ImageBind provides a shared 1024d space for image and audio — the gradient of the cosine similarity guides audio generation towards the image embedding.'
          }
        },
        synth: {
          explainWhatTitle: 'What does the Latent Audio Synth do?',
          explainWhatText: 'Stable Audio generates sound from text. The text is converted by a T5 encoder into a numerical vector with 768 dimensions \u2014 this vector is what you manipulate here. Instead of just changing \u201cwhat\u201d the model generates (via the prompt), you change \u201chow\u201d the model internally understands the text. Two prompts that sound similar can be far apart in this space \u2014 and vice versa.',
          explainHowTitle: 'How do I use this tool?',
          explainHowText: 'Enter text in Prompt A \u2014 this determines the base sound. Optional: Prompt B as a target point. The Alpha slider controls the mix: at 0 you hear only A, at 1 only B, at 0.5 a blend. Values above 1 extrapolate beyond B (the sound becomes more extreme), values below 0 go in the opposite direction. Magnitude scales the entire embedding \u2014 higher values produce more intense sounds. Noise injects randomness and creates unpredictable variations. The Spectral Strip (below the Generate button) shows all 768 dimensions as bars. You can shift individual dimensions by clicking and dragging, directly manipulating the sound. Right-click resets a dimension.',
          promptA: 'Prompt A (Base)',
          promptAPlaceholder: 'e.g. ocean waves',
          promptB: 'Prompt B (Optional, for interpolation)',
          promptBPlaceholder: 'e.g. piano melody',
          alpha: 'Alpha (Interpolation)',
          alphaHint: '0 = A only, 1 = B only, between = blend, >1 or <0 = extrapolation',
          magnitude: 'Magnitude (Scaling)',
          magnitudeHint: 'Global embedding scaling (1.0 = unchanged)',
          noise: 'Noise',
          noiseHint: 'Gaussian noise on embedding (0 = no noise)',
          duration: 'Duration (s)',
          steps: 'Steps',
          cfg: 'CFG',
          durationHint: 'Length of the generated audio clip in seconds',
          stepsHint: 'Denoising steps. More = higher quality.',
          cfgHint: 'Classifier-Free Guidance for audio generation',
          seedHint: '-1 = random, fixed value = reproducible result',
          loop: 'Loop playback',
          loopOn: 'Loop On',
          loopOff: 'Loop Off',
          stop: 'Stop',
          looping: 'Looping',
          playing: 'Playing',
          stopped: 'Stopped',
          transpose: 'Transpose (semitones)',
          midiSection: 'MIDI Control',
          midiUnsupported: 'Web MIDI is not supported by this browser.',
          midiInput: 'MIDI Input',
          midiNone: '(none)',
          midiMappings: 'CC Mappings',
          midiNoteC3: 'Note (C3 = Ref)',
          midiGenerate: 'Generate + Transpose',
          midiPitch: 'Pitch rel. C3',
          loopInterval: 'Loop interval',
          loopOptimize: 'Auto-optimize',
          loopPingPong: 'Ping-pong',
          loopIntervalHint: 'Start/end of loop region — shorten end to trim Stable Audio fade-out',
          modeLoop: 'Loop',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Wavetable',
          modeRate: 'Tempo (fast)',
          modePitch: 'Pitch (OLA)',
          wavetableScan: 'Scan Position',
          wavetableScanHint: 'Morph between frames (0 = start, 1 = end)',
          wavetableFrames: '{count} frames',
          midiScan: 'Scan Position',
          adsrTitle: 'ADSR Envelope',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Envelope for MIDI notes (Attack/Decay/Sustain/Release)',
          play: 'Play',
          normalize: 'Normalize loudness',
          peak: 'Peak',
          crossfade: 'Crossfade',
          transposeHint: 'Shifts pitch in semitones',
          crossfadeHint: 'Crossfade time at loop boundary (ms)',
          normalizeHint: 'Normalizes volume to maximum amplitude',
          saveRaw: 'Save raw',
          saveLoop: 'Save loop',
          embeddingStats: 'Embedding statistics',
          dimensions: {
            section: 'Dimension Explorer',
            hint: 'Drag on bars = set offset. Paint horizontally = multiple dimensions.',
            resetAll: 'Reset all',
            hoverActivation: 'Activation',
            hoverOffset: 'Offset',
            rightClickReset: 'Right-click = reset',
            sortDiff: 'Sorted by prompt difference',
            sortMagnitude: 'Sorted by activation',
            activeOffsets: '{count} offsets active',
            applyAndGenerate: 'Apply and regenerate',
            undo: 'Undo',
            redo: 'Redo'
          }
        },
        mmaudio: {
          explainWhatTitle: 'What does MMAudio do?',
          explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) was jointly trained on video and audio. It doesn\'t translate an image into text and then into sound, but processes image and text as parallel signals in the same network. The model learned which sounds belong to which visual scenes \u2014 a forest produces birdsong, a street produces traffic noise, a guitar produces plucking sounds.',
          explainHowTitle: 'How do I use this tool?',
          explainHowText: 'Upload an image and/or enter a text prompt \u2014 using both together yields the richest results. The image alone generates sounds matching the visual content. The text prompt can additionally steer the sound or be used alone without an image. In the Negative Prompt, describe sounds you do NOT want to hear (e.g. \u201cspeech, music\u201d). Duration sets the length (1\u20138 seconds). CFG Strength controls how strictly the model follows the prompt \u2014 low values (2\u20133) produce more varied results, higher values (6\u20138) are more prompt-faithful.',
          imageUpload: 'Upload image (optional)',
          prompt: 'Text prompt (optional)',
          promptPlaceholder: 'e.g. crackling campfire',
          negativePrompt: 'Negative prompt',
          duration: 'Duration (s)',
          maxDuration: 'Max 8s (model limit)',
          cfg: 'CFG',
          steps: 'Steps',
          compareHint: 'Compare: Text only vs. Image + Text'
        },
        guidance: {
          explainWhatTitle: 'What does ImageBind Guidance do?',
          explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) brings six senses \u2014 image, sound, text, depth, heat, motion \u2014 into a shared \u201clanguage\u201d. This tool uses that common ground: as the sound is generated step by step, it constantly asks \u201cDoes this sound like the image yet?\u201d and corrects the direction. The cosine similarity in the result shows how close the generated sound came to the image content.',
          explainHowTitle: 'How do I use this tool?',
          explainHowText: 'Upload an image \u2014 this is the target direction for the sound. Optional: a text prompt for additional steering. The \u201c\u03bb Guidance Strength\u201d slider is the most important parameter: low values (0.01\u20130.05) give the sound a lot of freedom, high values (0.3\u20131.0) tie it closely to the image. \u201cWarmup Steps\u201d determines from which step the image guidance kicks in \u2014 low values start immediately, higher values let the basic structure emerge unguided first. Total Steps and Duration control quality and length.',
          referencesTitle: 'Research References',
          imageUpload: 'Upload image',
          prompt: 'Base prompt (optional)',
          promptPlaceholder: 'e.g. ambient soundscape',
          lambda: 'Guidance strength',
          lambdaHint: 'How strongly the image steers audio generation',
          warmupSteps: 'Warmup steps',
          warmupHint: 'Gradient guidance only during first N steps',
          totalSteps: 'Total steps',
          duration: 'Duration (s)',
          cfg: 'CFG',
          totalStepsHint: 'Total denoising steps. More = higher quality.',
          durationHint: 'Length of the generated audio clip in seconds',
          cosineSimilarity: 'Cosine similarity (image-audio proximity)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Did you know?',
        learnMore: '📚 Learn more',
        currentlyHappening: '⚡ Currently happening:',
        energyUsed: 'Energy used',
        co2Produced: 'CO₂ produced'
      },
      energy: {
        kids_1: '💡 AI images need electricity – as much as charging your phone for 3 hours!',
        kids_2: '🔌 The GPU is like a super calculator that eats lots of power!',
        kids_3: '⚡ Each image needs as much energy as running an LED light for 10 minutes!',
        youth_1: '⚡ A GPU uses {watts}W while generating – like a small space heater!',
        youth_2: '🔋 One image uses about 0.01-0.02 kWh – sounds little, but adds up!',
        youth_3: '🌡️ The GPU is getting {temp}°C hot right now – that\'s why it needs cooling!',
        expert_1: '📊 Realtime: {watts}W at {util}% utilization = {kwh} kWh so far',
        expert_2: '🔥 TDP limit: {tdp}W | Current: {watts}W ({percent}% of limit)',
        expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – model + activations'
      },
      data: {
        kids_1: '🧮 The GPU is calculating 10 billion times right now – faster than you can count!',
        kids_2: '🎨 The image is created in 50 small steps – like a puzzle solving itself!',
        kids_3: '🧩 Millions of numbers are flying through the GPU right now!',
        youth_1: '🔄 Each image goes through ~50 "denoising steps" – 50 rounds of removing noise!',
        youth_2: '📐 8 billion parameters are being queried – per image!',
        youth_3: '🧠 The AI "thinks" in vectors with thousands of dimensions – like coordinates in a space.',
        expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – text + image in joint attention layers',
        expert_2: '📈 Self-Attention: O(n²) complexity – every token "sees" all others',
        expert_3: '⚙️ Classifier-Free Guidance: prompt influence vs. creativity balance'
      },
      model: {
        kids_1: '🎓 The AI model looked at millions of images to learn how to paint!',
        kids_2: '🤖 The AI is like an artist who never forgets what they\'ve seen!',
        kids_3: '✨ 8 billion connections in the model – more than stars you can see in the sky!',
        youth_1: '🧠 SD3.5 Large has 8 billion parameters – like 8 billion decision nodes.',
        youth_2: '📚 3 text encoders work together: CLIP-L, CLIP-G, and T5-XXL',
        youth_3: '🔢 The model needs {vram} GB VRAM just to be loaded!',
        expert_1: '🏗️ Architecture: Rectified Flow + MMDiT with 38 transformer blocks',
        expert_2: '📊 FP16/FP8 quantization: precision vs. VRAM trade-off',
        expert_3: '🔗 LoRA: Low-Rank Adaptation – only 0.1% of parameters retrained'
      },
      ethics: {
        kids_1: '🌍 AI learns from images on the internet – that\'s why it\'s important to be fair with other people\'s art!',
        kids_2: '⚖️ Not all artists were asked if the AI could learn from them.',
        kids_3: '🤝 Good AI respects people\'s work!',
        youth_1: '📜 Training data often comes from the internet. Artists debate: Fair Use or copying?',
        youth_2: '🏛️ The EU AI Act demands transparency: Where does the training data come from?',
        youth_3: '💭 Question: Who actually owns an AI-generated image?',
        expert_1: '⚠️ LAION-5B was partly created without creator consent – legal gray area.',
        expert_2: '📋 EU AI Act Art. 52: Labeling requirement for AI-generated content',
        expert_3: '🔍 Model Cards & Datasheets: Best practice for ML transparency'
      },
      environment: {
        kids_1: '☁️ Each AI image produces a bit of CO₂ – like driving a car, but less!',
        kids_2: '🌱 Think: Is this image worth the electricity?',
        kids_3: '🌞 Energy for AI often comes from power plants – some clean, some not.',
        youth_1: '🏭 German power grid: ~400g CO₂ per kWh – that adds up!',
        youth_2: '📈 {co2}g CO₂ for this image – with 1000 images that would be {totalKg} kg!',
        youth_3: '💡 Tip: Generate fewer images, but more thoughtfully – saves energy and CO₂.',
        expert_1: '📊 Calculation: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Scope 2 emissions: data center location is decisive',
        expert_3: '⚡ PUE (Power Usage Effectiveness): Additional energy overhead for cooling'
      },
      iceberg: {
        drawPrompt: 'AI generation uses a lot of energy. Draw icebergs and see what happens...',
        redraw: 'Redraw',
        startMelting: 'Start melting',
        melting: 'Iceberg melting...',
        melted: 'Melted!',
        meltedMessage: '{co2}g CO₂ produced',
        comparison: 'This CO₂ amount melts about {volume} cm³ of Arctic ice.',
        comparisonInfo: '(Each ton of CO₂ = approx. 6m³ sea ice loss)',
        gpuPower: 'Graphics card power consumption',
        gpuTemp: 'Graphics card temperature',
        co2Info: 'CO₂ emissions from power consumption (based on German energy mix)',
        drawAgain: 'Draw more icebergs...'
      },
      pixel: {
        grafikkarte: 'Graphics Card',
        energieverbrauch: 'Energy Usage',
        co2Menge: 'CO₂ Amount',
        smartphoneComparison: 'You would need to keep your phone off for {minutes} minutes to offset this CO₂ usage!',
        clickToProcess: 'Click on the data pixels to generate a mini image!'
      },
      forest: {
        trees: 'Trees',
        clickToPlant: 'Click to plant trees! Where you plant a tree, the factory will disappear.',
        gameOver: 'The forest is lost!',
        treesPlanted: 'You planted {count} trees.',
        complete: 'Generation complete',
        comparison: 'An average tree needs {minutes} minutes to absorb this amount of CO₂.'
      },
      rareearth: {
        clickToClean: 'Click the lake to remove toxic sludge!',
        sludgeRemoved: 'Sludge removed',
        environmentHealth: 'Environment',
        gameOverInactive: 'You gave up... mining continues',
        infoBanner: 'Rare earth mining for GPU chips leaves toxic sludge and destroys ecosystems. Your cleanup efforts cannot match the speed of extraction.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Environment',
        statsSludge: 'Sludge removed'
      }
    }
  },
  tr: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Yaratıcı Yapay Zeka Dönüşümleri'
    },
    form: {
      inputLabel: 'Metniniz',
      inputPlaceholder: 'örn. Çayırdaki bir çiçek',
      schemaLabel: 'Dönüşüm Stili',
      executeModeLabel: 'Çalışma Modu',
      safetyLabel: 'Güvenlik Seviyesi',
      generateButton: 'Oluştur'
    },
    schemas: {
      dada: 'Dada (Rastgele & Absürt)',
      bauhaus: 'Bauhaus (Geometrik)',
      stillepost: 'Stille Post (Yinelemeli)'
    },
    executionModes: {
      eco: 'Eko (Hızlı)',
      fast: 'Hızlı (Dengeli)',
      best: 'En İyi (Kalite)'
    },
    safetyLevels: {
      kids: 'Çocuklar',
      youth: 'Gençler',
      adult: 'Yetişkinler',
      research: 'Araştırma'
    },
    stages: {
      pipeline_starting: 'Pipeline Başlatılıyor',
      translation_and_safety: 'Çeviri & Güvenlik',
      interception: 'Dönüşüm',
      pre_output_safety: 'Çıktı Güvenliği',
      media_generation: 'Görsel Oluşturma',
      completed: 'Tamamlandı'
    },
    status: {
      idle: 'Hazır',
      executing: 'Pipeline çalışıyor...',
      connectionSlow: 'Bağlantı yavaş, yeniden deneniyor...',
      completed: 'Pipeline tamamlandı!',
      error: 'Hata oluştu'
    },
    entities: {
      input: 'Girdi',
      translation: 'Çeviri',
      safety: 'Güvenlik Kontrolü',
      interception: 'Dönüşüm',
      safety_pre_output: 'Çıktı Güvenliği',
      media: 'Oluşturulan Görsel'
    },
    properties: {
      chill: 'rahat',
      chaotic: 'çılgın',
      narrative: 'hikaye anlat',
      algorithmic: 'kurallara uy',
      historical: 'tarih',
      contemporary: 'güncel',
      explore: 'yapay zekayı dene',
      create: 'sanat yap',
      playful: 'eğlenceli',
      serious: 'ciddi'
    },
    phase2: {
      title: 'Prompt Girişi',
      userInput: 'Girdiniz',
      yourInput: 'Girdiniz',
      yourIdea: 'Fikriniz: Bu NE hakkında olmalı?',
      rules: 'Kurallarınız: Fikriniz NASIL hayata geçirilmeli?',
      yourInstructions: 'Talimatlarınız',
      what: 'NE',
      how: 'NASIL',
      userInputPlaceholder: 'örn. Çayırdaki bir çiçek',
      inputPlaceholder: 'Metniniz burada görünecek...',
      metaPrompt: 'Sanatsal Talimat',
      instruction: 'Talimat',
      transformation: 'Sanatsal Dönüşüm',
      metaPromptPlaceholder: 'Dönüşümü açıklayın...',
      result: 'Sonuç',
      expectedResult: 'Beklenen Sonuç',
      execute: 'Pipeline\'ı Çalıştır',
      executing: 'Çalışıyor...',
      transforming: 'LLM dönüştürüyor...',
      startTransformation: 'Dönüşümü Başlat',
      letsGo: 'Tamam, başlayalım!',
      modified: 'Değiştirildi',
      reset: 'Sıfırla',
      loadingConfig: 'Yapılandırma yükleniyor...',
      loadingMetaPrompt: 'Meta-prompt yükleniyor...',
      errorLoadingConfig: 'Yapılandırma yüklenirken hata oluştu',
      errorLoadingMetaPrompt: 'Meta-prompt yüklenirken hata oluştu',
      threeForces: 'Birlikte Çalışan 3 Kuvvet',
      twoForces: 'NE + NASIL → LLM → Sonuç',
      yourPrompt: 'Promptunuz:',
      writeYourText: 'Metninizi yazın...',
      examples: 'Örnekler',
      estimatedTime: '~12 saniye',
      stage12Time: '~5-10 saniye',
      willAppearAfterExecution: 'Çalıştırıldıktan sonra görünecek...',
      back: 'Geri',
      retry: 'Tekrar Dene',
      transformedPrompt: 'Dönüştürülmüş Prompt',
      notYetTransformed: 'Henüz dönüştürülmedi...',
      transform: 'Dönüştür',
      reTransform: 'Farklı şekilde tekrar dene',
      startAI: 'Yapay zeka, girdimi işle',
      aiWorking: 'Yapay zeka çalışıyor...',
      continueToMedia: 'Görsel Oluşturmaya Geç',
      readyForMedia: 'Görsel Oluşturma için Hazır',
      stage1: 'Aşama 1: Çeviri + Güvenlik...',
      stage2: 'Aşama 2: Dönüşüm...',
      selectMedia: 'Medyanızı seçin:',
      mediaImage: 'Görsel',
      mediaAudio: 'Ses',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Yakında',
      generateMedia: 'Başla!'
    },
    phase3: {
      generating: 'Görsel oluşturuluyor...',
      generatingHint: '~30 saniye'
    },
    common: {
      back: 'Geri',
      loading: 'Yükleniyor...',
      error: 'Hata',
      retry: 'Tekrar Dene',
      cancel: 'İptal',
      checkingSafety: 'Kontrol ediyor...'
    },
    gallery: {
      title: 'Favoriler',
      empty: 'Henüz favori yok',
      favorite: 'Favorilere ekle',
      unfavorite: 'Favorilerden kaldır',
      continue: 'Düzenlemeye devam et',
      restore: 'Oturumu geri yükle',
      viewMine: 'Favorilerim',
      viewAll: 'Tüm favoriler'
    },
    settings: {
      authRequired: 'Kimlik Doğrulama Gerekli',
      authPrompt: 'Ayarlara erişmek için lütfen şifreyi girin:',
      passwordPlaceholder: 'Şifre girin...',
      authenticate: 'Giriş Yap',
      authenticating: 'Kimlik doğrulanıyor...',
      // Admin page
      title: 'Yönetim',
      tabs: {
        export: 'Araştırma Verileri',
        config: 'Yapılandırma',
        demos: 'Minigame Demo',
        matrix: 'Model Matrisi'
      },
      loading: 'Ayarlar yükleniyor...',
      presets: {
        title: 'Model Ön Ayarları',
        help: 'Mevcut tüm ön ayarları görmek ve tek tıkla uygulamak için <strong>Model Matrisi</strong> sekmesini kullanın.',
        openMatrix: 'Model Matrisini Aç'
      },
      testingTools: {
        title: 'Eğitimciler için Test Araçları',
        help: 'Pedagojik mini oyunları ve animasyonları öğrencilerle kullanmadan önce test edin ve keşfedin.',
        openPreview: 'Mini Oyun Önizlemesini Aç',
        pixelEditor: 'Piksel Şablon Editörü',
        includes: 'İçerir: Piksel Animasyonu, Buzdağı Erimesi, Orman Oyunu, Nadir Toprak Elementleri'
      },
      general: {
        title: 'Genel Yapılandırma',
        uiMode: 'Arayüz Modu',
        uiModeHelp: 'Arayüz karmaşıklık seviyesi',
        kids: 'Çocuklar (8–12)',
        youth: 'Gençler (13–17)',
        expert: 'Uzman',
        safetyLevel: 'Güvenlik Seviyesi',
        defaultLanguage: 'Varsayılan Dil',
        germanDe: 'Almanca (de)',
        englishEn: 'İngilizce (en)',
        turkishTr: 'Türkçe (tr)',
        koreanKo: '한국어 (ko)',
        ukrainianUk: 'Українська (uk)',
        frenchFr: 'Fransızca (fr)'
      },
      safety: {
        kidsTitle: 'Çocuklar (8–12)',
        kidsDesc: 'Tüm filtreler aktif: §86a, KVKK, Çocuk Koruma (yaşa uygun parametreler), VLM görsel kontrolü',
        youthTitle: 'Gençler (13–17)',
        youthDesc: 'Tüm filtreler aktif: §86a, KVKK, Gençlik Koruma (gençlik parametreleri), VLM görsel kontrolü',
        adultTitle: 'Yetişkinler',
        adultDesc: '§86a + KVKK aktif. Gençlik koruması yok, VLM görsel kontrolü yok.',
        researchTitle: 'Araştırma Modu',
        researchDesc: 'HİÇBİR güvenlik filtresi aktif değil. Yalnızca bilimsel araştırma projeleri kapsamında araştırma kurumları tarafından kullanılabilir.'
      },
      safetyModels: {
        title: 'Yerel Güvenlik Modelleri',
        help: 'Ollama ile yerel — kişi adları ve güvenlik kontrolleri sistemi asla terk etmez',
        safetyModel: 'Güvenlik Modeli',
        safetyModelHelp: 'İçerik güvenliği için koruma modeli (§86a, gençlik koruma)',
        dsgvoModel: 'KVKK Doğrulama Modeli',
        dsgvoModelHelp: 'KVKK NER doğrulaması için genel amaçlı model (koruma modeli değil)',
        vlmModel: 'VLM Güvenlik Modeli',
        vlmModelHelp: 'Üretim sonrası görüntü güvenlik kontrolü için görsel model (çocuklar/gençler)',
        fast: 'hızlı, minimal',
        recommended: 'önerilen'
      },
      dsgvo: {
        title: 'KVKK Uyarısı',
        notCompliant: 'Aşağıdaki modeller <strong>KVKK uyumlu DEĞİLDİR</strong> (veriler AB dışında işlenir):',
        compliantHint: 'KVKK uyumlu seçenekler:'
      },
      models: {
        title: 'Model Yapılandırması',
        help: 'Sağlayıcı önekiyle model tanımlayıcıları: local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: 'Model Matrisi kullanılması önerilir. Ancak ayarlarınızı burada serbestçe yapılandırabilirsiniz.',
        ollamaAvailable: '{count} Ollama modeli mevcut (yazın veya açılır menüden seçin)',
        stage1Text: 'Aşama 1 – Metin Modeli',
        stage1Vision: 'Aşama 1 – Görüntü Modeli',
        stage2Interception: 'Aşama 2 – Interception Modeli',
        stage2Optimization: 'Aşama 2 – Optimizasyon Modeli',
        stage3: 'Aşama 3 – Çeviri/Güvenlik Modeli',
        stage4Legacy: 'Aşama 4 – Eski Model',
        chatHelper: 'Sohbet Yardımcı Modeli',
        imageAnalysis: 'Görsel Analiz Modeli',
        coding: 'Kod Üretimi (Tone.js, p5.js)'
      },
      api: {
        title: 'API Yapılandırması',
        llmProvider: 'LLM Sağlayıcısı',
        localFramework: 'Yerel LLM çerçevesi',
        externalProvider: 'Harici LLM Sağlayıcısı',
        cloudProvider: 'Bulut LLM sağlayıcısı – API anahtarı gerekli',
        noneLocal: 'Yok (Yalnızca yerel, KVKK)',
        mistralEu: 'Mistral AI (AB merkezli, KVKK)',
        anthropicDirect: 'Anthropic Direct API (KVKK DEĞİL)',
        openaiDirect: 'OpenAI Direct API (KVKK DEĞİL)',
        openrouterDirect: 'OpenRouter (KVKK DEĞİL, AB yönlendirme mevcut)',
        mistralInfo: 'Mistral AI (AB merkezli)',
        mistralDsgvo: 'KVKK uyumlu (AB altyapısı)',
        anthropicInfo: 'Anthropic Direct API',
        anthropicNotDsgvo: 'KVKK uyumlu DEĞİL',
        anthropicWarning: 'Veriler AB dışında işlenir. Yalnızca eğitim dışı bağlamlarda kullanın.',
        openaiInfo: 'OpenAI Direct API',
        openaiNotDsgvo: 'KVKK uyumlu DEĞİL (ABD merkezli)',
        openaiWarning: 'Veriler ABD\'de işlenir. Yalnızca eğitim dışı bağlamlarda kullanın.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: 'KVKK uyumlu DEĞİL (ABD şirketi)',
        openrouterWarning: 'AB sunucu yönlendirmesi OpenRouter ayarlarından yapılandırılabilir, ancak şirket ABD merkezlidir.',
        storedIn: 'Kayıt yeri',
        currentKey: 'Mevcut'
      },
      save: {
        saveApply: 'Kaydet ve Uygula',
        saving: 'Kaydediliyor...',
        applying: 'Uygulanıyor...',
        success: 'Ayarlar kaydedildi ve uygulandı',
        presetApplied: 'Ön ayar uygulandı: {preset}'
      }
    },
    pipeline: {
      yourInput: 'Girdiniz',
      result: 'Sonuç',
      generatedMedia: 'Oluşturulan görsel'
    },
    landing: {
      subtitlePrefix: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü\'nün pedagojik-sanatsal deney platformu',
      subtitleSuffix: 'kültürel-estetik medya eğitiminde üretken yapay zekanın keşifsel kullanımı için',
      research: '',
      features: {
        textTransformation: {
          title: 'Metin Dönüşümü',
          description: 'Yapay zeka aracılığıyla bakış açısı değişimi — promptunuz sanatsal-pedagojik lensler aracılığıyla görsel, video ve sese dönüştürülür.'
        },
        imageTransformation: {
          title: 'Görsel Dönüşümü',
          description: 'Görselleri farklı modeller ve bakış açıları aracılığıyla yeni görsellere ve videolara dönüştürün.'
        },
        multiImage: {
          title: 'Görsel Füzyonu',
          description: 'Birden fazla görseli birleştirin ve yapay zeka modelleri aracılığıyla yeni görsel kompozisyonlarla birleştirin.'
        },
        canvas: {
          title: 'Kanvas İş Akışı',
          description: 'Görsel iş akışı kompozisyonu — modülleri sürükle & bırak ile özel yapay zeka pipeline\'larına bağlayın.'
        },
        music: {
          title: 'Müzik Oluşturma',
          description: 'Sözler, etiketler ve stilistik kontrol ile yapay zeka destekli müzik yaratımı.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vektör uzayı araştırması — gerçeküstüleştirme, boyut eleme, gömme interpolasyonu.'
        }
      }
    },
    research: {
      locked: 'Yalnızca araştırma modunda kullanılabilir',
      lockedHint: '"Yetişkin" veya "Araştırma" güvenlik seviyesi gerektirir (config.py)',
      complianceTitle: 'Araştırma Modu Bildirimi',
      complianceWarning: 'Araştırma modunda, promptlar veya oluşturulan görseller için hiçbir güvenlik filtresi etkin değildir. Beklenmedik veya uygunsuz sonuçlar oluşabilir.',
      complianceAge: 'Bu mod, 16 yaşın altındaki kişilere tavsiye edilmez.',
      complianceConfirm: 'Bildirimleri anladığımı onaylıyorum',
      complianceCancel: 'İptal',
      complianceProceed: 'Devam Et'
    },
    presetOverlay: {
      title: 'Bakış Açısı Seçin',
      close: 'Kapat'
    },
    imageUpload: {
      clickHere: 'Buraya tıklayın',
      orDragImage: 'veya bir görsel sürükleyin',
      formatHint: 'PNG, JPG, WEBP (maks. 10MB)',
      invalidFormat: 'Geçersiz dosya formatı. Yalnızca PNG, JPG ve WEBP kabul edilir.',
      fileTooLarge: 'Dosya çok büyük. Maksimum: {max}MB',
      uploadFailed: 'Yükleme başarısız',
      infoOriginal: 'Orijinal:',
      infoSize: 'Boyut:'
    },
    mediaInput: {
      choosePreset: 'Bakış Açısı Seçin',
      translateToEnglish: 'İngilizceye çevir',
      copy: 'Kopyala',
      paste: 'Yapıştır',
      delete: 'Sil',
      loading: 'Yükleniyor...',
      contentBlocked: 'İçerik engellendi'
    },
    nav: {
      about: 'Hakkında',
      impressum: 'Künye',
      privacy: 'Gizlilik',
      docs: 'Dokümantasyon',
      language: 'Dil değiştir',
      settings: 'Ayarlar',
      canvas: 'Kanvas İş Akışı'
    },
    canvas: {
      title: 'Kanvas İş Akışı',
      newWorkflow: 'Yeni İş Akışı',
      importWorkflow: 'İçe Aktar',
      exportWorkflow: 'Dışa Aktar',
      execute: 'Çalıştır',
      ready: 'Hazır',
      errors: 'hatalar',
      discardWorkflow: 'Mevcut iş akışı iptal edilsin mi?',
      importError: 'Dosya içe aktarılamadı',
      selectTransformation: 'Dönüşüm Seçin',
      selectOutput: 'Çıktı Modeli Seçin',
      search: 'Ara...',
      noResults: 'Sonuç bulunamadı',
      dragHint: 'Modülleri kanvasa tıklayın veya sürükleyin',
      editNameHint: '(düzenlemek için çift tıklayın)',
      modules: 'Modüller',
      toggleSidebar: 'Kenar çubuğunu aç/kapat',
      dsgvoTooltip: 'Kanvas iş akışları harici LLM API\'larını kullanabilir. KVKK uyumluluğu kullanıcının sorumluluğundadır.',
      batchExecute: 'Toplu Çalıştırma',
      batchExecution: 'Toplu Çalıştırma',
      batchAbort: 'Toplu işlemi iptal et',
      abort: 'İptal',
      cancel: 'İptal',
      loading: 'Yükleniyor...',
      executingWorkflow: 'İş akışı çalıştırılıyor...',
      starting: 'Başlatılıyor...',
      nodes: 'düğüm',
      batchRunCount: 'Çalıştırma Sayısı',
      batchUseSeed: 'Temel Seed Kullan',
      batchBaseSeed: 'Temel Seed',
      batchSeedHint: 'Her çalıştırma: seed + index',
      batchStart: 'Toplu Başlat',
      stage: {
        configSelectPlaceholder: 'Seçin...',
        evaluationCriteriaFallback: 'Değerlendirme kriterleri...',
        feedbackInputTitle: 'Geri Bildirim Girişi',
        deleteTitle: 'Sil',
        selectLlmPlaceholder: 'LLM Seçin...',
        resizeTitle: 'Boyutlandır',
        input: {
          promptPlaceholder: 'Promptunuz...'
        },
        imageInput: {
          uploadLabel: 'Görsel Yükle'
        },
        interception: {
          contextPromptLabel: 'Bağlam Promptu',
          contextPromptPlaceholder: 'Dönüşüm talimatları...'
        },
        translation: {
          translationPromptLabel: 'Çeviri Promptu',
          translationPromptPlaceholder: 'Çeviri talimatları...'
        },
        modelAdaption: {
          targetModelLabel: 'Hedef Model',
          noAdaptionOption: 'Adaptasyon Yok',
          videoModelsOption: 'Video Modelleri',
          audioModelsOption: 'Ses Modelleri'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Karşılaştırma Kriterleri',
          criteriaPlaceholder: 'örn. Orijinallik, netlik, ayrıntıya göre karşılaştırın...',
          infoText: 'En fazla 3 metin çıkışı bağlayın'
        },
        seed: {
          modeLabel: 'Mod',
          modeFixed: 'Sabit',
          modeRandom: 'Rastgele',
          valueLabel: 'Değer',
          baseLabel: 'Temel'
        },
        resolution: {
          customOption: 'Özel',
          widthLabel: 'Genişlik',
          heightLabel: 'Yükseklik'
        },
        collector: {
          emptyText: 'Çalıştırma bekleniyor...'
        },
        evaluation: {
          typeLabel: 'Değerlendirme Türü',
          typeCreativity: 'Yaratıcılık',
          typeQuality: 'Kalite',
          typeCustom: 'Özel',
          criteriaLabel: 'Değerlendirme Kriterleri',
          outputTypeLabel: 'Çıkış Türü',
          outputCommentary: 'Yorum + Binary',
          outputScore: 'Yorum + Skor + Binary',
          outputAll: 'Tümü',
          evalPassTitle: 'Geçti (ileri)',
          evalFailTitle: 'Geri Bildirim (geri kanal)',
          evalCommentaryTitle: 'Yorum (ileri)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Görüntü Modeli Seçin...',
          frameworkLabel: 'Analiz Çerçevesi',
          frameworkPanofsky: 'Sanat Tarihsel (Panofsky)',
          frameworkEducational: 'Eğitim Teorisi',
          frameworkEthical: 'Etik',
          frameworkCritical: 'Eleştirel/Dekolonyal',
          frameworkCustom: 'Özel Talimat',
          customPromptLabel: 'Analiz Promptu',
          customPromptPlaceholder: 'Görselin nasıl analiz edilmesi gerektiğini açıklayın...'
        },
        display: {
          imageAlt: 'Önizleme',
          emptyText: 'Önizleme (çalıştırmadan sonra)'
        }
      }
    },
    about: {
      title: 'UCDCAE AI LAB Hakkında',
      intro: 'UCDCAE AI LAB, UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü\'nün kültürel-estetik medya eğitiminde üretken yapay zekanın keşifsel kullanımına yönelik pedagojik-sanatsal bir deney platformudur. AI4ArtsEd ve COMeARTS projeleri kapsamında geliştirilmiştir.',
      project: {
        title: 'Proje',
        description: 'Yapay zeka toplumu ve iş dünyasını dönüştürmektedir; giderek eğitimin bir konusu haline gelmektedir. Proje, kültürel çeşitliliğe duyarlı kültürel eğitim ortamlarında yapay zekanın (YZ) pedagojik kullanımının fırsatlarını, koşullarını ve sınırlarını araştırmaktadır.',
        paragraph2: 'Üç alt proje — Genel Pedagoji (TPap), Bilgisayar Bilimi (TPinf) ve Sanat Eğitimi (TPkp) — yaratıcılık odaklı pedagojik YZ pratik araştırması ile bilgisayar bilimi YZ kavramlaştırması ve programlamanın yakın iş birliği içinde kesiştiği bir yapı oluşturmaktadır. Proje başından itibaren tasarım sürecine sanatsal-pedagojik uygulayıcıları sistematik olarak dahil etmektedir; bir yanda mesleki (kalite, estetik, etik ve değer odaklı) pedagojik-pratik uygulama ile diğer yanda bilgisayar bilimi alt projesinin uygulama ve eğitim süreci arasında köprü görevi görmektedir.',
        paragraph3: 'Yaklaşık iki yıl süren katılımcı bir tasarım süreci, elverişli gerçek dünya koşulları altında yapay zeka sistemlerinin yapısal düzeyde sanatsal-pedagojik ilkeleri ne ölçüde içerebileceğini araştıran açık kaynaklı bir yapay zeka teknolojisi üretmeyi amaçlamaktadır.',
        paragraph4: 'Odak noktaları: a) kültürel eğitim için son derece yenilikçi teknolojilerin gelecekteki uygulanabilirliği ve katma değeri, b) öğretmenler ve öğrenciler arasında YZ okuryazarlığının kapsamı ve sınırları ve c) pedagojik etik ve teknoloji değerlendirmesi açısından karmaşık insan dışı aktörler tarafından pedagojik ortamların dönüşümünün değerlendirilebilirliği ve değerlendirmesi üzerine kapsamlı soru.',
        moreInfo: 'Daha fazla bilgi:'
      },
      subproject: {
        title: '"Genel Pedagoji" Alt Projesi',
        description: '"Genel Pedagoji" alt projesi, ortak araştırma projesinin ortak araştırma sorusu çerçevesinde katılımcı pratik araştırmaya dayalı sanatsal-pedagojik bir YZ tasarım sürecinin olanaklarını ve sınırlarını araştırmaktadır. Bu amaçla ilk proje yılında bir dizi araştırma, analiz, uzman çalıştayları ve açık alanlar yürütmektedir. Birden fazla döngüde geri bildirim döngüsü olarak tasarlanan sonraki proje aşaması, özellikle biçimsel olmayan kültürel eğitimde pedagojik uygulayıcılar ve sanatçı-eğitimcilerle bir prototip kullanımını ilişkisel ve kolektif dönüştürücü bir eğitim süreci olarak araştırmaktadır.'
      },
      team: {
        title: 'Ekip',
        projectLead: 'Proje Lideri',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Eğitim Enstitüsü',
        leadChair: 'Kültür ve Estetik Eğitime Odaklanan Eğitim Kürsüsü',
        leadUnesco: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü',
        researcher: 'Araştırma Görevlisi',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Eğitim Enstitüsü',
        researcherChair: 'Kültür ve Estetik Eğitime Odaklanan Eğitim Kürsüsü',
        researcherUnesco: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü'
      },
      funding: {
        title: 'Destekleyen'
      }
    },
    legal: {
      impressum: {
        title: 'Künye',
        publisher: 'Yayıncı',
        represented: 'Rektör tarafından temsil edilmektedir',
        responsible: 'İçerikten sorumlu',
        authority: 'Denetleme Makamı',
        moreInfo: 'Ek Bilgi',
        moreInfoText: 'FAU\'nun tam künyesi:',
        funding: 'Destekleyen'
      },
      privacy: {
        title: 'Gizlilik Politikası',
        notice: 'Bildirim: Oluşturulan içerik araştırma amaçlı sunucuda saklanmaktadır. Kullanıcı veya IP verisi toplanmamaktadır. Yüklenen görseller saklanmamaktadır.',
        usage: 'Bu platformun kullanımı yalnızca UCDCAE AI LAB\'ın kayıtlı iş birliği ortakları için izin verilmektedir. Bu bağlamda yapılan veri koruma anlaşmaları geçerlidir. Sorularınız için lütfen vanessa.baumann@fau.de adresiyle iletişime geçin.'
      }
    },
    docs: {
      title: 'Dokümantasyon & Rehber',
      intro: {
        title: 'Hoş Geldiniz',
        content: 'Yapay zeka dönüşümleriyle yaratıcı deneyler.'
      },
      gettingStarted: {
        title: 'Başlarken',
        step1: 'Kadranlardan özellikler seçin',
        step2: 'Metin veya görsel girin',
        step3: 'Dönüşümü başlatın'
      },
      modes: {
        title: 'Modlar',
        mode1: { name: 'Doğrudan', desc: 'Hızlı deneyler' },
        mode2: { name: 'Metin', desc: 'Metin tabanlı dönüşümler' },
        mode3: { name: 'Görsel', desc: 'Görsel tabanlı işlemler' }
      },
      support: {
        title: 'Destek',
        content: 'Sorular için:'
      },
      wikipedia: {
        title: 'Wikipedia Araştırması',
        subtitle: 'Sanatsal süreçlerin bir parçası olarak dünya hakkında bilgi',
        feature: 'Sanatsal süreçler yalnızca estetik bilgi değil, dünya hakkında olgusal bilgi de gerektirir. Yapay zeka, dönüşüm sırasında olgusal bilgi bulmak için Wikipedia\'yı araştırır.',
        languages: '70\'ten fazla dil desteklenmektedir',
        languagesDesc: 'Yapay zeka her konu için uygun Wikipedia dilini otomatik olarak seçer:',
        examples: {
          nigeria: 'Nijerya hakkında konu → Hausa, Yoruba, Igbo veya İngilizce',
          india: 'Hindistan hakkında konu → Hintçe, Tamilce, Bengalce veya diğer bölgesel diller',
          indigenous: 'Yerli kültürler → Keçuva, Maori, İnuktitut, vb.'
        },
        why: 'Şeffaflık: Yapay zeka ne biliyor?',
        whyDesc: 'Sistem tüm araştırma girişimlerini gösterir: Hem bulunan makaleleri (tıklanabilir bağlantılar olarak) hem de hiçbir şey bulunamayan terimleri. Bu, yapay zekanın ne bildiğini düşündüğünü — ve ne bilmediğini — görünür kılar.',
        culturalRespect: 'Kendiniz araştırmaya davet',
        culturalRespectDesc: 'Gösterilen Wikipedia bağlantıları, daha fazlasını öğrenmek için bir davettir. Kaynakları kontrol etmek ve kendi bilginizi genişletmek için bağlantılara tıklayın.',
        limitations: 'Yapay zeka araştırması bir yardımcıdır, konuyla kendi ilgilenmenizin yerini alamaz.'
      }
    },
    multiImage: {
      image1Label: 'Görsel 1',
      image2Label: 'Görsel 2 (isteğe bağlı)',
      image3Label: 'Görsel 3 (isteğe bağlı)',
      contextLabel: 'Görsellerle ne yapmak istediğinizi açıklayın',
      contextPlaceholder: 'örn. Görsel 2\'deki evi ve Görsel 3\'teki atı Görsel 1\'e ekleyin. Görsel 1\'deki renkleri ve stili koruyun.',
      modeTitle: 'Birden Fazla Görsel → Görsel',
      selectConfig: 'Modelinizi seçin:',
      generating: 'Görseller birleştiriliyor...'
    },
    imageTransform: {
      imageLabel: 'Görseliniz',
      contextLabel: 'Görselde ne değiştirmek istediğinizi açıklayın',
      contextPlaceholder: 'örn. Yağlı boya tabloya dönüştür... Daha renkli yap... Gün batımı ekle...'
    },
    videoGeneration: {
      promptLabel: 'Video Fikriniz',
      promptPlaceholder: 'örn. Gün batımında dağ manzarası üzerinde süzülen bir sıcak hava balonu...',
      modelLabel: 'Bir video modeli seçin:',
      generating: 'Video oluşturuluyor...'
    },
    textTransform: {
      inputLabel: 'Fikriniz = NE?',
      inputTooltip: 'Kreasyonunuzun ne hakkında olacağını girin.',
      inputPlaceholder: 'örn. Sokağımda bir festival: ...',
      contextLabel: 'Kurallarınız = NASIL?',
      contextTooltip: 'Fikrinizin nasıl sunulacağını girin veya daire simgesine tıklayın!',
      contextPlaceholder: 'örn. Her şeyi ağaçlardaki kuşların algıladığı gibi tanımlayın!',
      resultLabel: 'Fikir + Kurallar = Prompt',
      resultPlaceholder: 'Prompt, başlat butonuna tıklandıktan sonra görünecek (veya kendi metninizi girin)',
      optimizedLabel: 'Model-Optimize Edilmiş Prompt',
      optimizedPlaceholder: 'Optimize edilmiş prompt, model seçiminden sonra görünecek.'
    },
    training: {
      info: {
        title: 'LoRA Eğitimi Hakkında',
        studioDescription: 'Stable Diffusion 3.5 Large için kendi görsellerinizle özel LoRA modelleri eğitin.',
        description: 'Bu yerleşik eğitim hızlı testler için tasarlanmıştır.',
        limitations: 'Sınırlamalar',
        limitationDuration: 'Eğitim 1-3 saat sürer',
        limitationBlocking: 'Eğitim sırasında görsel oluşturmayı engeller',
        limitationConfig: 'Sınırlı yapılandırma seçenekleri',
        showMore: 'Daha fazla bilgi',
        showLess: 'Daha az göster'
      },
      placeholders: {
        projectName: 'örn. Okul Binamız',
        triggerWords: 'örn. okul_binamiz, okul_bahcesi, sinif'
      },
      labels: {
        projectName: 'Proje Adı',
        triggerWords: 'Tetikleyici Kelimeler',
        triggerHelp: 'Virgülle ayrılmış etiketler. İlki = birincil tetikleyici, geri kalanları = görsel başına ek etiketler.',
        images: 'Eğitim Görselleri (10–50 önerilir)',
        dropZone: 'Görselleri buraya tıklayın veya bırakın',
        imagesSelected: '{count} görsel seçildi',
        logs: 'Eğitim Günlükleri',
        waiting: 'Eğitimin başlaması bekleniyor...'
      },
      buttons: {
        start: 'Eğitimi Başlat',
        stop: 'Durdur',
        inProgress: 'Eğitim Devam Ediyor...',
        delete: 'Proje Dosyalarını Sil (KVKK)',
        cancel: 'İptal'
      },
      vram: {
        title: 'GPU VRAM Kontrolü',
        checking: 'VRAM kontrol ediliyor...',
        used: 'kullanılan',
        free: 'serbest',
        notEnough: 'Eğitim için yeterli serbest VRAM yok ({gb} GB gerekli).',
        clearQuestion: 'Devam etmek için VRAM temizlensin mi?',
        enough: 'Eğitim için yeterli VRAM mevcut.',
        clearing: 'VRAM temizleniyor...',
        newFree: 'Yeni serbest',
        clearBtn: 'ComfyUI + Ollama VRAM\'ı Temizle'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Yaş Filtresi',
      dsgvo_ner: 'KVKK',
      dsgvo_llm: 'KVKK',
      translation: '\u2192 EN',
      fast_filter: 'İçerik',
      llm_context_check: 'İçerik (LLM)',
      llm_safety_check: 'Gençlik Koruması',
      llm_check_failed: 'Kontrol Başarısız',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Promptunuz sorunsuzdu, ancak oluşturulan görsel bir görsel analiz yapay zekası tarafından uygunsuz olarak işaretlendi. Bu olabilir \u2014 görsel oluşturma her zaman öngörülebilir değildir. Tekrar deneyin, her oluşturma farklıdır!',
      para86a: 'Promptunuz, Alman hukuku kapsamında yasak olan semboller veya terimler içerdiği için engellendi (\u00A786a StGB). Bu kural hepimizi nefret ve şiddetten korur. Farklı bir konu deneyin!',
      dsgvo: 'Promptunuz, bir kişi adına benzeyen bir şey içerdiği için engellendi. Bu, Genel Veri Koruma Yönetmeliği (GDPR/KVKK) kapsamında koruma altındadır. İsimler yerine "bir kız" veya "yaşlı bir adam" gibi tanımlamalar kullanın.',
      kids: 'Promptunuz çocuk güvenlik filtresi tarafından engellendi. Bazı terimler korkutucu veya rahatsız edici olabileceğinden çocuklar için uygun değildir. Fikrinizi daha dostane kelimelerle açıklamayı deneyin!',
      youth: 'Promptunuz gençlik koruması filtresi tarafından engellendi. Bazı içerikler gençler için de uygun değildir. Fikrinizi farklı şekilde ifade etmeyi deneyin!',
      generic: 'Promptunuz güvenlik sistemi tarafından engellendi. Sistem sizi uygunsuz içerikten korur. Farklı bir ifade deneyin!',
      inputImage: 'Yüklenen görsel bir görsel analiz yapay zekası tarafından uygunsuz olarak işaretlendi. Lütfen farklı bir görsel kullanın.',
      vlmSaw: 'Görsel yapay zekası gördü',
      systemUnavailable: 'Güvenlik sistemi (Ollama) yanıt vermiyor, bu nedenle daha fazla işlem yapılamıyor. Lütfen sistem yöneticisiyle iletişime geçin.',
      suggestionLoading: 'Bekleyin, bir fikrim var...',
      suggestionError: 'Şu an bir öneri oluşturamadım. Farklı kelimelerle tekrar deneyin!'
    },
    splitCombine: {
      infoTitle: 'Böl & Birleştir - Anlamsal Vektör Füzyonu',
      infoDescription: 'Bu iş akışı iki promptu anlamsal vektör düzeyinde birleştirir. Sonuç basit bir karışım değil, anlam uzaylarının daha derin matematiksel bir bağlantısıdır.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Yapay zeka modellerinin anlamı sayısal uzaylar olarak nasıl temsil ettiğini keşfedin. Farklı kavramları matematiksel olarak birleştirdiğimizde ne olur?',
      techTitle: 'Teknik Detaylar',
      techText: 'Model: SD3.5 Large | Kodlayıcı: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Kısmi Eleme - Vektör Dekonstruksiyonu',
      infoDescription: 'Bu iş akışı anlamsal vektörün belirli bölümlerini özellikle manipüle eder. Belirli boyutları ortadan kaldırarak, anlam özelliklerinin hangi yönlerinin kaybolduğunu gözlemleyebiliriz.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Anlamın vektör uzayının farklı boyutlarında nasıl kodlandığını anlayın. Bazı parçaları "kapattığımızda" ne kalır?',
      techTitle: 'Teknik Detaylar',
      techText: 'Model: SD3.5 Large | Kodlayıcı: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Metin Kodlayıcı',
      modeLabel: 'Eleme Modu',
      dimensionRange: 'Boyut Aralığı',
      selected: 'Seçili',
      dimensions: 'Boyutlar',
      emptyTitle: 'Oluşturma bekleniyor...',
      emptySubtitle: 'Sonuçlar burada görünecek',
      referenceLabel: 'Referans Görsel',
      referenceDesc: 'Manipüle edilmemiş çıktı (orijinal)',
      innerLabel: 'İç aralık elendi',
      outerLabel: 'Dış aralık elendi'
    },
    surrealizer: {
      infoTitle: 'Gerçeküstüleştirici — Bilinenden Ötesine Ekstrapolasyon',
      infoDescription: 'İki yapay zeka "beyni" metninizi okur: CLIP-L dili görseller aracılığıyla, T5 ise onu saf dilbilimsel olarak anlar. Kaydırıcı sadece aralarında karıştırmaz — görseli T5\'in tek başına üretebileceğinin çok ötesine iter. Yapay zekanın ardından eğitim sırasında hiç karşılaşmadığı vektörleri yorumlaması gerekir. Sonuç: Yapay zeka halüsinasyonları — hiçbir promptun doğrudan üretemeyeceği görseller.',
      purposeTitle: 'Kaydırıcı',
      purposeText: 'α < 0: CLIP-L yükseltildi, T5 olumsuzlandı — üst 3328 boyut (CLIP-L\'nin sıfırla doldurulduğu) ters çevrilmiş T5 vektörleri alır. Transformatördeki çapraz dikkat örüntüleri tersine döner: görsel odaklı halüsinasyonlar. ◆ α = 0: saf CLIP-L — normal görsel. ◆ α = 1: saf T5-XXL — hâlâ normal, ancak farklı kalite. ◆ α > 1: T5\'in ötesine ekstrapolasyon. α = 20\'de formül gömmeyi T5\'in 19 katı ötesinde keşfedilmemiş vektör uzayına iter — dilbilimsel odaklı halüsinasyonlar. ◆ En iyi nokta: α = 15–35.',
      techTitle: 'Nasıl Çalışır',
      techText: 'Promptunuz iki kodlayıcı aracılığıyla ayrı ayrı gönderilir: CLIP-L (görsel olarak eğitilmiş, 77 token, 768 boyut → 4096\'ya doldurulmuş) ve T5-XXL (dilbilimsel olarak eğitilmiş, 512 token, 4096 boyut). İlk 77 token konumu birleştirilir: (1-α)·CLIP-L + α·T5. Geri kalan T5 tokenları (78–512) anlamsal çapa olarak değişmeden kalır — α ne kadar uç olursa olsun görseli metninize bağlarlar. α > 1\'de bu karışım değil ekstrapoloasyondur: hiçbir eğitimin üretmediği vektörler. α < 0\'da T5 olumsuzlanır ve CLIP-L yükseltilir — transformatördeki çapraz dikkat örüntüleri tersine çevrildiğinden niteliksel olarak farklı halüsinasyonlar.',
      sliderLabel: 'Ekstrapolasyon (α)',
      sliderNormal: 'normal',
      sliderWeird: 'tuhaf',
      sliderCrazy: 'çılgın',
      sliderExtremeWeird: 'süper tuhaf',
      sliderExtremeCrazy: 'süper çılgın',
      sliderHint: "α<0: CLIP ötesi {'|'} α=0: saf CLIP {'|'} α=1: saf T5 {'|'} α>1: T5 ötesi",
      expandLabel: 'T5 için promptu genişlet',
      expandSuggest: 'Kısa prompt algılandı — T5 genişletmesi az kelimeyle sonuçları önemli ölçüde iyileştirir.',
      expandHint: 'Promptunuzda az kelime var (~{count} CLIP token). Optimal halüsinasyonlar için yapay zeka T5 bağlamını anlatısal olarak genişletebilir.',
      expandActive: 'Prompt genişletiliyor...',
      expandResultLabel: 'T5 genişletmesi (yalnızca T5 kodlayıcı)',
      advancedLabel: 'Gelişmiş Ayarlar',
      negativeLabel: 'Negatif Prompt',
      negativeHint: 'Aynı α ile ekstrapolasyon yapılır. Görselin NEREDEN uzaklaştığını belirler — farklı negatifler temelden farklı estetikler üretir.',
      cfgLabel: 'CFG Ölçeği',
      cfgHint: 'Classifier-Free Guidance: prompt etkisinin gücü. Yüksek = daha güçlü etki, daha az varyasyon.'
    },
    musicGeneration: {
      infoTitle: 'Müzik Oluşturma',
      infoDescription: 'Metin ve stil etiketlerinden müzik oluşturun. Yapay zeka sözlerinize ve tür belirtimlerinize göre melodiler, ritimler ve armoniler üretir.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Yapay zekanın müzikal kavramları nasıl yorumladığını keşfedin. Sözlerdeki kelime seçimi melodiyi nasıl etkiler?',
      lyricsLabel: 'Sözler (Metin)',
      lyricsPlaceholder: '[Kıta]\nSözleriniz burada...\n\n[Nakarat]\nNakarat...',
      tagsLabel: 'Stil Etiketleri',
      tagsPlaceholder: 'pop, piyano, neşeli, kadın vokal, 120bpm',
      selectModel: 'Bir müzik modeli seçin:',
      generate: 'Müzik Oluştur',
      generating: 'Müzik oluşturuluyor...'
    },
    musicGen: {
      simpleMode: 'Basit',
      advancedMode: 'Gelişmiş',
      lyricsLabel: 'Sözler',
      lyricsPlaceholder: 'Şarkı sözlerinizi [Kıta], [Nakarat], [Köprü] gibi yapı işaretçileriyle yazın...\n\nÖrnek:\n[Kıta]\nde doo doo doo\nde blaa blaa blaa\n\n[Nakarat]\nis all I want to sing to you',
      tagsLabel: 'Stil Etiketleri',
      tagsPlaceholder: 'Tür, ruh hali, enstrümanlar...\n\nÖrnek: ska, agresif, neşeli, yüksek kalite, bas ve saksofon üçlüsü',
      refineButton: 'Sözleri & Etiketleri İyileştir',
      refinedLyricsLabel: 'İyileştirilmiş Sözler',
      refinedLyricsPlaceholder: 'İyileştirilmiş sözleriniz burada görünecek...',
      refiningLyricsMessage: 'Yapay zeka sözlerinizi iyileştiriyor...',
      refinedTagsLabel: 'İyileştirilmiş Etiketler',
      refinedTagsPlaceholder: 'İyileştirilmiş stil etiketleri burada görünecek...',
      refiningTagsMessage: 'Yapay zeka eşleşen stil etiketleri oluşturuyor...',
      selectModel: 'Bir Müzik Modeli Seçin',
      generateButton: 'Müzik Oluştur',
      quality: 'Kalite'
    },
    musicGenV2: {
      lyricsWorkshop: 'Söz Atölyesi',
      lyricsInput: 'Metniniz',
      lyricsPlaceholder: 'Sözler, tema, anahtar kelimeler veya ruh hali yazın...',
      themeToLyrics: 'Anahtar Kelimelerden Şarkı Sözlerine',
      refineLyrics: 'Şarkı Sözlerini Yapılandır',
      resultLabel: 'Sonuç',
      resultPlaceholder: 'Sözleriniz burada görünecek...',
      expandingTheme: 'Yapay zeka anahtar kelimelerinizden şarkı sözleri yazıyor...',
      refiningLyrics: 'Yapay zeka şarkı sözlerinizi yapılandırıyor...',
      soundExplorer: 'Ses Kaşifi',
      suggestFromLyrics: 'Sözlerden Öner',
      suggestingTags: 'Yapay zeka sözlerinizi analiz ediyor...',
      mostImportant: 'en önemli',
      dimGenre: 'Tür',
      dimTimbre: 'Tını',
      dimGender: 'Ses',
      dimMood: 'Ruh Hali',
      dimInstrument: 'Enstrümanlar',
      dimScene: 'Sahne',
      dimRegion: 'Bölge (UNESCO)',
      dimTopic: 'Konu',
      audioLength: 'Ses Uzunluğu',
      generateButton: 'Müzik Oluştur',
      selectModel: 'Model',
      customTags: 'Özel Etiketler',
      customTagsPlaceholder: 'örn. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: 'Görsel Laboratuvarı',
        textlab: 'Latent Metin Laboratuvarı',
        crossmodal: 'Çapraz Modal Laboratuvar'
      },
      imageLab: {
        headerTitle: 'Görsel Laboratuvarı — Görsel Vektör Uzayı Araştırması',
        headerSubtitle: 'Difüzyon modellerinin metinden nasıl görsel oluşturduğunu araştırmak için beş araç: gürültü gidermeden dikkat ve füzyona, vektör aritmetiğine kadar.',
        tabs: {
          archaeology: {
            label: 'Gürültü Giderme Arkeolojisi',
            short: 'Modelin çalışmasını izleyin'
          },
          attention: {
            label: 'Dikkat Kartografisi',
            short: 'Modelin nereye baktığını görün'
          },
          fusion: {
            label: 'Kodlayıcı Füzyonu',
            short: 'Gerçeküstü harmanlama'
          },
          probing: {
            label: 'Özellik Araştırması',
            short: 'Boyut düzeyinde analiz'
          },
          algebra: {
            label: 'Kavram Cebiri',
            short: 'Vektör aritmetiği'
          }
        }
      },
      comingSoon: 'Bu araç gelecek bir sürümde hayata geçirilecek.',
      shared: {
        negativeHint: 'Modelin aktif olarak kaçınması gereken terimler (ör. "bulanık, metin")',
        stepsHint: 'Daha fazla adım = daha yüksek kalite ama daha uzun üretim süresi',
        cfgHint: 'Classifier-Free Guidance: yüksek = prompt\'a daha bağlı, daha az varyasyon',
        seedHint: '-1 = rastgele, sabit değer = tekrarlanabilir sonuç',
        recordingActive: 'Kayıt aktif',
        recordingCount: '{count} kayıt',
        recordingTooltip: 'Araştırma verileri otomatik olarak kaydedilir',
      },
      attention: {
        headerTitle: 'Dikkat Kartografisi — Hangi kelime hangi görsel bölgeyi yönlendirir?',
        headerSubtitle: 'Prompttaki her kelime için, oluşturulan görsel üzerinde bir ısı haritası katmanı, o kelimenin görüntünün hangi bölgesinde en fazla etkisi olduğunu gösterir. Bu, modelin anlamsal kavramları uzamsal olarak nasıl dağıttığını ortaya koyar.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Bir difüzyon modeli görsel oluştururken promptu kelime kelime talimat seti gibi okumaz. Bunun yerine "dikkat" adı verilen bir mekanizma, her kelimenin etkisini farklı görsel bölgelere dağıtır. "Ev" kelimesi ağırlıklı olarak evin göründüğü bölgeyi etkiler — ancak aynı zamanda komşu alanları da, çünkü model tüm sahnenin bağlamını anlar. Bu araç bu dağılımı görünür kılar: bir kelimeye tıklayın ve hangi görsel bölgelerin aydınlandığını görün.',
        explainHowTitle: 'Isı haritasını nasıl okuyabilirim?',
        explainHowText: 'Parlak, yoğun renk = kelimenin o bölge üzerindeki güçlü etkisi. Koyu veya yok renk = az etki. Birden fazla kelime seçerseniz, farklı renklerde görünürler. Not: haritalar MÜKEMMEL keskin kenarlı değil — bu bir hata değil, modelin kavramları bağlamsal olarak, izolasyonda değil işlediğini gösterir. Çiftlik sahnesindeki bir "ev" aynı zamanda hayvanlar ve alanlar üzerinde biraz etkiye sahiptir, çünkü model sahneyi bir bütün olarak anlar.',
        explainReadTitle: 'İki kaydırıcı ne ortaya koyuyor?',
        explainReadText: 'Gürültü giderme adımı kaydırıcısı, 25 adımlık oluşturma sürecinde dikkatı HANGI noktada görüntülediğinizi gösterir. Erken adımlar kabataslak düzen planlamasını, geç adımlar detay atamayı gösterir. Ağ derinliği seçici, transformatörde dikkatın ölçüldüğü YERİ gösterir: sığ katmanlar (girdiye yakın) küresel kompozisyon planlamasını, orta katmanlar anlamsal atamayı, derin katmanlar ince ayarı gösterir. Her iki eksen bağımsızdır — farklı kombinasyonları sistematik olarak keşfetmeye değer.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5, ortak dikkat ile MMDiT (Çok Modlu Difüzyon Transformatörü) kullanır: görsel ve metin tokenları 24 transformatör bloğu boyunca birbirlerine dikkat eder. Metin→görsel dikkat alt matrisini çıkarmak için 3 seçili blokta varsayılan SDPA işlemcisini manuel softmax(QK^T/√d) işlemcisiyle değiştiriyoruz. Haritalar 64x64 çözünürlüktür (yama ızgarası), iki doğrusal interpolasyon aracılığıyla görsel çözünürlüğe büyütülür. SD3.5 iki metin kodlayıcı kullanır: CLIP-L (BPE, 77 token) ve T5-XXL (SentencePiece, 512 token). Her ikisi de farklı tokenleştirme stratejilerinin dikkati nasıl etkilediğini görmek için burada değiştirilebilir.',
        referencesTitle: 'Araştırma Kaynakları',
        promptLabel: 'Prompt',
        promptPlaceholder: 'örn. Çiftlik arazisiyle, doğa ve hayvanlarla çevrili bir manzarada bir ev duruyor. Bazı insanlar görülebilir.',
        generate: 'Oluştur + Analiz Et',
        generating: 'Görsel oluşturuluyor ve dikkat çıkarılıyor...',
        emptyHint: 'Modelin dikkat haritalarını görselleştirmek için bir prompt girin ve Oluştur\'a tıklayın.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        tokensLabel: 'Tokenlar',
        tokensHint: 'Bir veya daha fazla kelimeye tıklayın. Alt kelime tokenları (örn. "Ku"+"gel") otomatik olarak birleştirilir. Birden fazla kelime farklı renklerde görünür.',
        timestepLabel: 'Gürültü giderme adımı',
        timestepHint: 'Difüzyon modelleri görüntüleri gürültüden görüntüye 25 adımda oluşturur. Erken adımlar kaba yapıyı oluşturur, geç adımlar detayları rafine eder. Bu kaydırıcı, modelin her adımda neye dikkat ettiğini gösterir.',
        step: 'Adım',
        layerLabel: 'Ağ derinliği',
        layerHint: 'Her gürültü giderme adımında sinyal tüm 24 transformatör katmanından geçer. Sığ katmanlar (girdiye yakın) küresel kompozisyonu, orta katmanlar anlamsal atamayı, derin katmanlar (çıktıya yakın) ince detayları yakalar. Her iki kontrol de bağımsızdır: adım = süreçte ne zaman, derinlik = ağda nerede.',
        layerEarly: 'Sığ (Kompozisyon)',
        layerMid: 'Orta (Anlambilim)',
        layerLate: 'Derin (Detay)',
        opacityLabel: 'Isı Haritası',
        opacityHint: 'Görsel üzerindeki renkli katmanın gücü.',
        baseImageLabel: 'Temel görsel',
        baseColor: 'Renkli',
        baseBW: 'S/B',
        baseOff: 'Kapalı',
        baseImageHint: 'Renkli orijinal görüntüyü gösterir. S/B ısı haritası renkleri öne çıksın diye görüntüyü renksizleştirir. Kapalı görüntüyü tamamen gizler ve yalnızca dikkat haritasını gösterir.',
        encoderLabel: 'Metin Kodlayıcı',
        encoderClipL: 'CLIP-L (77 Token)',
        encoderT5: 'T5-XXL (512 Token)',
        encoderHint: 'SD3.5 farklı tokenleştirmeli iki metin kodlayıcı kullanır. CLIP-L BPE (Byte-Pair Encoding) kullanır, T5-XXL SentencePiece kullanır. Her iki kodlayıcının aynı promptu nasıl işlediğini ve her birinin hangi görsel bölgelerini yönlendirdiğini karşılaştırın.',
        download: 'Görseli İndir'
      },
      probing: {
        headerTitle: 'Özellik Araştırması — Hangi boyutlar neyi kodlar?',
        headerSubtitle: 'İki promptu karşılaştırın ve hangi gömme boyutlarının anlamsal farkı kodladığını keşfedin. Görseli nasıl etkilediklerini görmek için tek tek boyutları seçici olarak aktarın.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Her kelime, metin kodlayıcı tarafından yüksek boyutlu bir vektöre dönüştürülür (örn. T5 için 4096 boyut). Prompttaki bir kelimeyi değiştirdiğinizde — örn. "kırmızı"dan "mavi"ye — belirli boyutlar diğerlerinden daha fazla değişir. Bu araç size HANGİ boyutların en fazla değiştiğini gösterir ve Prompt B\'den bireysel boyutları Prompt A\'ya seçici olarak aktarmanıza olanak tanır.',
        explainHowTitle: 'Aktarım nasıl çalışır?',
        explainHowText: 'Çubuk grafik, fark büyüklüğüne göre sıralanmış tüm boyutları gösterir. Bir pencere seçmek için sıra aralığı kontrollerini (Başlangıç/Bitiş) kullanın — örn. yalnızca ilk 100 veya özellikle sıra 880–920. "Aktar"a tıklamak aynı ayarlarla (aynı tohum!) görüntüyü yeniden oluşturur — ancak Prompt B\'den seçili boyutlarla. Bu, o boyutların tam olarak ne "kodladığını" görmenizi sağlar.',
        explainReadTitle: 'Çubuk grafiği nasıl okuyabilirim?',
        explainReadText: 'Her çubuk bir gömme boyutunu temsil eder. Uzunluk, o boyutun Prompt A ve B arasında ne kadar farklılaştığını gösterir. Büyük farklılıklara sahip boyutlar, anlamsal değişimin en olası taşıyıcılarıdır. Ancak not: gömmeler dağıtılmıştır — görünür bir değişiklik üretmek için genellikle birden fazla boyutun birlikte gereklidir.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5 üç metin kodlayıcı kullanır: CLIP-L (768d), CLIP-G (1280d) ve T5-XXL (4096d). Her birini ayrı ayrı araştırabilirsiniz. Fark, tüm token konumlarında ortalama mutlak sapma olarak hesaplanır: mean(abs(B-A), dim=tokens). Aktarım, seçili boyutları tüm token konumlarında aynı anda değiştirir.',
        referencesTitle: 'Araştırma Kaynakları',
        promptALabel: 'Prompt A (Orijinal)',
        promptBLabel: 'Prompt B (Karşılaştırma)',
        promptAPlaceholder: 'örn. Gölün yanında kırmızı bir ev',
        promptBPlaceholder: 'örn. Gölün yanında mavi bir ev',
        encoderLabel: 'Kodlayıcı',
        encoderAll: 'Tümü (önerilen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analiz Et',
        analyzing: 'Promptlar kodlanıyor ve karşılaştırılıyor...',
        transferBtn: 'Prompt B\'den seçili vektör boyutlarını oluşturulan görsele aktar',
        transferring: 'Değiştirilmiş gömme ile görsel oluşturuluyor...',
        rankFromLabel: 'Sıradan başla',
        rankToLabel: 'Sıraya kadar',
        sliderLabel: 'Prompt B\'den boyutları seçin',
        range1Label: 'Aralık 1',
        range2Label: 'Aralık 2',
        addRange: 'Aralık ekle',
        selectionDesc: 'Prompt B\'den {count} boyut seçildi (sıra {ranges} / {total})',
        listTitle: 'Prompt A\'ya göre en büyük farklılığa sahip Prompt B\'nin {count} boyutu',
        sortAsc: 'Artan',
        sortDesc: 'Azalan',
        originalLabel: 'Orijinal (Prompt A)',
        modifiedLabel: 'Değiştirilmiş (Prompt B\'den Aktarım)',
        modifiedHint: 'Aşağıdan bir sıra aralığı seçin ve "Aktar"a tıklayın — bu, B\'den aktarılan boyutlarla Prompt A\'yı gösterecek (aynı tohum).',
        noDifference: 'Gömmeler aynıdır — Prompt B\'yi değiştirin.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        selectAll: 'Tümü',
        selectNone: 'Hiçbiri',
        encoderHint: 'Tümü = tüm kodlayıcılar birlikte. CLIP-L/CLIP-G/T5 = analiz için tek bir kodlayıcıyı izole eder.',
        sliderHint: 'En önemli gömme boyutlarının sıra aralığını seçin (A ve B arasındaki farka göre sıralanmış).',
        transferHint: 'Seçilen boyutları Prompt B\'den Prompt A\'ya aktarır ve yeni bir görüntü üretir.',
        downloadOriginal: 'Orijinali İndir',
        downloadModified: 'Değiştirilmişi İndir'
      },
      algebra: {
        headerTitle: 'Kavram Cebiri \u2014 Görsel gömmeler üzerinde vektör aritmetiği',
        headerSubtitle: 'Ünlü word2vec anolojisini görsel üretimine uygulayın: Kral \u2212 Adam + Kadın \u2248 Kraliçe. Üç prompt kodlanır ve cebirsel olarak birleştirilir.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: '2013\'te Mikolov, kelime gömmelerinin anlamsal ilişkileri doğrusal yönler olarak kodladığını gösterdi: "Kral" vektörü eksi "Adam" artı "Kadın" = "Kraliçe"ye yakın bir vektör verir. Bu araç, bu fikri SD3.5\'in metin kodlayıcılarına uygular: tek kelimeleri manipüle etmek yerine, tüm prompt gömmelerini manipüle edersiniz. Sonuç, kavram A\'yı içeren ancak B\'nin C ile değiştirildiği bir görseldir.',
        explainHowTitle: 'Cebir nasıl çalışır \u2014 ve neden sadece negatif prompt kullanmak yeterli değil?',
        explainHowText: 'Üç prompt girersiniz: A (temel), B (çıkar) ve C (ekle). Formül şudur: Sonuç = A \u2212 Ölçek\u2081\u00d7B + Ölçek\u2082\u00d7C. Ölçek kaydırıcıları yoğunluğu kontrol eder: 1,0\'da B tam olarak çıkarılır ve C tam olarak eklenir. 0,5\'te yalnızca yarısı. 1,0\'ın üzerindeki değerler etkiyi güçlendirir. \u2014 Neden sadece "A + C" promptu ve negatif olarak "B" kullanmıyorsunuz? Çünkü bu temelden farklı bir şey yapar: Negatif prompt, gürültü giderme sürecini 25 adımın HER BİRİNDE B\'den uzaklaştırır \u2014 model adım adım "B değil"i nasıl yorumlayacağına karar verir. Kavram Cebiri ise görsel üretimden ÖNCE yeni bir vektör hesaplar: çıkarma, difüzyon sürecinde değil gömme uzayında gerçekleşir. Sonuç, doğrudan "B-liksizlik artı C-lik olan A"yı kodlayan tek bir vektördür. Negatif prompt "bunu yapma" der. Cebir "bu kavramı çıkar ve o diğerini koy" der \u2014 adım adım bir kaçınma stratejisi yerine anlam uzayında cerrahi bir operasyon.',
        explainReadTitle: 'Sonuçlar ne anlama gelir?',
        explainReadText: 'Solda referans görsel (yalnızca Prompt A, aynı tohum) görürsünüz. Sağda cebir sonucu. Analoji işe yararsa, sağdaki görüntü kavram A\'yı ancak B\u2192C anlamsal değişimiyle göstermelidir. Örnek: "Sahilde gün batımı" \u2212 "Sahil" + "Dağlar" \u2248 "Dağların üzerinde gün batımı". L2 mesafesi, sonucun orijinalden ne kadar uzaklaştığını gösterir. \u2014 İşlem değişmeli midir? Hayır. B\'nin çıkarılması ve C\'nin eklenmesi, A vektörüne göre gerçekleşir. B\u2192C yönü yalnızca A bağlamında anlam ifade eder: "Kral \u2212 Adam" Kral vektöründen "eril" yönleri çıkarır, "+ Kadın" "dişil" yönleri ekler \u2014 sonuç "Kraliçe"ye yakın düşer. C, B\'nin kaldırıldığı yere cerrahi olarak yerleştirilmez; sadece eklenir. Bunun hâlâ işe yaraması, anlamsal ilişkilerin vektör uzayında tutarlı doğrusal yönler olarak kodlandığını gösterir.',
        techTitle: 'Teknik detaylar',
        techText: 'Cebir, seçili kodlayıcı gömmelerinde gerçekleştirilir: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d) veya tümü birleşik (589 token \u00d7 4096d). Aynı işlem havuzlanmış gömmelere de uygulanır (2048d). Her iki görsel de adil karşılaştırma için aynı tohumu kullanır.',
        referencesTitle: 'Araştırma Kaynakları',
        promptALabel: 'Prompt A (Temel)',
        promptAPlaceholder: 'örn. Palmiye ağaçlarıyla sahilde gün batımı',
        promptBLabel: 'Prompt B (Çıkar)',
        promptBPlaceholder: 'örn. Palmiye ağaçlarıyla sahil',
        promptCLabel: 'Prompt C (Ekle)',
        promptCPlaceholder: 'örn. Karlı dağlar',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Kodlayıcı',
        encoderAll: 'Tümü (önerilen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Hesapla',
        generating: 'Gömmeler hesaplanıyor ve görseller oluşturuluyor...',
        referenceLabel: 'Referans (Prompt A)',
        resultLabel: 'Sonuç (A \u2212 B + C)',
        l2Label: 'Orijinalden L2 mesafesi',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        scaleSubLabel: 'Çıkarma ölçeği',
        scaleAddLabel: 'Ekleme ölçeği',
        encoderHint: 'Tümü = tüm kodlayıcılar birlikte. CLIP-L/CLIP-G/T5 = aritmetik için tek bir kodlayıcıyı izole eder.',
        scaleSubHint: 'Çıkarma ağırlığı (B). Yüksek = B kavramının daha güçlü kaldırılması.',
        scaleAddHint: 'Toplama ağırlığı (C). Yüksek = C kavramının daha güçlü eklenmesi.',
        l2Hint: 'Gömme uzayında Öklid mesafesi. Küçük = daha benzer, büyük = daha farklı.',
        downloadReference: 'Referansı İndir',
        downloadResult: 'Sonucu İndir',
        resultHint: 'Üç prompt girin ve Hesapla\'ya tıklayın \u2014 vektör aritmetiğinin sonucu burada görünecek.'
      },
      archaeology: {
        headerTitle: 'Gürültü Giderme Arkeolojisi \u2014 Gürültü nasıl görüntüye dönüşür?',
        headerSubtitle: 'Her gürültü giderme adımını gözlemleyin. Difüzyon modelleri soldan sağa çizmez \u2014 kabataslak şekillerden ince detaylara her yerde aynı anda çalışırlar.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Difüzyon modeli bir görüntü oluştururken gürültüyü kademeli olarak giderir. Soldan sağa çizmenin aksine, model TÜM görüntü bölgelerinde aynı anda çalışır. İlk adımlarda kabataslak yapılar ortaya çıkar: Yukarı nerede, aşağı nerede? Ufuk nerede? Orta adımlarda anlamsal içerik görünür: nesneler, şekiller, renkler. Son adımlar dokuları ve detayları rafine eder. Bu araç her tek adımı görünür kılar.',
        explainHowTitle: 'Bu aracı nasıl kullanırım?',
        explainHowText: 'Bir prompt girin ve Oluştur\'a tıklayın. Model 25 ara görüntü üretir (gürültü giderme adımı başına bir). Bunlar aşağıda film şeridi olarak görünür. Her adımı tam boyutta görüntülemek için bir küçük resme tıklayın veya zaman çizelgesi kaydırıcısını kullanın. Erken ve geç adımları karşılaştırın: Model ne çizdiğini ne zaman "biliyor"?',
        explainReadTitle: 'Üç aşama ne ortaya koyuyor?',
        explainReadText: 'Erken adımlar (1\u20138): Küresel kompozisyon \u2014 temel yapı, renk dağılımı, düzen planlaması. Orta adımlar (9\u201317): Anlamsal ortaya çıkış \u2014 nesneler tanınabilir hale gelir, şekiller kristalleşir. Geç adımlar (18\u201325): Detay iyileştirme \u2014 dokular, kenarlar, ince desenler. Geçişler kademeliydi ancak aşamalar açıkça gösteriyor: model önce küresel olarak "planlar", sonra yerel olarak rafine eder. Özellikle ilginç: İlk adım ince taneli pikseller değil, renkli yamalar gösterir. Bunun nedeni gürültünün piksel uzayında değil latent uzayda (16 kanallı 128\u00d7128) oluşturulmasıdır. VAE her latent pikseli ~8\u00d78 piksel yamasına çevirir \u2014 saf Gaussian gürültüsü bile tutarlı renk kümeleri haline gelir. Model asla bireysel pikseller içinde "düşünmez", her zaman bu sıkıştırılmış uzayda çalışır.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5 Large, 25 varsayılan adımla düzeltilmiş akış zamanlayıcısı kullanır. Her adımda mevcut latent vektörler VAE aracılığıyla decode edilir (1024\u00d71024 JPEG). VAE (Varyasyonel Otokodlayıcı) matematiksel latent uzayı piksellere çevirir. Latent temsil 16 kanallı 128\u00d7128\'dir \u2014 her latent piksel görselde ~8\u00d78 piksel yamasına karşılık gelir. Bu yüzden ilk adım bile ince piksel gürültüsü yerine renkli kümeler gösterir: VAE rastgele 16 boyutlu vektörleri tutarlı renk yamaları olarak yorumlar.',
        referencesTitle: 'Araştırma Kaynakları',
        promptLabel: 'Prompt',
        promptPlaceholder: 'örn. İnsanlarla, binalarla ve çeşmeyle orta çağ kasabasında bir pazar yeri',
        generate: 'Oluştur',
        generating: 'Görsel oluşturuluyor \u2014 her adım kaydediliyor...',
        emptyHint: 'Gürültü giderme sürecini görselleştirmek için bir prompt girin ve Oluştur\'a tıklayın.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        filmstripLabel: 'Gürültü Giderme Film Şeridi',
        timelineLabel: 'Adım',
        phaseEarly: 'Kompozisyon',
        phaseMid: 'Anlambilim',
        phaseLate: 'Detay',
        phaseEarlyDesc: 'Küresel yapı ve renk dağılımı ortaya çıkıyor',
        phaseMidDesc: 'Nesneler ve şekiller tanınabilir hale geliyor',
        phaseLateDesc: 'Dokular ve ince detaylar keskinleştiriliyor',
        finalImageLabel: 'Son görsel (tam çözünürlük)',
        timelineHint: 'Gürültü giderme adımları arasında gezinir — görüntünün gürültüden son kompozisyona nasıl oluştuğunu gösterir.',
        download: 'Görseli İndir'
      },
      textLab: {
        headerTitle: 'Latent Metin Laboratuvarı \u2014 Bilimsel LLM Dekonstruksiyonu',
        headerSubtitle: 'Temsil Mühendisliği, karşılaştırmalı model arkeolojisi ve sistematik önyargı analizi: Dil modellerini araştırmak için araştırmaya dayalı üç araç.',
        explanationToggle: 'A\u00e7\u0131klamay\u0131 g\u00f6ster',
        modelPanel: {
          presetLabel: 'Hazır Ayar',
          presetNone: 'Hazır ayar yok (özel kimlik)',
          customModelLabel: 'HuggingFace Model Kimliği',
          customModelPlaceholder: 'örn. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Nicemleme',
          quantAuto: 'Otomatik',
          quantizationHint: 'bf16 = tam kalite, int8 = yarı VRAM, int4 = minimum VRAM ama en düşük kalite',
        },
        temperatureHint: 'Metin üretiminin rastgeleliği. Düşük = deterministik, yüksek = daha yaratıcı.',
        maxTokensHint: 'Üretilen maksimum token sayısı (kelime parçaları).',
        textSeedHint: '-1 = rastgele, sabit değer = tekrarlanabilir sonuç',
        tabs: {
          repeng: { label: 'Representation Engineering', short: 'LLM\'lerde y\u00f6nlendirme vekt\u00f6rleri bulma' },
          compare: { label: 'Model Kar\u015f\u0131la\u015ft\u0131rmas\u0131', short: '\u0130ki LLM\'yi katman katman kar\u015f\u0131la\u015ft\u0131rma' },
          bias: { label: '\u00d6nyarg\u0131 Arkeolojisi', short: 'LLM\'lerdeki gizli \u00f6nyarg\u0131lar\u0131 ortaya \u00e7\u0131karma' },
        },
        repeng: {
          title: 'Temsil Mühendisliği',
          subtitle: 'Aktivasyon uzayında kavram yönleri bulun ve üretimi yönlendirin',
          explainWhatTitle: 'Bu deney ne g\u00f6steriyor?',
          explainWhatText: 'Zou vd. (2023) \u201cRepresentation Engineering\u201d ve Li vd. (2024) \u201cInference-Time Intervention\u201d \u00e7al\u0131\u015fmalar\u0131na dayan\u0131r. LLM\u2019ler soyut kavramlar\u0131 y\u00fcksek boyutlu aktivasyon uzay\u0131nda y\u00f6nler olarak kodlar. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: 'Nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Bu deney modelden bir \u201cdo\u011fruluk y\u00f6n\u00fc\u201d \u00e7\u0131kar\u0131r. Kontrast \u00e7iftler do\u011fru ve yanl\u0131\u015f ifadeler i\u00e7erir. Y\u00f6n ters \u00e7evrildi\u011finde (\u03b1 = -1), model yanl\u0131\u015f yan\u0131t \u00fcretmelidir. \u00d6neri: \u0130ngilizce promptlar daha iyi \u00e7al\u0131\u015f\u0131r.',
          referencesTitle: 'Araştırma Kaynakları',
          expectedResults: 'Beklenen sonuçlar: α = 0\'da (temel), model doğru yanıtı üretir. α = -1\'de (inversiyon), yanlış bir yanıt görünmelidir \u2014 deneyin özü budur. α = +1\'de model zaten doğru yanıt verdiği için az şey değişir. |α| > 2\'nin ötesinde, eserler (tekrarlar, saçmalık) baskın olur. Zou vd.\'ne göre "tatlı nokta" |α| 0,5 ile 2,0 arasındadır. Açıklanan varyans > %50, temiz ayrımı gösterir \u2014 bunun altında, kontrast çiftler çok benzer veya çok azdır (en az 3 önerilir).',
          pairsTitle: 'Kontrast Çiftleri',
          pairsSubtitle: 'En az 3 çift önerilir. Her çift yalnızca hedef kavramda farklı olmalıdır (doğru ve yanlış). Örnekler düzenlenebilir.',
          positiveLabel: 'Pozitif (doğru)',
          negativeLabel: 'Negatif (yanlış)',
          positivePlaceholder: 'örn. Fransa\'nın başkenti Paris\'tir',
          negativePlaceholder: 'örn. Fransa\'nın başkenti Berlin\'dir',
          addPair: 'Çift ekle',
          removePair: 'Kaldır',
          targetLayerLabel: 'Hedef katman',
          targetLayerHint: 'Hangi transformer katmanının yönlendirme vektörünü aldığı. Farklı katmanlar metin üretiminin farklı yönlerini etkiler.',
          targetLayerAuto: 'Son katman',
          findDirection: 'Yön bul',
          finding: 'Kavram yönü hesaplanıyor...',
          directionFound: 'Kavram yönü bulundu',
          varianceLabel: 'Açıklanan varyans',
          dimLabel: 'Boyutlar',
          projectionsTitle: 'Kontrast çift projeksiyanları',
          testTitle: 'Test + Manipülasyon',
          testSubtitle: 'Bir cümle girin ve üretimi kavram yönü boyunca yönlendirin',
          testPromptLabel: 'Test promptu',
          testPromptPlaceholder: 'örn. Almanya\'nın başkenti',
          alphaLabel: 'Manipülasyon gücü (α)',
          alphaHint: 'Yönlendirme vektörünün gücü. 0 = etki yok, yüksek = kontrast çiftlerinin daha güçlü etkisi.',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          seedLabel: 'Tohum (-1 = rastgele)',
          generateBtn: 'Manipülasyonla oluştur',
          generating: 'Manipüle edilmiş üretim çalıştırılıyor...',
          baselineLabel: 'Temel (manipülasyon yok)',
          manipulatedLabel: 'Manipüle edilmiş (α = {alpha})',
          projectionLabel: 'Kavram yönüne projeksiyon',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        compare: {
          title: 'Karşılaştırmalı Model Arkeolojisi',
          subtitle: 'İki model yükleyin ve iç temsillerini sistematik olarak karşılaştırın',
          explainWhatTitle: 'Bu deney ne g\u00f6steriyor?',
          explainWhatText: 'Belinkov (2022) ve Olsson vd. (2022) \u00e7al\u0131\u015fmalar\u0131na dayan\u0131r. Is\u0131 haritas\u0131 her iki modelin katmanlar\u0131 aras\u0131ndaki CKA\u2019y\u0131 g\u00f6sterir. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: 'Nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Model A aktif haz\u0131r ayard\u0131r. \u0130kinci bir model (Model B) se\u00e7in ve y\u00fckleyin. Metin girin ve \u201cKar\u015f\u0131la\u015ft\u0131r\u201d\u0131 t\u0131klay\u0131n. CKA \u0131s\u0131 haritas\u0131 hangi katmanlar\u0131n benzer temsil etti\u011fini g\u00f6sterir.',
          referencesTitle: 'Araştırma Kaynakları',
          modelATitle: 'Model A (hazır ayar seçiminden)',
          modelAHint: 'Yukarıdaki hazır ayar menüsünden değiştirin',
          modelBTitle: 'Model B (ikinci model)',
          modelBPresetLabel: 'Hazır Ayar',
          modelBCustomLabel: 'HuggingFace Model Kimliği',
          modelBCustomPlaceholder: 'örn. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Model B\'yi Yükle',
          modelBLoaded: 'Model B yüklendi',
          modelBNone: 'Model B yüklü değil',
          promptLabel: 'Prompt',
          promptPlaceholder: 'örn. Kedi paspasın üzerinde oturdu ve kuşları izledi',
          seedLabel: 'Tohum',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          compareBtn: 'Karşılaştır',
          comparing: 'Modeller karşılaştırılıyor...',
          heatmapTitle: 'Katman Hizalaması (CKA)',
          heatmapAxisA: 'Model A \u2014 Katmanlar',
          heatmapAxisB: 'Model B \u2014 Katmanlar',
          heatmapExplain: 'Parlak hücreler = yüksek temsil benzerliği. Köşegen örüntüler, modellerin bilgiyi benzer bir sırayla işlediğini gösterir.',
          attentionTitle: 'Dikkat Karşılaştırması (son katman)',
          modelALabel: 'Model A',
          modelBLabel: 'Model B',
          generationTitle: 'Üretim Karşılaştırması (aynı tohum)',
          layerStatsTitle: 'Katman İstatistikleri',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        bias: {
          title: 'Önyargı Arkeolojisi',
          subtitle: 'Kontrollü token manipülasyonu aracılığıyla sistematik önyargı deneyleri',
          explainWhatTitle: 'Bu deney ne g\u00f6steriyor?',
          explainWhatText: 'Zou vd. (2023) ve Bricken vd. (2023) \u00e7al\u0131\u015fmalar\u0131na dayan\u0131r. Bu ara\u00e7 sistematik \u00f6nyarg\u0131lar\u0131 ara\u015ft\u0131r\u0131r. \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: 'Nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Bir deney t\u00fcr\u00fc se\u00e7in. Modelin devam etmesini sa\u011flayan bir prompt girin. Sonu\u00e7lar temel ve manip\u00fcle edilmi\u015f \u00fcretimleri g\u00f6sterir.',
          referencesTitle: 'Araştırma Kaynakları',
          presetLabel: 'Deney türü',
          presetGender: 'Cinsiyet \u2014 Cinsiyetli zamirleri bastır',
          presetSentiment: 'Duygu \u2014 Pozitif/negatifi güçlendir',
          presetDomain: 'Alan \u2014 Bilimsel/şiirsel güçlendir',
          presetCustom: 'Özel deney',
          promptLabel: 'Prompt',
          promptPlaceholder: 'örn. Doktor hastaya dedi ki',
          customBoostLabel: 'Güçlendirme tokenları (virgülle ayrılmış)',
          customBoostPlaceholder: 'örn. karanlık,gölge,gece',
          customSuppressLabel: 'Bastırma tokenları (virgülle ayrılmış)',
          customSuppressPlaceholder: 'örn. ışık,güneş,parlak',
          numSamplesLabel: 'Koşul başına örnek',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          seedLabel: 'Temel tohum',
          runBtn: 'Deneyi çalıştır',
          running: 'Önyargı deneyi çalıştırılıyor...',
          baselineTitle: 'Temel (manipülasyon yok)',
          groupTitle: 'Grup: {name}',
          modeSuppress: 'bastırıldı',
          modeBoost: 'güçlendirildi',
          tokensLabel: 'Tokenlar',
          sampleSeedLabel: 'Tohum',
          genderDesc: 'Tüm cinsiyetli zamirleri bastırır ve modelin hangi varsayılanları seçtiğini gözlemler.',
          sentimentDesc: 'Pozitif veya negatif kelimeleri güçlendirir ve tüm metin akışının ne kadar güçlü etkilendiğini ölçer.',
          domainDesc: 'Bilimsel veya şiirsel kelime dağarcığını güçlendirir ve kayıt değişimlerini gözlemler.',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        error: {
          gpuUnreachable: 'GPU hizmetine ulaşılamıyor. Çalışıyor mu?',
          loadFailed: 'Model yüklenemedi.',
          operationFailed: 'İşlem başarısız oldu.'
        }
      },
      crossmodal: {
        headerTitle: 'Çapraz Modal Laboratuvar',
        headerSubtitle: 'Latent uzaylardan ses: T5 gömme manipülasyonu, görsel rehberli ses üretimi, çapraz modal aktarım',
        explanationToggle: 'Ayr\u0131nt\u0131l\u0131 a\u00e7\u0131klamay\u0131 g\u00f6ster',
        generate: 'Oluştur',
        generating: 'Oluşturuluyor...',
        result: 'Sonuç',
        seed: 'Tohum',
        generationTime: 'Oluşturma süresi',
        tabs: {
          synth: {
            label: 'Latent Ses Sentezci',
            short: 'T5 gömme manipülasyonu',
            title: 'Latent Ses Sentezci',
            description: 'Stable Audio\'nun T5 koşullandırma uzayının (768d) doğrudan manipülasyonu. Promptlar arasında interpolasyon, promptun ötesinde ekstrapolasyon, gömmeleri ölçeklendirme ve gürültü enjeksiyonu. Ultra kısa döngüler, gerçek zamana yakın.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Görsel/metinden ses (CVPR 2025)',
            title: 'MMAudio — Video/Görselden Ses',
            description: 'Görsel ve metin aynı ağa ayrı sinyaller olarak girer — görsel dile çevrilmez, ikisi birlikte ses üretimini yönlendirir. Model, video ve ses üzerinde birlikte eğitilmiştir ve görülen ile duyulan arasındaki doğrudan ilişkileri öğrenir. 8s\'ye kadar, 44,1kHz, ~1,2s hesaplama süresi. (Cheng vd., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'ImageBind Rehberliği',
            short: 'Gradyan tabanlı görsel rehberlik',
            title: 'ImageBind Gradyan Rehberliği',
            description: 'Stable Audio gürültü giderme süreci sırasında gradyan tabanlı yönlendirme. ImageBind görsel ve ses için paylaşılan 1024d uzayı sağlar \u2014 kosinüs benzerliğinin gradyanı ses üretimini görsel gömmeye doğru yönlendirir.'
          }
        },
        synth: {
          explainWhatTitle: 'Latent Ses Sentezci ne yapar?',
          explainWhatText: 'Stable Audio metinden ses \u00fcretir. Metin, bir T5 kodlay\u0131c\u0131 taraf\u0131ndan 768 boyutlu say\u0131sal bir vekt\u00f6re d\u00f6n\u00fc\u015ft\u00fcr\u00fcl\u00fcr \u2014 burada do\u011frudan bu vekt\u00f6r\u00fc manip\u00fcle edebilirsiniz. Modelin \u201cne\u201d \u00fcretti\u011fini de\u011fi\u015ftirmek (prompt arac\u0131l\u0131\u011f\u0131yla) yerine, modelin metni i\u00e7sel olarak \u201cnas\u0131l\u201d anlad\u0131\u011f\u0131n\u0131 de\u011fi\u015ftirirsiniz. Benzer g\u00f6r\u00fcnen iki prompt bu uzayda birbirinden \u00e7ok uzak olabilir \u2014 ve tam tersi.',
          explainHowTitle: 'Bu arac\u0131 nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Prompt A\'ya bir metin girin \u2014 bu temel sesi belirler. \u0130ste\u011fe ba\u011fl\u0131: Prompt B hedef nokta olarak. Alpha kayd\u0131r\u0131c\u0131s\u0131 kar\u0131\u015f\u0131m\u0131 kontrol eder: 0\'da yaln\u0131zca A, 1\'de yaln\u0131zca B, 0.5\'te bir kar\u0131\u015f\u0131m duyars\u0131n\u0131z. 1\'in \u00fczerindeki de\u011ferler B\'nin \u00f6tesine ekstrapolasyon yapar (ses daha a\u015f\u0131r\u0131 hale gelir), 0\'\u0131n alt\u0131ndaki de\u011ferler z\u0131t y\u00f6ne gider. Magnitude t\u00fcm g\u00f6mmeyi \u00f6l\u00e7ekler \u2014 daha y\u00fcksek de\u011ferler daha yo\u011fun sesler \u00fcretir. Noise rastgelelik enjekte eder ve \u00f6ng\u00f6r\u00fclemez varyasyonlar olu\u015fturur. Spectral Strip (Generate butonunun alt\u0131nda) t\u00fcm 768 boyutu \u00e7ubuk olarak g\u00f6sterir. Tek tek boyutlar\u0131 t\u0131klay\u0131p s\u00fcr\u00fckleyerek kayd\u0131rabilir ve sesi do\u011frudan manip\u00fcle edebilirsiniz. Sa\u011f t\u0131klama bir boyutu s\u0131f\u0131rlar.',
          promptA: 'Prompt A (Temel)',
          promptAPlaceholder: '\u00f6rn. okyanus dalgalar\u0131',
          promptB: 'Prompt B (İsteğe bağlı, interpolasyon için)',
          promptBPlaceholder: 'örn. piyano melodisi',
          alpha: 'Alpha (İnterpolasyon)',
          alphaHint: '0 = yalnızca A, 1 = yalnızca B, arası = karışım, >1 veya <0 = ekstrapolasyon',
          magnitude: 'Büyüklük (Ölçekleme)',
          magnitudeHint: 'Küresel gömme ölçekleme (1,0 = değişmez)',
          noise: 'Gürültü',
          noiseHint: 'Gömme üzerinde Gaussian gürültüsü (0 = gürültü yok)',
          duration: 'Süre (s)',
          steps: 'Adımlar',
          cfg: 'CFG',
          durationHint: 'Üretilen ses klibinin saniye cinsinden uzunluğu',
          stepsHint: 'Gürültü giderme adımları. Daha fazla = daha yüksek kalite.',
          cfgHint: 'Ses üretimi için Classifier-Free Guidance',
          seedHint: '-1 = rastgele, sabit değer = tekrarlanabilir sonuç',
          loop: 'Döngü oynatma',
          loopOn: 'Döngü Açık',
          loopOff: 'Döngü Kapalı',
          stop: 'Durdur',
          looping: 'Döngüde',
          playing: 'Oynatılıyor',
          stopped: 'Durduruldu',
          transpose: 'Transpozisyon (yarım tonlar)',
          midiSection: 'MIDI Kontrolü',
          midiUnsupported: 'Web MIDI bu tarayıcı tarafından desteklenmiyor.',
          midiInput: 'MIDI Girişi',
          midiNone: '(yok)',
          midiMappings: 'CC Eşlemeleri',
          midiNoteC3: 'Nota (C3 = Ref)',
          midiGenerate: 'Oluştur + Transpose',
          midiPitch: 'C3\'e göre perde',
          loopInterval: 'Döngü aralığı',
          loopOptimize: 'Otomatik optimize',
          loopPingPong: 'Ping-pong',
          loopIntervalHint: 'Döngü bölgesinin başlangıç/sonu — Stable Audio solmasını kesmek için sonu kısaltın',
          modeLoop: 'Döngü',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Dalga Tablosu',
          modeRate: 'Tempo (hızlı)',
          modePitch: 'Perde (OLA)',
          wavetableScan: 'Tarama Konumu',
          wavetableScanHint: 'Kareler arasında morph (0 = başlangıç, 1 = bitiş)',
          wavetableFrames: '{count} kare',
          midiScan: 'Tarama Konumu',
          adsrTitle: 'ADSR Zarfı',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'MIDI notaları için zarf (Atış/Azalma/Tutma/Salıverme)',
          play: 'Oynat',
          normalize: 'Ses seviyesini normalleştir',
          peak: 'Zirve',
          crossfade: 'Geçiş',
          transposeHint: 'Tonu yarım ton cinsinden kaydırır',
          crossfadeHint: 'Döngü sınırında geçiş süresi (ms)',
          normalizeHint: 'Ses seviyesini maksimum genliğe normalleştirir',
          saveRaw: 'Ham kaydet',
          saveLoop: 'Döngü kaydet',
          embeddingStats: 'Gömme istatistikleri',
          dimensions: {
            section: 'Boyut Gezgini',
            hint: 'Çubuklara sürükle = ofset ayarla. Yatay olarak boya = birden fazla boyut.',
            resetAll: 'Tümünü sıfırla',
            hoverActivation: 'Aktivasyon',
            hoverOffset: 'Ofset',
            rightClickReset: 'Sağ tıkla = sıfırla',
            sortDiff: 'Prompt farkına göre sıralandı',
            sortMagnitude: 'Aktivasyona göre sıralandı',
            activeOffsets: '{count} ofset aktif',
            applyAndGenerate: 'Uygula ve yeniden oluştur',
            undo: 'Geri Al',
            redo: 'Yinele'
          }
        },
        mmaudio: {
          explainWhatTitle: 'MMAudio ne yapar?',
          explainWhatText: 'MMAudio (Cheng vd., CVPR 2025) video ve ses \u00fczerinde birlikte e\u011fitilmi\u015ftir. Bir g\u00f6r\u00fcnt\u00fcy\u00fc metne, sonra sese \u00e7evirmez \u2014 g\u00f6r\u00fcnt\u00fc ve metni ayn\u0131 a\u011fda paralel sinyaller olarak i\u015fler. Model, hangi seslerin hangi g\u00f6rsel sahnelere ait oldu\u011funu \u00f6\u011frenmi\u015ftir \u2014 bir orman ku\u015f c\u0131v\u0131lt\u0131s\u0131, bir cadde trafik g\u00fcr\u00fclt\u00fcs\u00fc, bir gitar t\u0131ng\u0131rdama sesleri \u00fcretir.',
          explainHowTitle: 'Bu arac\u0131 nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Bir g\u00f6rsel y\u00fckleyin ve/veya bir metin promptu girin \u2014 ikisini birlikte kullanmak en zengin sonu\u00e7lar\u0131 verir. G\u00f6rsel tek ba\u015f\u0131na g\u00f6rsel i\u00e7eri\u011fe uygun sesler \u00fcretir. Metin promptu sesi ek olarak y\u00f6nlendirebilir veya g\u00f6rsel olmadan tek ba\u015f\u0131na kullan\u0131labilir. Negatif Prompt\'ta duymak \u0130STEMED\u0130\u011e\u0130N\u0130Z sesleri tan\u0131mlay\u0131n (\u00f6rn. \u201ckon\u015fma, m\u00fczik\u201d). Duration uzunlu\u011fu belirler (1\u20138 saniye). CFG Strength modelin promptu ne kadar s\u0131k\u0131 takip etti\u011fini kontrol eder \u2014 d\u00fc\u015f\u00fck de\u011ferler (2\u20133) daha \u00e7e\u015fitli, y\u00fcksek de\u011ferler (6\u20138) daha prompt-sad\u0131k sonu\u00e7lar \u00fcretir.',
          imageUpload: 'G\u00f6rsel y\u00fckle (iste\u011fe ba\u011fl\u0131)',
          prompt: 'Metin promptu (iste\u011fe ba\u011fl\u0131)',
          promptPlaceholder: '\u00f6rn. \u00e7at\u0131rdayan kamp ate\u015fi',
          negativePrompt: 'Negatif prompt',
          duration: 'Süre (s)',
          maxDuration: 'Maks 8s (model sınırı)',
          cfg: 'CFG',
          steps: 'Adımlar',
          compareHint: 'Karşılaştır: Yalnızca metin - Görsel + Metin'
        },
        guidance: {
          explainWhatTitle: 'ImageBind Guidance ne yapar?',
          explainWhatText: 'ImageBind (Girdhar vd., CVPR 2023) alt\u0131 duyuyu \u2014 g\u00f6r\u00fcnt\u00fc, ses, metin, derinlik, s\u0131cakl\u0131k, hareket \u2014 ortak bir \u201cdile\u201d birle\u015ftirir. Bu ara\u00e7 bu ortak zemini kullan\u0131r: ses ad\u0131m ad\u0131m \u00fcretilirken s\u00fcrekli \u201cBu ses g\u00f6r\u00fcnt\u00fcye benziyor mu?\u201d diye sorar ve y\u00f6n\u00fc d\u00fczeltir. Sonu\u00e7taki kosin\u00fcs benzerli\u011fi, \u00fcretilen sesin g\u00f6rsel i\u00e7eri\u011fe ne kadar yakla\u015ft\u0131\u011f\u0131n\u0131 g\u00f6sterir.',
          explainHowTitle: 'Bu arac\u0131 nas\u0131l kullan\u0131r\u0131m?',
          explainHowText: 'Bir g\u00f6rsel y\u00fckleyin \u2014 bu sesin hedef y\u00f6n\u00fcn\u00fc belirler. \u0130ste\u011fe ba\u011fl\u0131: ek y\u00f6nlendirme i\u00e7in bir metin promptu. \u201c\u03bb Guidance Strength\u201d kayd\u0131r\u0131c\u0131s\u0131 en \u00f6nemli parametredir: d\u00fc\u015f\u00fck de\u011ferler (0.01\u20130.05) sese \u00e7ok \u00f6zg\u00fcrl\u00fck tan\u0131r, y\u00fcksek de\u011ferler (0.3\u20131.0) sesi g\u00f6rsele s\u0131k\u0131ca ba\u011flar. \u201cWarmup Steps\u201d g\u00f6rsel rehberli\u011fin hangi ad\u0131mda ba\u015flayaca\u011f\u0131n\u0131 belirler \u2014 d\u00fc\u015f\u00fck de\u011ferler hemen ba\u015flar, y\u00fcksek de\u011ferler temel yap\u0131n\u0131n \u00f6nce y\u00f6nlendirilmeden olu\u015fmas\u0131n\u0131 sa\u011flar. Total Steps ve Duration kaliteyi ve uzunlu\u011fu kontrol eder.',
          referencesTitle: 'Araştırma Kaynakları',
          imageUpload: 'Görsel yükle',
          prompt: 'Temel prompt (isteğe bağlı)',
          promptPlaceholder: 'örn. ortam ses manzarası',
          lambda: 'Rehberlik gücü',
          lambdaHint: 'Görselin ses üretimini ne kadar güçlü yönlendirdiği',
          warmupSteps: 'Isınma adımları',
          warmupHint: 'Gradyan rehberliği yalnızca ilk N adım boyunca',
          totalSteps: 'Toplam adımlar',
          duration: 'Süre (s)',
          cfg: 'CFG',
          totalStepsHint: 'Toplam gürültü giderme adımları. Daha fazla = daha yüksek kalite.',
          durationHint: 'Üretilen ses klibinin saniye cinsinden uzunluğu',
          cosineSimilarity: 'Kosinüs benzerliği (görsel-ses yakınlığı)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Biliyor muydunuz?',
        learnMore: '📚 Daha fazla bilgi',
        currentlyHappening: '⚡ Şu anda oluyor:',
        energyUsed: 'Kullanılan enerji',
        co2Produced: 'Üretilen CO₂'
      },
      energy: {
        kids_1: '💡 Yapay zeka görselleri elektrik harcıyor – telefonunuzu 3 saat şarj etmek kadar!',
        kids_2: '🔌 GPU, çok güç tüketen bir süper hesap makinesi gibidir!',
        kids_3: '⚡ Her görsel, bir LED ışığı 10 dakika yakmak için yeterli enerji gerektiriyor!',
        youth_1: '⚡ GPU oluştururken {watts}W kullanıyor – küçük bir elektrik ısıtıcısı gibi!',
        youth_2: '🔋 Bir görsel yaklaşık 0,01-0,02 kWh kullanıyor – az gibi görünüyor ama birikir!',
        youth_3: '🌡️ GPU şu an {temp}°C sıcaklığa ulaşıyor – bu yüzden soğutma gerekiyor!',
        expert_1: '📊 Anlık: {watts}W, %{util} kullanımda = şimdiye kadar {kwh} kWh',
        expert_2: '🔥 TDP sınırı: {tdp}W | Şu anki: {watts}W (sınırın %{percent}\'i)',
        expert_3: '💾 VRAM: {used}/{total} GB (%{percent}) – model + aktivasyonlar'
      },
      data: {
        kids_1: '🧮 GPU şu an 10 milyar kez hesaplıyor – sizin sayabileceğinizden daha hızlı!',
        kids_2: '🎨 Görsel 50 küçük adımda oluşturuluyor – kendi kendini çözen bir bulmaca gibi!',
        kids_3: '🧩 Şu an milyonlarca sayı GPU\'dan geçiyor!',
        youth_1: '🔄 Her görsel ~50 "gürültü giderme adımı"ndan geçiyor – gürültü gidermenin 50 turu!',
        youth_2: '📐 8 milyar parametre sorgulanıyor – görsel başına!',
        youth_3: '🧠 Yapay zeka binlerce boyutlu vektörler içinde "düşünüyor" – bir uzaydaki koordinatlar gibi.',
        expert_1: '🔬 MMDiT: Çok Modlu Difüzyon Transformatörü – birleşik dikkat katmanlarında metin + görsel',
        expert_2: '📈 Self-Attention: O(n²) karmaşıklığı – her token diğerlerini "görüyor"',
        expert_3: '⚙️ Classifier-Free Guidance: prompt etkisi ve yaratıcılık dengesi'
      },
      model: {
        kids_1: '🎓 Yapay zeka modeli nasıl resim yapacağını öğrenmek için milyonlarca görsele baktı!',
        kids_2: '🤖 Yapay zeka, gördüklerini asla unutmayan bir sanatçı gibidir!',
        kids_3: '✨ Modelde 8 milyar bağlantı var – gökyüzünde görebileceğiniz yıldızlardan daha fazla!',
        youth_1: '🧠 SD3.5 Large\'ın 8 milyar parametresi var – 8 milyar karar düğümü gibi.',
        youth_2: '📚 3 metin kodlayıcı birlikte çalışıyor: CLIP-L, CLIP-G ve T5-XXL',
        youth_3: '🔢 Model yalnızca yüklenmek için {vram} GB VRAM\'a ihtiyaç duyuyor!',
        expert_1: '🏗️ Mimari: 38 transformatör bloğuyla Rectified Flow + MMDiT',
        expert_2: '📊 FP16/FP8 nicemleme: hassasiyet ile VRAM dengesi',
        expert_3: '🔗 LoRA: Düşük Sıralı Uyarlama – yalnızca parametrelerin %0,1\'i yeniden eğitildi'
      },
      ethics: {
        kids_1: '🌍 Yapay zeka internetteki görsellerden öğrenir – bu yüzden diğer insanların sanatına karşı adil olmak önemlidir!',
        kids_2: '⚖️ Tüm sanatçılara yapay zekanın onlardan öğrenip öğrenemeyeceği sorulmadı.',
        kids_3: '🤝 İyi yapay zeka insanların çalışmalarına saygı gösterir!',
        youth_1: '📜 Eğitim verileri genellikle internetten gelir. Sanatçılar tartışıyor: Adil Kullanım mı yoksa kopyalama mı?',
        youth_2: '🏛️ AB Yapay Zeka Yasası şeffaflık talep eder: Eğitim verileri nereden geliyor?',
        youth_3: '💭 Soru: Yapay zeka tarafından oluşturulan bir görsele aslında kim sahip olur?',
        expert_1: '⚠️ LAION-5B kısmen yaratıcı rızası olmadan oluşturuldu – hukuki gri alan.',
        expert_2: '📋 AB Yapay Zeka Yasası Md. 52: Yapay zeka tarafından oluşturulan içerik için etiketleme zorunluluğu',
        expert_3: '🔍 Model Kartları & Veri Sayfaları: ML şeffaflığı için en iyi uygulama'
      },
      environment: {
        kids_1: '☁️ Her yapay zeka görseli biraz CO₂ üretir – arabayla sürüş gibi ama daha az!',
        kids_2: '🌱 Düşün: Bu görsel elektriğe değer mi?',
        kids_3: '🌞 Yapay zeka enerjisi genellikle güç santrallerinden gelir – bir kısmı temiz, bir kısmı değil.',
        youth_1: '🏭 Alman elektrik şebekesi: kWh başına ~400g CO₂ – bu birikir!',
        youth_2: '📈 Bu görsel için {co2}g CO₂ – 1000 görselde bu {totalKg} kg olur!',
        youth_3: '💡 İpucu: Daha az görsel oluşturun ama daha düşünceli – enerji ve CO₂ tasarrufu sağlar.',
        expert_1: '📊 Hesaplama: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Kapsam 2 emisyonları: veri merkezi konumu belirleyicidir',
        expert_3: '⚡ PUE (Güç Kullanım Etkinliği): Soğutma için ek enerji yükü'
      },
      iceberg: {
        drawPrompt: 'Yapay zeka üretimi çok fazla enerji kullanır. Buzdağları çizin ve ne olduğunu görün...',
        redraw: 'Yeniden Çiz',
        startMelting: 'Erimeyi başlat',
        melting: 'Buzdağı eriyor...',
        melted: 'Eridi!',
        meltedMessage: '{co2}g CO₂ üretildi',
        comparison: 'Bu CO₂ miktarı yaklaşık {volume} cm³ Arktik buzunu eritir.',
        comparisonInfo: '(Her ton CO₂ ≈ 6m³ deniz buzu kaybı)',
        gpuPower: 'Ekran kartı güç tüketimi',
        gpuTemp: 'Ekran kartı sıcaklığı',
        co2Info: 'Güç tüketiminden CO₂ emisyonları (Alman enerji karışımına göre)',
        drawAgain: 'Daha fazla buzdağı çizin...'
      },
      pixel: {
        grafikkarte: 'Ekran Kartı',
        energieverbrauch: 'Enerji Kullanımı',
        co2Menge: 'CO₂ Miktarı',
        smartphoneComparison: 'Bu CO₂ kullanımını telafi etmek için telefonunuzu {minutes} dakika kapalı tutmanız gerekirdi!',
        clickToProcess: 'Mini görsel oluşturmak için veri piksellerine tıklayın!'
      },
      forest: {
        trees: 'Ağaçlar',
        clickToPlant: 'Ağaç dikmek için tıklayın! Ağaç diktiğiniz yerde fabrika kaybolacak.',
        gameOver: 'Orman kayboldu!',
        treesPlanted: '{count} ağaç diktiniz.',
        complete: 'Oluşturma tamamlandı',
        comparison: 'Ortalama bir ağacın bu CO₂ miktarını absorbe etmesi {minutes} dakika gerektirir.'
      },
      rareearth: {
        clickToClean: 'Zehirli çamuru temizlemek için gölü tıklayın!',
        sludgeRemoved: 'Çamur temizlendi',
        environmentHealth: 'Çevre',
        gameOverInactive: 'Vazgeçtiniz... madencilik devam ediyor',
        infoBanner: 'GPU çipleri için nadir toprak madenciliği zehirli çamur bırakır ve ekosistemleri yok eder. Temizleme çabalarınız çıkarım hızına yetişemez.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Çevre',
        statsSludge: 'Çamur temizlendi'
      }
    }

  },
  ko: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: '창의적 AI 변환'
    },
    form: {
      inputLabel: '텍스트 입력',
      inputPlaceholder: '예: 초원 위의 꽃',
      schemaLabel: '변환 스타일',
      executeModeLabel: '실행 모드',
      safetyLabel: '안전 수준',
      generateButton: '생성'
    },
    schemas: {
      dada: 'Dada (랜덤 & 부조리)',
      bauhaus: 'Bauhaus (기하학적)',
      stillepost: 'Stille Post (반복적)'
    },
    executionModes: {
      eco: 'Eco (빠름)',
      fast: 'Fast (균형)',
      best: 'Best (품질)'
    },
    safetyLevels: {
      kids: '어린이',
      youth: '청소년',
      adult: '성인',
      research: '연구'
    },
    stages: {
      pipeline_starting: '파이프라인 시작',
      translation_and_safety: '번역 & 안전 검사',
      interception: '변환',
      pre_output_safety: '출력 안전 검사',
      media_generation: '이미지 생성',
      completed: '완료'
    },
    status: {
      idle: '준비',
      executing: '파이프라인 실행 중...',
      connectionSlow: '연결이 느립니다. 재시도 중...',
      completed: '파이프라인 완료!',
      error: '오류 발생'
    },
    entities: {
      input: '입력',
      translation: '번역',
      safety: '안전 검사',
      interception: '변환',
      safety_pre_output: '출력 안전 검사',
      media: '생성된 이미지'
    },
    properties: {
      chill: '차분한',
      chaotic: '격렬한',
      narrative: '이야기 들려주기',
      algorithmic: '규칙 따르기',
      historical: '역사',
      contemporary: '현재',
      explore: 'AI 테스트',
      create: '예술 만들기',
      playful: '장난스러운',
      serious: '진지한'
    },
    phase2: {
      title: '프롬프트 입력',
      userInput: '입력',
      yourInput: '입력',
      yourIdea: '아이디어: 무엇에 대한 것인가요?',
      rules: '규칙: 어떻게 구현할까요?',
      yourInstructions: '지시사항',
      what: '무엇',
      how: '어떻게',
      userInputPlaceholder: '예: 초원 위의 꽃',
      inputPlaceholder: '텍스트가 여기에 표시됩니다...',
      metaPrompt: '예술적 지시',
      instruction: '지시',
      transformation: '예술적 변환',
      metaPromptPlaceholder: '변환을 설명하세요...',
      result: '결과',
      expectedResult: '예상 결과',
      execute: '파이프라인 실행',
      executing: '실행 중...',
      transforming: 'LLM 변환 중...',
      startTransformation: '변환 시작',
      letsGo: '좋아요, 시작합시다!',
      modified: '수정됨',
      reset: '초기화',
      loadingConfig: '설정 불러오는 중...',
      loadingMetaPrompt: '메타 프롬프트 불러오는 중...',
      errorLoadingConfig: '설정 불러오기 오류',
      errorLoadingMetaPrompt: '메타 프롬프트 불러오기 오류',
      threeForces: '함께 작용하는 3가지 힘',
      twoForces: '무엇 + 어떻게 → LLM → 결과',
      yourPrompt: '프롬프트:',
      writeYourText: '텍스트를 작성하세요...',
      examples: '예시',
      estimatedTime: '~12초',
      stage12Time: '~5-10초',
      willAppearAfterExecution: '실행 후 표시됩니다...',
      back: '뒤로',
      retry: '재시도',
      transformedPrompt: '변환된 프롬프트',
      notYetTransformed: '아직 변환되지 않음...',
      transform: '변환',
      reTransform: '다시 시도',
      startAI: 'AI, 입력을 처리하세요',
      aiWorking: 'AI가 작업 중입니다...',
      continueToMedia: '이미지 생성으로 계속',
      readyForMedia: '이미지 생성 준비 완료',
      stage1: '1단계: 번역 + 안전 검사...',
      stage2: '2단계: 변환...',
      selectMedia: '매체를 선택하세요:',
      mediaImage: '이미지',
      mediaAudio: '오디오',
      mediaVideo: '비디오',
      media3D: '3D',
      comingSoon: '곧 출시',
      generateMedia: '시작!'
    },
    phase3: {
      generating: '이미지 생성 중...',
      generatingHint: '~30초'
    },
    common: {
      back: '뒤로',
      loading: '불러오는 중...',
      error: '오류',
      retry: '재시도',
      cancel: '취소',
      checkingSafety: '확인 중...'
    },
    gallery: {
      title: '즐겨찾기',
      empty: '아직 즐겨찾기가 없습니다',
      favorite: '즐겨찾기에 추가',
      unfavorite: '즐겨찾기에서 제거',
      continue: '편집 계속',
      restore: '세션 복원',
      viewMine: '내 즐겨찾기',
      viewAll: '전체 즐겨찾기'
    },
    settings: {
      authRequired: '인증 필요',
      authPrompt: '설정에 접근하려면 비밀번호를 입력하세요:',
      passwordPlaceholder: '비밀번호 입력...',
      authenticate: '로그인',
      authenticating: '인증 중...',
      title: '관리',
      tabs: {
        export: '연구 데이터',
        config: '설정',
        demos: '미니게임 데모',
        matrix: '모델 매트릭스'
      },
      loading: '설정 불러오는 중...',
      presets: {
        title: '모델 프리셋',
        help: '<strong>모델 매트릭스</strong> 탭을 사용하여 모든 사용 가능한 프리셋을 보고 한 번의 클릭으로 적용하세요.',
        openMatrix: '모델 매트릭스 열기'
      },
      testingTools: {
        title: '교육자를 위한 테스트 도구',
        help: '학습자와 함께 사용하기 전에 교육용 미니게임과 애니메이션을 테스트하고 탐색하세요.',
        openPreview: '미니게임 미리보기 열기',
        pixelEditor: '픽셀 템플릿 편집기',
        includes: '포함: 픽셀 애니메이션, 빙산 녹이기, 숲 게임, 희토류'
      },
      general: {
        title: '일반 설정',
        uiMode: 'UI 모드',
        uiModeHelp: '인터페이스 복잡도 수준',
        kids: '어린이 (8-12세)',
        youth: '청소년 (13-17세)',
        expert: '전문가',
        safetyLevel: '안전 수준',
        defaultLanguage: '기본 언어',
        germanDe: '독일어 (de)',
        englishEn: '영어 (en)',
        turkishTr: '터키어 (tr)',
        koreanKo: '한국어 (ko)',
        ukrainianUk: 'Українська (uk)',
        frenchFr: '프랑스어 (fr)'
      },
      safety: {
        kidsTitle: '어린이 (8-12세)',
        kidsDesc: '모든 필터 활성: §86a, 개인정보보호법, 청소년 보호 (연령 맞춤 매개변수), VLM 이미지 검사',
        youthTitle: '청소년 (13-17세)',
        youthDesc: '모든 필터 활성: §86a, 개인정보보호법, 청소년 보호 (청소년 매개변수), VLM 이미지 검사',
        adultTitle: '성인',
        adultDesc: '§86a + 개인정보보호법 활성. 청소년 보호 없음, VLM 이미지 검사 없음.',
        researchTitle: '연구 모드',
        researchDesc: '안전 필터 없음. 연구 기관의 과학 연구 프로젝트 맥락에서만 사용이 허가됩니다.'
      },
      safetyModels: {
        title: '로컬 안전 모델',
        help: 'Ollama를 통한 로컬 — 개인 이름과 안전 검사가 시스템을 벗어나지 않습니다',
        safetyModel: '안전 모델',
        safetyModelHelp: '콘텐츠 안전을 위한 가드 모델 (§86a, 청소년 보호)',
        dsgvoModel: '개인정보보호법 검증 모델',
        dsgvoModelHelp: '개인정보보호법 NER 검증을 위한 범용 모델 (가드 모델 아님)',
        vlmModel: 'VLM 안전 모델',
        vlmModelHelp: '생성 후 이미지 안전 검사를 위한 비전 모델 (어린이/청소년)',
        fast: '빠름, 최소',
        recommended: '권장'
      },
      dsgvo: {
        title: '개인정보보호법 경고',
        notCompliant: '다음 모델은 <strong>개인정보보호법을 준수하지 않습니다</strong> (EU 외부에서 데이터 처리):',
        compliantHint: '개인정보보호법 준수 옵션:'
      },
      models: {
        title: '모델 설정',
        help: '프로바이더 접두사가 있는 모델 식별자: local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: '모델 매트릭스 사용을 권장합니다. 그러나 여기에서 자유롭게 설정할 수 있습니다.',
        ollamaAvailable: '{count}개의 Ollama 모델 사용 가능 (입력하거나 드롭다운에서 선택)',
        stage1Text: '1단계 - 텍스트 모델',
        stage1Vision: '1단계 - 비전 모델',
        stage2Interception: '2단계 - 인터셉션 모델',
        stage2Optimization: '2단계 - 최적화 모델',
        stage3: '3단계 - 번역/안전 모델',
        stage4Legacy: '4단계 - 레거시 모델',
        chatHelper: '채팅 도우미 모델',
        imageAnalysis: '이미지 분석 모델',
        coding: '코드 생성 (Tone.js, p5.js)'
      },
      api: {
        title: 'API 설정',
        llmProvider: 'LLM 프로바이더',
        localFramework: '로컬 LLM 프레임워크',
        externalProvider: '외부 LLM 프로바이더',
        cloudProvider: '클라우드 LLM 프로바이더 - API 키 필요',
        noneLocal: '없음 (로컬 전용, 개인정보보호법)',
        mistralEu: 'Mistral AI (EU 기반, 개인정보보호법)',
        anthropicDirect: 'Anthropic Direct API (개인정보보호법 미준수)',
        openaiDirect: 'OpenAI Direct API (개인정보보호법 미준수)',
        openrouterDirect: 'OpenRouter (개인정보보호법 미준수, EU 라우팅 가능)',
        mistralInfo: 'Mistral AI (EU 기반)',
        mistralDsgvo: '개인정보보호법 준수 (EU 인프라)',
        anthropicInfo: 'Anthropic Direct API',
        anthropicNotDsgvo: '개인정보보호법 미준수',
        anthropicWarning: 'EU 외부에서 데이터 처리. 비교육적 맥락에서만 사용하세요.',
        openaiInfo: 'OpenAI Direct API',
        openaiNotDsgvo: '개인정보보호법 미준수 (미국 기반)',
        openaiWarning: '미국에서 데이터 처리. 비교육적 맥락에서만 사용하세요.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: '개인정보보호법 미준수 (미국 기업)',
        openrouterWarning: 'OpenRouter 설정에서 EU 서버 라우팅 구성 가능, 단 회사는 미국 기반입니다.',
        storedIn: '저장 위치',
        currentKey: '현재'
      },
      save: {
        saveApply: '저장 및 적용',
        saving: '저장 중...',
        applying: '적용 중...',
        success: '설정이 저장 및 적용되었습니다',
        presetApplied: '적용된 프리셋: {preset}'
      }
    },
    pipeline: {
      yourInput: '입력',
      result: '결과',
      generatedMedia: '생성된 이미지'
    },
    landing: {
      subtitlePrefix: 'UNESCO 디지털 문화 및 예술 교육 석좌교수의 교육적-예술적 실험 플랫폼',
      subtitleSuffix: '문화-미학적 미디어 교육에서 생성 AI의 탐구적 활용을 위한',
      research: '',
      features: {
        textTransformation: {
          title: '텍스트 변환',
          description: 'AI를 통한 관점 전환 — 프롬프트가 예술적-교육적 렌즈를 통해 이미지, 비디오, 사운드로 변환됩니다.'
        },
        imageTransformation: {
          title: '이미지 변환',
          description: '다양한 모델과 관점을 통해 이미지를 새로운 이미지와 비디오로 변환합니다.'
        },
        multiImage: {
          title: '이미지 융합',
          description: '여러 이미지를 결합하고 AI 모델을 통해 새로운 이미지 합성물로 병합합니다.'
        },
        canvas: {
          title: '캔버스 워크플로우',
          description: '시각적 워크플로우 구성 — 드래그 앤 드롭으로 모듈을 연결하여 맞춤 AI 파이프라인을 만듭니다.'
        },
        music: {
          title: '음악 생성',
          description: '가사, 태그, 스타일 제어를 통한 AI 기반 음악 창작.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: '벡터 공간 연구 — 초현실화, 차원 제거, 임베딩 보간.'
        }
      }
    },
    research: {
      locked: '연구 모드에서만 사용 가능',
      lockedHint: '안전 수준 "성인" 또는 "연구" 필요 (config.py)',
      complianceTitle: '연구 모드 안내',
      complianceWarning: '연구 모드에서는 프롬프트나 생성된 이미지에 대한 안전 필터가 활성화되지 않습니다. 예상치 못한 또는 부적절한 결과가 발생할 수 있습니다.',
      complianceAge: '이 모드는 16세 미만에게 권장되지 않습니다.',
      complianceConfirm: '안내 사항을 이해했음을 확인합니다',
      complianceCancel: '취소',
      complianceProceed: '계속'
    },
    presetOverlay: {
      title: '관점 선택',
      close: '닫기'
    },
    imageUpload: {
      clickHere: '여기를 클릭',
      orDragImage: '하거나 이미지를 끌어다 놓으세요',
      formatHint: 'PNG, JPG, WEBP (최대 10MB)',
      invalidFormat: '잘못된 파일 형식입니다. PNG, JPG, WEBP만 허용됩니다.',
      fileTooLarge: '파일이 너무 큽니다. 최대: {max}MB',
      uploadFailed: '업로드 실패',
      infoOriginal: '원본:',
      infoSize: '크기:'
    },
    mediaInput: {
      choosePreset: '관점 선택',
      translateToEnglish: '영어로 번역',
      copy: '복사',
      paste: '붙여넣기',
      delete: '삭제',
      loading: '불러오는 중...',
      contentBlocked: '콘텐츠 차단됨'
    },
    nav: {
      about: '소개',
      impressum: '법적 고지',
      privacy: '개인정보',
      docs: '문서',
      language: '언어 변경',
      settings: '설정',
      canvas: '캔버스 워크플로우'
    },
    canvas: {
      title: '캔버스 워크플로우',
      newWorkflow: '새 워크플로우',
      importWorkflow: '가져오기',
      exportWorkflow: '내보내기',
      execute: '실행',
      ready: '준비',
      errors: '오류',
      discardWorkflow: '현재 워크플로우를 삭제하시겠습니까?',
      importError: '파일 가져오기 실패',
      selectTransformation: '변환 선택',
      selectOutput: '출력 모델 선택',
      search: '검색...',
      noResults: '결과를 찾을 수 없습니다',
      dragHint: '모듈을 클릭하거나 캔버스로 끌어다 놓으세요',
      editNameHint: '(더블클릭하여 편집)',
      modules: '모듈',
      toggleSidebar: '사이드바 전환',
      dsgvoTooltip: '캔버스 워크플로우는 외부 LLM API를 사용할 수 있습니다. 개인정보보호법 준수는 사용자의 책임입니다.',
      batchExecute: '일괄 실행',
      batchExecution: '일괄 실행',
      batchAbort: '일괄 중단',
      abort: '중단',
      cancel: '취소',
      loading: '불러오는 중...',
      executingWorkflow: '워크플로우 실행 중...',
      starting: '시작 중...',
      nodes: '노드',
      batchRunCount: '실행 횟수',
      batchUseSeed: '기본 시드 사용',
      batchBaseSeed: '기본 시드',
      batchSeedHint: '각 실행: 시드 + 인덱스',
      batchStart: '일괄 시작',
      stage: {
        configSelectPlaceholder: '선택...',
        evaluationCriteriaFallback: '평가 기준...',
        feedbackInputTitle: '피드백 입력',
        deleteTitle: '삭제',
        selectLlmPlaceholder: 'LLM 선택...',
        resizeTitle: '크기 조절',
        input: {
          promptPlaceholder: '프롬프트...'
        },
        imageInput: {
          uploadLabel: '이미지 업로드'
        },
        interception: {
          contextPromptLabel: '컨텍스트 프롬프트',
          contextPromptPlaceholder: '변환 지시사항...'
        },
        translation: {
          translationPromptLabel: '번역 프롬프트',
          translationPromptPlaceholder: '번역 지시사항...'
        },
        modelAdaption: {
          targetModelLabel: '대상 모델',
          noAdaptionOption: '적응 없음',
          videoModelsOption: '비디오 모델',
          audioModelsOption: '오디오 모델'
        },
        comparisonEvaluator: {
          criteriaLabel: '비교 기준',
          criteriaPlaceholder: '예: 독창성, 명확성, 세부사항으로 비교...',
          infoText: '최대 3개의 텍스트 출력 연결'
        },
        seed: {
          modeLabel: '모드',
          modeFixed: '고정',
          modeRandom: '랜덤',
          valueLabel: '값',
          baseLabel: '기본값'
        },
        resolution: {
          customOption: '사용자 정의',
          widthLabel: '너비',
          heightLabel: '높이'
        },
        collector: {
          emptyText: '실행 대기 중...'
        },
        evaluation: {
          typeLabel: '평가 유형',
          typeCreativity: '창의성',
          typeQuality: '품질',
          typeCustom: '사용자 정의',
          criteriaLabel: '평가 기준',
          outputTypeLabel: '출력 유형',
          outputCommentary: '코멘트 + 이진',
          outputScore: '코멘트 + 점수 + 이진',
          outputAll: '전체',
          evalPassTitle: '통과 (전달)',
          evalFailTitle: '피드백 (역방향)',
          evalCommentaryTitle: '코멘트 (전달)'
        },
        imageEvaluation: {
          visionModelPlaceholder: '비전 모델 선택...',
          frameworkLabel: '분석 프레임워크',
          frameworkPanofsky: '미술사적 (Panofsky)',
          frameworkEducational: '교육 이론',
          frameworkEthical: '윤리적',
          frameworkCritical: '비판적/탈식민주의적',
          frameworkCustom: '사용자 정의',
          customPromptLabel: '분석 프롬프트',
          customPromptPlaceholder: '이미지 분석 방법을 설명하세요...'
        },
        display: {
          imageAlt: '미리보기',
          emptyText: '미리보기 (실행 후)'
        }
      }
    },
    about: {
      title: 'UCDCAE AI LAB 소개',
      intro: 'UCDCAE AI LAB은 문화-미학적 미디어 교육에서 생성 인공지능의 탐구적 활용을 위한 UNESCO 디지털 문화 및 예술 교육 석좌교수의 교육적-예술적 실험 플랫폼입니다. AI4ArtsEd 및 COMeARTS 프로젝트 내에서 개발되었습니다.',
      project: {
        title: '프로젝트',
        description: 'AI는 사회와 노동 세계를 변화시키고 있으며, 점점 더 교육의 주제가 되고 있습니다. 이 프로젝트는 문화 교육의 문화적 다양성에 민감한 환경에서 인공지능(AI)의 교육적 활용의 기회, 조건 및 한계를 탐구합니다.',
        paragraph2: '일반 교육학(TPap), 컴퓨터 과학(TPinf), 미술 교육(TPkp)의 세 하위 프로젝트에서 창의성 지향적 교육적 AI 실천 연구와 컴퓨터 과학 AI 구상 및 프로그래밍이 긴밀한 협력 속에서 서로 맞물립니다.',
        paragraph3: '약 2년에 걸친 참여적 디자인 프로세스는 유리한 현실 조건 하에서 AI 시스템이 이미 구조적 수준에서 예술적-교육적 원칙을 어느 정도 포함할 수 있는지를 탐구하는 오픈소스 AI 기술을 생산하는 것을 목표로 합니다.',
        paragraph4: '초점은 a) 문화 교육을 위한 고도로 혁신적인 기술의 미래 적용 가능성 및 부가가치, b) 교사와 학습자의 AI 리터러시 범위와 한계, c) 교육적 윤리 및 기술 평가 측면에서 복잡한 비인간 행위자에 의한 교육적 환경 변환의 평가 가능성에 대한 포괄적 질문에 있습니다.',
        moreInfo: '추가 정보:'
      },
      subproject: {
        title: '하위 프로젝트 "일반 교육학"',
        description: '하위 프로젝트 "일반 교육학"은 공동 연구 프로젝트의 공동 연구 질문 틀 내에서 참여적 실천 연구를 기반으로 예술적-교육적 AI 디자인 프로세스의 가능성과 한계를 연구합니다.'
      },
      team: {
        title: '팀',
        projectLead: '프로젝트 책임자',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: '교육학 연구소',
        leadChair: '문화 및 미학 교육 중점 교육학 석좌교수',
        leadUnesco: 'UNESCO 디지털 문화 및 예술 교육 석좌교수',
        researcher: '연구원',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: '교육학 연구소',
        researcherChair: '문화 및 미학 교육 중점 교육학 석좌교수',
        researcherUnesco: 'UNESCO 디지털 문화 및 예술 교육 석좌교수'
      },
      funding: {
        title: '후원'
      }
    },
    legal: {
      impressum: {
        title: '법적 고지',
        publisher: '발행자',
        represented: '총장이 대표함',
        responsible: '콘텐츠 책임자',
        authority: '감독 기관',
        moreInfo: '추가 정보',
        moreInfoText: 'FAU의 전체 법적 고지:',
        funding: '후원'
      },
      privacy: {
        title: '개인정보 보호정책',
        notice: '안내: 생성된 콘텐츠는 연구 목적으로 서버에 저장됩니다. 사용자 또는 IP 데이터는 수집되지 않습니다. 업로드된 이미지는 저장되지 않습니다.',
        usage: '이 플랫폼의 사용은 UCDCAE AI LAB의 등록된 협력 파트너에게만 허용됩니다. 이 맥락에서 합의된 데이터 보호 계약이 적용됩니다. 질문이 있으시면 vanessa.baumann@fau.de로 연락하세요.'
      }
    },
    docs: {
      title: '문서 및 가이드',
      intro: {
        title: '환영합니다',
        content: 'AI 변환을 통한 창의적 실험.'
      },
      gettingStarted: {
        title: '시작하기',
        step1: '사분면에서 속성 선택',
        step2: '텍스트 또는 이미지 입력',
        step3: '변환 시작'
      },
      modes: {
        title: '모드',
        mode1: { name: '직접', desc: '빠른 실험' },
        mode2: { name: '텍스트', desc: '텍스트 기반 변환' },
        mode3: { name: '이미지', desc: '이미지 기반 절차' }
      },
      support: {
        title: '지원',
        content: '질문은:'
      },
      wikipedia: {
        title: '위키피디아 연구',
        subtitle: '예술적 과정의 일부로서의 세계에 대한 지식',
        feature: '예술적 과정에는 미적 지식뿐만 아니라 세계의 사실에 대한 지식도 필요합니다. AI는 변환 중에 위키피디아를 검색하여 사실 정보를 찾습니다.',
        languages: '70개 이상의 언어가 지원됩니다',
        languagesDesc: 'AI가 각 주제에 적합한 언어의 위키피디아를 자동으로 선택합니다:',
        examples: {
          nigeria: '나이지리아 주제 → 하우사어, 요루바어, 이그보어 또는 영어',
          india: '인도 주제 → 힌디어, 타밀어, 벵골어 또는 기타 지역 언어',
          indigenous: '원주민 문화 → 케추아어, 마오리어, 이누크티투트어 등'
        },
        why: '투명성: AI는 무엇을 알고 있나요?',
        whyDesc: '시스템은 모든 연구 시도를 표시합니다: 발견된 문서(클릭 가능한 링크)와 아무것도 발견되지 않은 용어 모두. 이를 통해 AI가 무엇을 안다고 생각하는지 — 그리고 무엇을 모르는지가 드러납니다.',
        culturalRespect: '직접 연구하도록 초대',
        culturalRespectDesc: '표시된 위키피디아 링크는 스스로 더 알아보도록 하는 초대입니다. 링크를 클릭하여 출처를 확인하고 자신의 지식을 넓히세요.',
        limitations: 'AI 연구는 주제에 대한 자체적인 탐구를 대체하는 것이 아닌 보조 수단입니다.'
      }
    },
    multiImage: {
      image1Label: '이미지 1',
      image2Label: '이미지 2 (선택)',
      image3Label: '이미지 3 (선택)',
      contextLabel: '이미지로 무엇을 하고 싶은지 설명하세요',
      contextPlaceholder: '예: 이미지 2의 집과 이미지 3의 말을 이미지 1에 삽입하세요. 이미지 1의 색상과 스타일을 유지하세요.',
      modeTitle: '여러 이미지 → 이미지',
      selectConfig: '모델을 선택하세요:',
      generating: '이미지 융합 중...'
    },
    imageTransform: {
      imageLabel: '이미지',
      contextLabel: '이미지에서 변경하고 싶은 것을 설명하세요',
      contextPlaceholder: '예: 유화로 변환... 더 다채롭게... 석양 추가...'
    },
    videoGeneration: {
      promptLabel: '비디오 아이디어',
      promptPlaceholder: '예: 석양 무렵 산 풍경 위를 떠다니는 열기구...',
      modelLabel: '비디오 모델을 선택하세요:',
      generating: '비디오 생성 중...'
    },
    textTransform: {
      inputLabel: '아이디어 = 무엇?',
      inputTooltip: '창작물의 주제를 입력하세요.',
      inputPlaceholder: '예: 우리 거리의 축제: ...',
      contextLabel: '규칙 = 어떻게?',
      contextTooltip: '아이디어를 어떻게 표현할지 입력하거나, 원 아이콘을 클릭하세요!',
      contextPlaceholder: '예: 나무 위의 새들이 인지하는 대로 모든 것을 묘사하세요!',
      resultLabel: '아이디어 + 규칙 = 프롬프트',
      resultPlaceholder: '시작 클릭 후 프롬프트가 표시됩니다 (또는 직접 텍스트 입력)',
      optimizedLabel: '모델 최적화 프롬프트',
      optimizedPlaceholder: '모델 선택 후 최적화된 프롬프트가 표시됩니다.'
    },
    training: {
      info: {
        title: 'LoRA 학습 정보',
        studioDescription: '자체 이미지로 Stable Diffusion 3.5 Large용 맞춤 LoRA 모델을 학습합니다.',
        description: '이 내장 학습은 빠른 테스트를 위해 설계되었습니다.',
        limitations: '제한사항',
        limitationDuration: '학습 시간 1-3시간',
        limitationBlocking: '학습 중 이미지 생성 차단',
        limitationConfig: '제한된 설정 옵션',
        showMore: '더 알아보기',
        showLess: '접기'
      },
      placeholders: {
        projectName: '예: 우리 학교 건물',
        triggerWords: '예: our_school_building, schoolyard, classroom'
      },
      labels: {
        projectName: '프로젝트 이름',
        triggerWords: '트리거 단어',
        triggerHelp: '쉼표로 구분된 태그. 첫 번째 = 기본 트리거, 나머지 = 이미지당 추가 태그.',
        images: '학습 이미지 (10-50개 권장)',
        dropZone: '클릭하거나 이미지를 여기에 놓으세요',
        imagesSelected: '{count}개 이미지 선택됨',
        logs: '학습 로그',
        waiting: '학습 시작 대기 중...'
      },
      buttons: {
        start: '학습 시작',
        stop: '중지',
        inProgress: '학습 진행 중...',
        delete: '프로젝트 파일 삭제 (개인정보보호법)',
        cancel: '취소'
      },
      vram: {
        title: 'GPU VRAM 검사',
        checking: 'VRAM 확인 중...',
        used: '사용됨',
        free: '여유',
        notEnough: '학습에 충분한 여유 VRAM이 없습니다 ({gb} GB 필요).',
        clearQuestion: 'VRAM을 정리하고 계속하시겠습니까?',
        enough: '학습에 충분한 VRAM을 사용할 수 있습니다.',
        clearing: 'VRAM 정리 중...',
        newFree: '새 여유',
        clearBtn: 'ComfyUI + Ollama VRAM 정리'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: '연령 필터',
      dsgvo_ner: '개인정보보호법',
      dsgvo_llm: '개인정보보호법',
      translation: '\u2192 EN',
      fast_filter: '콘텐츠',
      llm_context_check: '콘텐츠 (LLM)',
      llm_safety_check: '청소년 보호',
      llm_check_failed: '검사 실패',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: '프롬프트는 괜찮았지만, 생성된 이미지가 이미지 분석 AI에 의해 부적절하다고 판단되었습니다. 이런 일은 발생할 수 있습니다 — 이미지 생성은 항상 예측 가능하지 않습니다. 다시 시도해 보세요, 매번 생성 결과가 다릅니다!',
      para86a: '프롬프트가 독일 법률(§86a StGB)에 의해 금지된 기호 또는 용어를 포함하고 있어 차단되었습니다. 이 규칙은 혐오와 폭력으로부터 우리 모두를 보호합니다. 다른 주제를 시도해 보세요!',
      dsgvo: '프롬프트가 사람 이름처럼 보이는 내용을 포함하고 있어 차단되었습니다. 이것은 개인정보보호법(GDPR)에 의해 보호됩니다. 이름 대신 "한 소녀" 또는 "한 노인"과 같은 설명을 사용하세요.',
      kids: '프롬프트가 어린이 안전 필터에 의해 차단되었습니다. 일부 용어는 무섭거나 불안할 수 있어 어린이에게 적합하지 않습니다. 더 친근한 단어로 아이디어를 설명해 보세요!',
      youth: '프롬프트가 청소년 보호 필터에 의해 차단되었습니다. 일부 콘텐츠는 청소년에게도 적합하지 않습니다. 아이디어를 다시 표현해 보세요!',
      generic: '프롬프트가 안전 시스템에 의해 차단되었습니다. 시스템은 부적절한 콘텐츠로부터 여러분을 보호합니다. 다른 표현을 시도해 보세요!',
      inputImage: '업로드된 이미지가 이미지 분석 AI에 의해 부적절하다고 판단되었습니다. 다른 이미지를 사용해 주세요.',
      vlmSaw: '이미지 AI가 본 것',
      systemUnavailable: '안전 시스템(Ollama)이 응답하지 않아 추가 처리가 불가능합니다. 시스템 관리자에게 문의해 주세요.',
      suggestionLoading: '잠깐만요, 아이디어가 있어요...',
      suggestionError: '지금은 제안을 생성할 수 없습니다. 다른 단어로 다시 시도해 보세요!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine - 의미 벡터 융합',
      infoDescription: '이 워크플로우는 의미 벡터 수준에서 두 프롬프트를 융합합니다. 결과는 단순한 혼합이 아닌 의미 공간의 더 깊은 수학적 연결입니다.',
      purposeTitle: '교육적 목적',
      purposeText: 'AI 모델이 의미를 수치 공간으로 어떻게 표현하는지 탐구합니다. 다른 개념을 수학적으로 병합하면 어떻게 되나요?',
      techTitle: '기술 세부사항',
      techText: '모델: SD3.5 Large | 인코더: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Partial Elimination - 벡터 해체',
      infoDescription: '이 워크플로우는 의미 벡터의 일부를 구체적으로 조작합니다. 특정 차원을 제거하여 의미의 어떤 측면이 사라지는지 관찰할 수 있습니다.',
      purposeTitle: '교육적 목적',
      purposeText: '의미가 벡터 공간의 다른 차원에 걸쳐 어떻게 인코딩되는지 이해합니다. 일부를 "끄면" 무엇이 남나요?',
      techTitle: '기술 세부사항',
      techText: '모델: SD3.5 Large | 인코더: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: '텍스트 인코더',
      modeLabel: '제거 모드',
      dimensionRange: '차원 범위',
      selected: '선택됨',
      dimensions: '차원',
      emptyTitle: '생성 대기 중...',
      emptySubtitle: '결과가 여기에 표시됩니다',
      referenceLabel: '참조 이미지',
      referenceDesc: '조작되지 않은 출력 (원본)',
      innerLabel: '내부 범위 제거됨',
      outerLabel: '외부 범위 제거됨'
    },
    surrealizer: {
      infoTitle: '초현실화기 — 알려진 것을 넘어선 외삽',
      infoDescription: '두 개의 AI "두뇌"가 텍스트를 읽습니다: CLIP-L은 이미지를 통해 언어를 이해하고, T5는 순수하게 언어적으로 이해합니다. 슬라이더는 단순히 혼합하지 않고 — T5만으로 생성할 수 있는 것을 훨씬 넘어서 이미지를 밀어냅니다. AI는 학습 중에 한 번도 접하지 못한 벡터를 해석해야 합니다. 결과: AI 환각 — 어떤 프롬프트로도 직접 생성할 수 없는 이미지.',
      purposeTitle: '슬라이더',
      purposeText: 'α < 0: CLIP-L 증폭, T5 부정 — 상위 3328차원(CLIP-L이 제로패딩된 곳)이 반전된 T5 벡터를 받습니다. 트랜스포머의 교차 어텐션 패턴이 뒤집힙니다: 시각 주도 환각. ◆ α = 0: 순수 CLIP-L — 일반 이미지. ◆ α = 1: 순수 T5-XXL — 여전히 일반적이지만 다른 품질. ◆ α > 1: T5를 넘어선 외삽. α = 20에서 공식은 임베딩을 T5 너머 탐험되지 않은 벡터 공간으로 19배 밀어냅니다 — 언어 주도 환각. ◆ 최적 범위: α = 15-35.',
      techTitle: '작동 방식',
      techText: '프롬프트가 두 인코더를 통해 개별적으로 전송됩니다: CLIP-L (시각적으로 학습, 77토큰, 768차원 → 4096으로 패딩) 및 T5-XXL (언어적으로 학습, 512토큰, 4096차원). 처음 77개 토큰 위치가 융합됩니다: (1-α)·CLIP-L + α·T5. 나머지 T5 토큰(78-512)은 의미적 앵커로 변경 없이 유지됩니다 — α가 아무리 극단적이어도 이미지를 텍스트에 연결합니다. α > 1에서 이것은 혼합이 아닌 외삽입니다: 어떤 학습도 생성하지 못한 벡터. α < 0에서 T5가 부정되고 CLIP-L이 증폭됩니다 — 트랜스포머의 교차 어텐션 패턴이 반전되므로 질적으로 다른 환각.',
      sliderLabel: '외삽 (α)',
      sliderNormal: '일반',
      sliderWeird: '이상한',
      sliderCrazy: '미친',
      sliderExtremeWeird: '매우 이상한',
      sliderExtremeCrazy: '매우 미친',
      sliderHint: "α<0: CLIP 너머 {'|'} α=0: 순수 CLIP {'|'} α=1: 순수 T5 {'|'} α>1: T5 너머",
      expandLabel: 'T5용 프롬프트 확장',
      expandSuggest: '짧은 프롬프트 감지 — T5 확장이 적은 단어로 결과를 크게 개선합니다.',
      expandHint: '프롬프트의 단어 수가 적습니다 (~{count} CLIP 토큰). 최적의 환각을 위해 AI가 T5 컨텍스트를 서사적으로 확장할 수 있습니다.',
      expandActive: '프롬프트 확장 중...',
      expandResultLabel: 'T5 확장 (T5 인코더만)',
      advancedLabel: '고급 설정',
      negativeLabel: '네거티브 프롬프트',
      negativeHint: '동일한 α로 외삽됩니다. 이미지가 무엇으로부터 멀어지는지를 결정합니다 — 다른 네거티브는 근본적으로 다른 미학을 생성합니다.',
      cfgLabel: 'CFG 스케일',
      cfgHint: 'Classifier-Free Guidance: 프롬프트 영향의 강도. 높을수록 = 더 강한 효과, 적은 변화.'
    },
    musicGeneration: {
      infoTitle: '음악 생성',
      infoDescription: '텍스트와 스타일 태그로 음악을 만듭니다. AI가 가사와 장르 사양에 기반하여 멜로디, 리듬, 화성을 생성합니다.',
      purposeTitle: '교육적 목적',
      purposeText: 'AI가 음악적 개념을 어떻게 해석하는지 탐구합니다. 가사의 단어 선택이 멜로디에 어떤 영향을 미치나요?',
      lyricsLabel: '가사 (텍스트)',
      lyricsPlaceholder: '[Verse]\n여기에 가사를...\n\n[Chorus]\n코러스...',
      tagsLabel: '스타일 태그',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: '음악 모델을 선택하세요:',
      generate: '음악 생성',
      generating: '음악 생성 중...'
    },
    musicGen: {
      simpleMode: '간단',
      advancedMode: '고급',
      lyricsLabel: '가사',
      lyricsPlaceholder: '[Verse], [Chorus], [Bridge] 등의 구조 마커와 함께 가사를 작성하세요...\n\n예시:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
      tagsLabel: '스타일 태그',
      tagsPlaceholder: '장르, 분위기, 악기...\n\n예시: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: '가사 & 태그 다듬기',
      refinedLyricsLabel: '다듬어진 가사',
      refinedLyricsPlaceholder: '다듬어진 가사가 여기에 표시됩니다...',
      refiningLyricsMessage: 'AI가 가사를 다듬고 있습니다...',
      refinedTagsLabel: '다듬어진 태그',
      refinedTagsPlaceholder: '다듬어진 스타일 태그가 여기에 표시됩니다...',
      refiningTagsMessage: 'AI가 맞춤 스타일 태그를 생성하고 있습니다...',
      selectModel: '음악 모델 선택',
      generateButton: '음악 생성',
      quality: '품질'
    },
    musicGenV2: {
      lyricsWorkshop: '가사 워크샵',
      lyricsInput: '텍스트',
      lyricsPlaceholder: '가사, 주제, 키워드 또는 분위기를 작성하세요...',
      themeToLyrics: '키워드를 노래 가사로',
      refineLyrics: '노래 가사 구성',
      resultLabel: '결과',
      resultPlaceholder: '가사가 여기에 표시됩니다...',
      expandingTheme: 'AI가 키워드로 노래 가사를 작성하고 있습니다...',
      refiningLyrics: 'AI가 노래 가사를 구성하고 있습니다...',
      soundExplorer: '사운드 탐색기',
      suggestFromLyrics: '가사에서 제안',
      suggestingTags: 'AI가 가사를 분석하고 있습니다...',
      mostImportant: '가장 중요',
      dimGenre: '장르',
      dimTimbre: '음색',
      dimGender: '보컬',
      dimMood: '분위기',
      dimInstrument: '악기',
      dimScene: '장면',
      dimRegion: '지역 (유네스코)',
      dimTopic: '주제',
      audioLength: '오디오 길이',
      generateButton: '음악 생성',
      selectModel: '모델',
      customTags: '맞춤 태그',
      customTagsPlaceholder: '예: acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: '이미지 랩',
        textlab: 'Latent 텍스트 랩',
        crossmodal: '크로스모달 랩'
      },
      imageLab: {
        headerTitle: '이미지 랩 — 시각적 벡터 공간 연구',
        headerSubtitle: '디퓨전 모델이 텍스트에서 이미지를 생성하는 방법을 조사하는 다섯 가지 도구: 디노이징에서 어텐션과 융합, 벡터 산술까지.',
        tabs: {
          archaeology: {
            label: '디노이징 고고학',
            short: '모델의 작업 과정 관찰'
          },
          attention: {
            label: '어텐션 지도',
            short: '모델이 어디를 보는지 확인'
          },
          fusion: {
            label: '인코더 융합',
            short: '초현실적 블렌딩'
          },
          probing: {
            label: '특성 프로빙',
            short: '차원 수준 분석'
          },
          algebra: {
            label: '개념 대수학',
            short: '벡터 산술'
          }
        }
      },
      comingSoon: '이 도구는 향후 버전에서 구현될 예정입니다.',
      shared: {
        negativeHint: '모델이 적극적으로 피해야 할 용어 (예: "흐릿한, 텍스트")',
        stepsHint: '더 많은 단계 = 더 높은 품질이지만 더 긴 생성 시간',
        cfgHint: 'Classifier-Free Guidance: 높을수록 = 프롬프트에 더 충실, 변형 감소',
        seedHint: '-1 = 무작위, 고정값 = 재현 가능한 결과',
        recordingActive: '녹화 활성',
        recordingCount: '{count}개 기록',
        recordingTooltip: '연구 데이터가 자동으로 저장됩니다',
      },
      attention: {
        headerTitle: '어텐션 지도 — 어떤 단어가 어떤 이미지 영역을 조종하나요?',
        headerSubtitle: '프롬프트의 각 단어에 대해 생성된 이미지 위에 히트맵 오버레이가 해당 단어가 이미지의 어디에 가장 큰 영향을 미쳤는지 보여줍니다.',
        explanationToggle: '자세한 설명 보기',
        explainWhatTitle: '이 도구는 무엇을 보여주나요?',
        explainWhatText: '디퓨전 모델이 이미지를 생성할 때, 프롬프트를 지시 세트처럼 단어별로 읽지 않습니다. 대신 "어텐션"이라는 메커니즘이 각 단어의 영향을 다른 이미지 영역에 분배합니다. "집"이라는 단어는 주로 집이 나타나는 영역에 영향을 미치지만 — 모델이 전체 장면의 맥락을 이해하기 때문에 인접 영역에도 영향을 미칩니다. 이 도구는 그 분포를 시각화합니다.',
        explainHowTitle: '히트맵을 어떻게 읽나요?',
        explainHowText: '밝고 강렬한 색상 = 해당 영역에 대한 단어의 강한 영향. 어둡거나 없는 색상 = 적은 영향. 여러 단어를 선택하면 다른 색상으로 나타납니다. 참고: 맵은 완벽하게 날카로운 경계를 가지지 않습니다 — 이것은 버그가 아니라 모델이 개념을 고립적으로가 아닌 맥락적으로 처리한다는 것을 보여줍니다.',
        explainReadTitle: '두 슬라이더는 무엇을 보여주나요?',
        explainReadText: '디노이징 스텝 슬라이더는 25단계 생성 과정에서 언제의 어텐션을 보고 있는지 보여줍니다. 초기 단계는 대략적인 레이아웃 계획을, 후기 단계는 세부 할당을 보여줍니다. 네트워크 깊이 선택기는 트랜스포머 어텐션이 어디에서 측정되는지 보여줍니다.',
        techTitle: '기술 세부사항',
        techText: 'SD3.5는 공동 어텐션을 가진 MMDiT를 사용합니다: 이미지와 텍스트 토큰이 24개의 트랜스포머 블록에서 서로 주의를 기울입니다. 텍스트→이미지 어텐션 부분 행렬을 추출하기 위해 3개의 선택된 블록에서 기본 SDPA 프로세서를 수동 softmax(QK^T/√d) 프로세서로 교체합니다.',
        referencesTitle: '연구 참고문헌',
        promptLabel: '프롬프트',
        promptPlaceholder: '예: 농지와 자연, 동물에 둘러싸인 풍경 속의 집. 일부 사람들이 보입니다.',
        generate: '생성 + 분석',
        generating: '이미지 생성 및 어텐션 추출 중...',
        emptyHint: '모델의 어텐션 맵을 시각화하려면 프롬프트를 입력하고 생성을 클릭하세요.',
        advancedLabel: '고급 설정',
        negativeLabel: '네거티브 프롬프트',
        stepsLabel: '스텝',
        cfgLabel: 'CFG',
        seedLabel: '시드',
        tokensLabel: '토큰',
        tokensHint: '하나 이상의 단어를 클릭하세요. 하위 단어 토큰(예: "Ku"+"gel")은 자동으로 결합됩니다. 여러 단어는 다른 색상으로 나타납니다.',
        timestepLabel: '디노이징 스텝',
        timestepHint: '디퓨전 모델은 25단계에 걸쳐 노이즈에서 이미지를 생성합니다. 초기 단계는 대략적인 구조를, 후기 단계는 세부 사항을 확립합니다.',
        step: '스텝',
        layerLabel: '네트워크 깊이',
        layerHint: '각 디노이징 스텝에서 신호는 24개의 모든 트랜스포머 레이어를 통과합니다. 얕은 레이어는 전역 구성을, 중간 레이어는 의미 할당을, 깊은 레이어는 세부 사항을 포착합니다.',
        layerEarly: '얕은 (구성)',
        layerMid: '중간 (의미)',
        layerLate: '깊은 (세부)',
        opacityLabel: '히트맵',
        opacityHint: '이미지 위의 색상 오버레이 강도.',
        baseImageLabel: '기본 이미지',
        baseColor: '색상',
        baseBW: '흑백',
        baseOff: '끄기',
        baseImageHint: '색상은 원본 이미지를 보여줍니다. 흑백은 히트맵 색상이 돋보이도록 채도를 낮춥니다. 끄기는 이미지를 완전히 숨기고 어텐션 맵만 보여줍니다.',
        encoderLabel: '텍스트 인코더',
        encoderClipL: 'CLIP-L (77 토큰)',
        encoderT5: 'T5-XXL (512 토큰)',
        encoderHint: 'SD3.5는 다른 토크나이제이션을 가진 두 텍스트 인코더를 사용합니다. CLIP-L은 BPE를, T5-XXL은 SentencePiece를 사용합니다. 두 인코더가 같은 프롬프트를 어떻게 처리하고 각각 어떤 이미지 영역을 조종하는지 비교하세요.',
        download: '이미지 다운로드'
      },
      probing: {
        headerTitle: '특성 프로빙 — 어떤 차원이 무엇을 인코딩하나요?',
        headerSubtitle: '두 프롬프트를 비교하고 의미적 차이를 인코딩하는 임베딩 차원을 발견합니다. 개별 차원을 선택적으로 전송하여 이미지에 미치는 영향을 확인합니다.',
        explanationToggle: '자세한 설명 보기',
        explainWhatTitle: '이 도구는 무엇을 보여주나요?',
        explainWhatText: '모든 단어는 텍스트 인코더에 의해 고차원 벡터로 변환됩니다(예: T5의 경우 4096차원). 프롬프트에서 단어를 변경하면 — 예: "빨간"을 "파란"으로 — 특정 차원이 다른 것보다 더 많이 변합니다. 이 도구는 어떤 차원이 가장 많이 변하는지 보여주고 프롬프트 B에서 개별 차원을 선택적으로 프롬프트 A로 전송할 수 있게 합니다.',
        explainHowTitle: '전송은 어떻게 작동하나요?',
        explainHowText: '막대 차트는 차이 크기순으로 정렬된 모든 차원을 보여줍니다. 순위 범위 컨트롤(시작/끝)을 사용하여 범위를 선택합니다. "전송"을 클릭하면 동일한 설정(동일 시드!)으로 이미지가 재생성됩니다 — 단, 프롬프트 B의 선택된 차원으로.',
        explainReadTitle: '막대 차트를 어떻게 읽나요?',
        explainReadText: '각 막대는 하나의 임베딩 차원을 나타냅니다. 길이는 프롬프트 A와 B 사이의 차이를 보여줍니다. 큰 차이가 있는 차원이 의미 변화의 가장 유력한 운반체입니다.',
        techTitle: '기술 세부사항',
        techText: 'SD3.5는 세 개의 텍스트 인코더를 사용합니다: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d). 각각을 개별적으로 프로빙할 수 있습니다.',
        referencesTitle: '연구 참고문헌',
        promptALabel: '프롬프트 A (원본)',
        promptBLabel: '프롬프트 B (비교)',
        promptAPlaceholder: '예: 호수 옆의 빨간 집',
        promptBPlaceholder: '예: 호수 옆의 파란 집',
        encoderLabel: '인코더',
        encoderAll: '전체 (권장)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: '분석',
        analyzing: '프롬프트 인코딩 및 비교 중...',
        transferBtn: '프롬프트 B의 선택된 벡터 차원을 생성된 이미지에 전송',
        transferring: '수정된 임베딩으로 이미지 생성 중...',
        rankFromLabel: '시작 순위',
        rankToLabel: '종료 순위',
        sliderLabel: '프롬프트 B에서 차원 선택',
        range1Label: '범위 1',
        range2Label: '범위 2',
        addRange: '범위 추가',
        selectionDesc: '프롬프트 B의 {count}개 차원 선택됨 (순위 {ranges} / {total})',
        listTitle: '프롬프트 A와 가장 큰 차이를 가진 프롬프트 B의 {count}개 차원',
        sortAsc: '오름차순',
        sortDesc: '내림차순',
        originalLabel: '원본 (프롬프트 A)',
        modifiedLabel: '수정됨 (프롬프트 B에서 전송)',
        modifiedHint: '아래에서 순위 범위를 선택하고 "전송"을 클릭하세요 — 이것은 B에서 전송된 차원이 있는 프롬프트 A를 보여줍니다 (동일 시드).',
        noDifference: '임베딩이 동일합니다 — 프롬프트 B를 변경하세요.',
        advancedLabel: '고급 설정',
        negativeLabel: '네거티브 프롬프트',
        stepsLabel: '스텝',
        cfgLabel: 'CFG',
        seedLabel: '시드',
        selectAll: '전체',
        selectNone: '없음',
        encoderHint: '전체 = 모든 인코더 결합. CLIP-L/CLIP-G/T5 = 분석을 위해 하나의 인코더를 분리합니다.',
        sliderHint: '가장 중요한 임베딩 차원의 순위 범위를 선택합니다 (A와 B 사이의 차이로 정렬).',
        transferHint: '선택한 차원을 프롬프트 B에서 프롬프트 A로 전송하고 새 이미지를 생성합니다.',
        downloadOriginal: '원본 다운로드',
        downloadModified: '수정본 다운로드'
      },
      algebra: {
        headerTitle: '개념 대수학 — 이미지 임베딩에 대한 벡터 산술',
        headerSubtitle: '유명한 word2vec 유추를 이미지 생성에 적용합니다: 왕 - 남자 + 여자 ≈ 여왕. 세 프롬프트가 인코딩되고 대수적으로 결합됩니다.',
        explanationToggle: '자세한 설명 보기',
        explainWhatTitle: '이 도구는 무엇을 보여주나요?',
        explainWhatText: '2013년 Mikolov는 단어 임베딩이 의미적 관계를 선형 방향으로 인코딩한다는 것을 보여주었습니다: "왕" 벡터에서 "남자"를 빼고 "여자"를 더하면 "여왕"에 가까운 벡터가 됩니다. 이 도구는 그 아이디어를 SD3.5의 텍스트 인코더에 적용합니다.',
        explainHowTitle: '대수학은 어떻게 작동하나요 — 왜 네거티브 프롬프트를 사용하지 않나요?',
        explainHowText: '세 프롬프트를 입력합니다: A (기본), B (빼기), C (더하기). 공식: 결과 = A - 스케일₁×B + 스케일₂×C. 네거티브 프롬프트는 25단계 모두에서 B로부터 멀어지도록 합니다. 개념 대수학은 이미지 생성 전에 임베딩 공간에서 새 벡터를 계산합니다 — 의미 공간에서의 수술적 조작입니다.',
        explainReadTitle: '결과의 의미는 무엇인가요?',
        explainReadText: '왼쪽에 참조 이미지(프롬프트 A만, 동일 시드)가, 오른쪽에 대수학 결과가 보입니다. 유추가 작동하면 오른쪽 이미지는 개념 A를 보여주되 의미적 변화 B→C가 적용되어야 합니다. L2 거리는 결과가 원본으로부터 얼마나 이동했는지 보여줍니다.',
        techTitle: '기술 세부사항',
        techText: '대수학은 선택된 인코더 임베딩에 대해 수행됩니다: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), 또는 모두 결합 (589토큰 × 4096d). 풀링된 임베딩(2048d)에도 동일한 연산이 적용됩니다.',
        referencesTitle: '연구 참고문헌',
        promptALabel: '프롬프트 A (기본)',
        promptAPlaceholder: '예: 야자수가 있는 해변의 석양',
        promptBLabel: '프롬프트 B (빼기)',
        promptBPlaceholder: '예: 야자수가 있는 해변',
        promptCLabel: '프롬프트 C (더하기)',
        promptCPlaceholder: '예: 눈 덮인 산',
        formulaLabel: 'A - B + C = ?',
        encoderLabel: '인코더',
        encoderAll: '전체 (권장)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: '계산',
        generating: '임베딩 계산 및 이미지 생성 중...',
        referenceLabel: '참조 (프롬프트 A)',
        resultLabel: '결과 (A - B + C)',
        l2Label: '원본으로부터의 L2 거리',
        advancedLabel: '고급 설정',
        negativeLabel: '네거티브 프롬프트',
        stepsLabel: '스텝',
        cfgLabel: 'CFG',
        seedLabel: '시드',
        scaleSubLabel: '빼기 스케일',
        scaleAddLabel: '더하기 스케일',
        encoderHint: '전체 = 모든 인코더 결합. CLIP-L/CLIP-G/T5 = 연산을 위해 하나의 인코더를 분리합니다.',
        scaleSubHint: '빼기 가중치 (B). 높을수록 = 개념 B의 더 강한 제거.',
        scaleAddHint: '더하기 가중치 (C). 높을수록 = 개념 C의 더 강한 주입.',
        l2Hint: '임베딩 공간에서의 유클리드 거리. 작을수록 = 더 유사, 클수록 = 더 다름.',
        downloadReference: '참조 다운로드',
        downloadResult: '결과 다운로드',
        resultHint: '세 프롬프트를 입력하고 계산을 클릭하세요 — 벡터 산술의 결과가 여기에 나타납니다.'
      },
      archaeology: {
        headerTitle: '디노이징 고고학 — 노이즈가 어떻게 이미지가 되나요?',
        headerSubtitle: '모든 디노이징 스텝을 관찰합니다. 디퓨전 모델은 왼쪽에서 오른쪽으로 그리지 않고 — 모든 곳에서 동시에 작업합니다.',
        explanationToggle: '자세한 설명 보기',
        explainWhatTitle: '이 도구는 무엇을 보여주나요?',
        explainWhatText: '디퓨전 모델은 점진적으로 노이즈를 제거하여 이미지를 만듭니다. 왼쪽에서 오른쪽으로 그리는 것과 달리 모델은 모든 이미지 영역에서 동시에 작업합니다. 첫 번째 스텝에서 대략적인 구조가 나타나고, 중간 스텝에서 의미적 내용이 나타나며, 마지막 스텝에서 텍스처와 세부 사항이 다듬어집니다.',
        explainHowTitle: '이 도구를 어떻게 사용하나요?',
        explainHowText: '프롬프트를 입력하고 생성을 클릭하세요. 모델이 25개의 중간 이미지를 생성합니다. 이것들은 아래에 필름스트립으로 나타납니다. 썸네일을 클릭하거나 타임라인 슬라이더를 사용하여 각 스텝을 전체 크기로 봅니다.',
        explainReadTitle: '세 단계는 무엇을 보여주나요?',
        explainReadText: '초기 스텝 (1-8): 전역 구성 — 기본 구조, 색상 분포, 레이아웃 계획. 중간 스텝 (9-17): 의미적 출현 — 객체가 인식 가능해지고 형태가 결정화됩니다. 후기 스텝 (18-25): 세부 사항 개선 — 텍스처, 가장자리, 미세 패턴.',
        techTitle: '기술 세부사항',
        techText: 'SD3.5 Large는 기본 25스텝의 Rectified Flow를 스케줄러로 사용합니다. 각 스텝에서 현재 잠재 벡터가 VAE(1024×1024 JPEG)를 통해 디코딩됩니다. 잠재 표현은 16채널에서 128×128입니다.',
        referencesTitle: '연구 참고문헌',
        promptLabel: '프롬프트',
        promptPlaceholder: '예: 사람, 건물, 분수가 있는 중세 마을의 시장',
        generate: '생성',
        generating: '이미지 생성 중 — 모든 스텝 기록 중...',
        emptyHint: '디노이징 과정을 시각화하려면 프롬프트를 입력하고 생성을 클릭하세요.',
        advancedLabel: '고급 설정',
        negativeLabel: '네거티브 프롬프트',
        stepsLabel: '스텝',
        cfgLabel: 'CFG',
        seedLabel: '시드',
        filmstripLabel: '디노이징 필름스트립',
        timelineLabel: '스텝',
        phaseEarly: '구성',
        phaseMid: '의미',
        phaseLate: '세부',
        phaseEarlyDesc: '전역 구조와 색상 분포 등장',
        phaseMidDesc: '객체와 형태가 인식 가능해짐',
        phaseLateDesc: '텍스처와 미세한 세부 사항 선명해짐',
        finalImageLabel: '최종 이미지 (전체 해상도)',
        timelineHint: '디노이징 단계를 스크럽합니다 — 이미지가 노이즈에서 최종 구성으로 어떻게 나타나는지 보여줍니다.',
        download: '이미지 다운로드'
      },
      textLab: {
        headerTitle: 'Latent 텍스트 랩 — 과학적 LLM 해체',
        headerSubtitle: 'Representation Engineering, 비교 모델 고고학, 체계적 바이어스 분석: 언어 모델을 조사하기 위한 세 가지 연구 기반 도구.',
        explanationToggle: '\uc124\uba85 \ubcf4\uae30',
        modelPanel: {
          presetLabel: '프리셋',
          presetNone: '프리셋 없음 (사용자 정의 ID)',
          customModelLabel: 'HuggingFace 모델 ID',
          customModelPlaceholder: '예: meta-llama/Llama-3.2-1B',
          quantizationLabel: '양자화',
          quantAuto: '자동',
          quantizationHint: 'bf16 = 최대 품질, int8 = VRAM 절반, int4 = 최소 VRAM이지만 최저 품질',
        },
        temperatureHint: '텍스트 생성의 무작위성. 낮음 = 결정적, 높음 = 더 창의적.',
        maxTokensHint: '생성되는 최대 토큰 수 (단어 조각).',
        textSeedHint: '-1 = 무작위, 고정값 = 재현 가능한 결과',
        tabs: {
          repeng: { label: '\ud45c\ud604 \uacf5\ud559', short: 'LLM\uc5d0\uc11c \uc870\ud5a5 \ubca1\ud130 \ucc3e\uae30' },
          compare: { label: '\ubaa8\ub378 \ube44\uad50', short: '\ub450 LLM\uc744 \ub808\uc774\uc5b4\ubcc4\ub85c \ube44\uad50' },
          bias: { label: '\ud3b8\ud5a5 \uace0\uace0\ud559', short: 'LLM\uc758 \uc228\uaca8\uc9c4 \ud3b8\ud5a5 \ubc1c\uacac' },
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: '활성화 공간에서 개념 방향을 찾고 생성을 조종',
          explainWhatTitle: '\uc774 \uc2e4\ud5d8\uc740 \ubb34\uc5c7\uc744 \ubcf4\uc5ec\uc8fc\ub098\uc694?',
          explainWhatText: 'Zou et al. (2023) \u201cRepresentation Engineering\u201d \ubc0f Li et al. (2024)\uc5d0 \uae30\ubc18\ud569\ub2c8\ub2e4. LLM\uc740 \ucd94\uc0c1\uc801 \uac1c\ub150\uc744 \ud65c\uc131\ud654 \uacf5\uac04\uc758 \ubc29\ud5a5\uc73c\ub85c \ucf54\ub529\ud569\ub2c8\ub2e4. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: '\uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\uc774 \uc2e4\ud5d8\uc740 \ubaa8\ub378\uc5d0\uc11c \u201c\uc9c4\uc2e4 \ubc29\ud5a5\u201d\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4. \ub300\uc870 \uc30d\uc740 \ucc38\uc778 \ubb38\uc7a5\uacfc \uac70\uc9d3\uc778 \ubb38\uc7a5\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uad8c\uc7a5: \uc601\uc5b4 \ud504\ub86c\ud504\ud2b8\uac00 \ub354 \uc798 \uc791\ub3d9\ud569\ub2c8\ub2e4.',
          referencesTitle: '연구 참고문헌',
          expectedResults: '예상 결과: α = 0 (기준선)에서 모델은 올바른 답을 생성합니다. α = -1 (반전)에서 잘못된 답이 나타나야 합니다.',
          pairsTitle: '대조 쌍',
          pairsSubtitle: '최소 3쌍 권장. 각 쌍은 대상 개념(참 vs. 거짓)에서만 다르어야 합니다.',
          positiveLabel: '긍정 (참)',
          negativeLabel: '부정 (거짓)',
          positivePlaceholder: '예: 프랑스의 수도는 파리입니다',
          negativePlaceholder: '예: 프랑스의 수도는 베를린입니다',
          addPair: '쌍 추가',
          removePair: '제거',
          targetLayerLabel: '대상 레이어',
          targetLayerHint: '어떤 트랜스포머 레이어가 조종 벡터를 받는지. 다른 레이어는 텍스트 생성의 다른 측면에 영향을 줍니다.',
          targetLayerAuto: '마지막 레이어',
          findDirection: '방향 찾기',
          finding: '개념 방향 계산 중...',
          directionFound: '개념 방향 발견됨',
          varianceLabel: '설명된 분산',
          dimLabel: '차원',
          projectionsTitle: '대조 쌍 투영',
          testTitle: '테스트 + 조작',
          testSubtitle: '문장을 입력하고 개념 방향을 따라 생성을 조종하세요',
          testPromptLabel: '테스트 프롬프트',
          testPromptPlaceholder: '예: 독일의 수도는',
          alphaLabel: '조작 강도 (α)',
          alphaHint: '조종 벡터의 강도. 0 = 효과 없음, 높을수록 = 대조 쌍의 더 강한 영향.',
          temperatureLabel: '온도',
          maxTokensLabel: '최대 토큰',
          seedLabel: '시드 (-1 = 랜덤)',
          generateBtn: '조작된 생성',
          generating: '조작된 생성 실행 중...',
          baselineLabel: '기준선 (조작 없음)',
          manipulatedLabel: '조작됨 (α = {alpha})',
          projectionLabel: '개념 방향에 대한 투영',
          interpretationTitle: '해석',
          interpreting: '결과 분석 중...',
          interpretationError: '해석을 생성할 수 없습니다'
        },
        compare: {
          title: '비교 모델 고고학',
          subtitle: '두 모델을 로드하고 내부 표현을 체계적으로 비교',
          explainWhatTitle: '\uc774 \uc2e4\ud5d8\uc740 \ubb34\uc5c7\uc744 \ubcf4\uc5ec\uc8fc\ub098\uc694?',
          explainWhatText: 'Belinkov (2022) \ubc0f Olsson et al. (2022)\uc5d0 \uae30\ubc18\ud569\ub2c8\ub2e4. \ud788\ud2b8\ub9f5\uc740 \ub450 \ubaa8\ub378 \ub808\uc774\uc5b4 \uac04 CKA\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: '\uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\ubaa8\ub378 A\ub294 \ud65c\uc131 \ud504\ub9ac\uc14b\uc785\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ubaa8\ub378(B)\uc744 \uc120\ud0dd\ud558\uace0 \ub85c\ub4dc\ud558\uc138\uc694. \ud14d\uc2a4\ud2b8\ub97c \uc785\ub825\ud558\uace0 \u201c\ube44\uad50\u201d\ub97c \ud074\ub9ad\ud558\uc138\uc694.',
          referencesTitle: '연구 참고문헌',
          modelATitle: '모델 A (프리셋 선택에서)',
          modelAHint: '위의 프리셋 드롭다운에서 변경',
          modelBTitle: '모델 B (두 번째 모델)',
          modelBPresetLabel: '프리셋',
          modelBCustomLabel: 'HuggingFace 모델 ID',
          modelBCustomPlaceholder: '예: TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: '모델 B 로드',
          modelBLoaded: '모델 B 로드됨',
          modelBNone: '모델 B 미로드',
          promptLabel: '프롬프트',
          promptPlaceholder: '예: The cat sat on the mat and watched the birds',
          seedLabel: '시드',
          temperatureLabel: '온도',
          maxTokensLabel: '최대 토큰',
          compareBtn: '비교',
          comparing: '모델 비교 중...',
          heatmapTitle: '레이어 정렬 (CKA)',
          heatmapAxisA: '모델 A — 레이어',
          heatmapAxisB: '모델 B — 레이어',
          heatmapExplain: '밝은 셀 = 높은 표현 유사성. 대각선 패턴은 모델이 유사한 순서로 정보를 처리한다는 것을 보여줍니다.',
          attentionTitle: '어텐션 비교 (마지막 레이어)',
          modelALabel: '모델 A',
          modelBLabel: '모델 B',
          generationTitle: '생성 비교 (동일 시드)',
          layerStatsTitle: '레이어 통계',
          interpretationTitle: '해석',
          interpreting: '결과 분석 중...',
          interpretationError: '해석을 생성할 수 없습니다'
        },
        bias: {
          title: '바이어스 고고학',
          subtitle: '제어된 토큰 조작을 통한 체계적 바이어스 실험',
          explainWhatTitle: '\uc774 \uc2e4\ud5d8\uc740 \ubb34\uc5c7\uc744 \ubcf4\uc5ec\uc8fc\ub098\uc694?',
          explainWhatText: 'Zou et al. (2023) \ubc0f Bricken et al. (2023)\uc5d0 \uae30\ubc18\ud569\ub2c8\ub2e4. \uc774 \ub3c4\uad6c\ub294 \uccb4\uacc4\uc801 \ubc14\uc774\uc5b4\uc2a4\ub97c \uc870\uc0ac\ud569\ub2c8\ub2e4. \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: '\uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\uc2e4\ud5d8 \uc720\ud615\uc744 \uc120\ud0dd\ud558\uc138\uc694. \ubaa8\ub378\uc774 \uacc4\uc18d\ud558\ub3c4\ub85d \ud504\ub86c\ud504\ud2b8\ub97c \uc785\ub825\ud558\uc138\uc694. \uacb0\uacfc\ub294 \uae30\uc900\uc120 \ub300 \uc870\uc791\ub41c \uc0dd\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.',
          referencesTitle: '연구 참고문헌',
          presetLabel: '실험 유형',
          presetGender: '성별 — 성별 대명사 억제',
          presetSentiment: '감정 — 긍정/부정 부스트',
          presetDomain: '도메인 — 과학적/시적 부스트',
          presetCustom: '사용자 정의 실험',
          promptLabel: '프롬프트',
          promptPlaceholder: '예: The doctor said to the patient',
          customBoostLabel: '부스트 토큰 (쉼표 구분)',
          customBoostPlaceholder: '예: dark,shadow,night',
          customSuppressLabel: '억제 토큰 (쉼표 구분)',
          customSuppressPlaceholder: '예: light,sun,bright',
          numSamplesLabel: '조건당 샘플 수',
          temperatureLabel: '온도',
          maxTokensLabel: '최대 토큰',
          seedLabel: '기본 시드',
          runBtn: '실험 실행',
          running: '바이어스 실험 실행 중...',
          baselineTitle: '기준선 (조작 없음)',
          groupTitle: '그룹: {name}',
          modeSuppress: '억제됨',
          modeBoost: '부스트됨',
          tokensLabel: '토큰',
          sampleSeedLabel: '시드',
          genderDesc: '모든 성별 대명사를 억제하고 모델이 어떤 기본값을 선택하는지 관찰합니다.',
          sentimentDesc: '긍정적 또는 부정적 단어를 부스트하고 전체 텍스트 흐름이 얼마나 강하게 영향받는지 측정합니다.',
          domainDesc: '과학적 또는 시적 어휘를 부스트하고 레지스터 변화를 관찰합니다.',
          interpretationTitle: '해석',
          interpreting: '결과 분석 중...',
          interpretationError: '해석을 생성할 수 없습니다'
        },
        error: {
          gpuUnreachable: 'GPU 서비스에 연결할 수 없습니다. 실행 중인가요?',
          loadFailed: '모델 로드 실패.',
          operationFailed: '작업 실패.'
        }
      },
      crossmodal: {
        headerTitle: '크로스모달 랩',
        headerSubtitle: '잠재 공간의 소리: T5 임베딩 조작, 이미지 유도 오디오 생성, 크로스모달 전송',
        explanationToggle: '자세한 설명 보기',
        generate: '생성',
        generating: '생성 중...',
        result: '결과',
        seed: '시드',
        generationTime: '생성 시간',
        tabs: {
          synth: {
            label: 'Latent 오디오 신스',
            short: 'T5 임베딩 조작',
            title: 'Latent 오디오 신스',
            description: 'Stable Audio의 T5 조건 공간(768d) 직접 조작. 프롬프트 간 보간, 프롬프트 너머 외삽, 임베딩 스케일링 및 노이즈 주입.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: '이미지/텍스트를 오디오로 (CVPR 2025)',
            title: 'MMAudio — 비디오/이미지를 오디오로',
            description: '이미지와 텍스트가 별도의 신호로 같은 네트워크에 입력됩니다 — 이미지는 언어로 변환되지 않으며, 둘 다 동시에 소리 생성을 안내합니다. 모델은 비디오와 오디오로 공동 학습되어 보이는 것과 들리는 것 사이의 직접적인 연관성을 학습합니다. 최대 8초, 44.1kHz, ~1.2초 계산 시간. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'ImageBind 가이던스',
            short: '그래디언트 기반 이미지 가이던스',
            title: 'ImageBind 그래디언트 가이던스',
            description: 'Stable Audio 디노이징 과정 중 그래디언트 기반 조종. ImageBind는 이미지와 오디오를 위한 공유 1024d 공간을 제공합니다.'
          }
        },
        synth: {
          explainWhatTitle: 'Latent \uc624\ub514\uc624 \uc2e0\uc2a4\ub294 \ubb34\uc5c7\uc744 \ud558\ub098\uc694?',
          explainWhatText: 'Stable Audio\ub294 \ud14d\uc2a4\ud2b8\ub85c\ubd80\ud130 \uc18c\ub9ac\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8\ub294 T5 \uc778\ucf54\ub354\uc5d0 \uc758\ud574 768\ucc28\uc6d0\uc758 \uc22b\uc790 \ubca1\ud130\ub85c \ubcc0\ud658\ub429\ub2c8\ub2e4 \u2014 \uc5ec\uae30\uc11c \uc774 \ubca1\ud130\ub97c \uc9c1\uc811 \uc870\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc774 \u201c\ubb34\uc5c7\uc744\u201d \uc0dd\uc131\ud558\ub294\uc9c0(\ud504\ub86c\ud504\ud2b8\ub97c \ud1b5\ud574)\ub9cc \ubc14\uafb8\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \ubaa8\ub378\uc774 \ud14d\uc2a4\ud2b8\ub97c \ub0b4\ubd80\uc801\uc73c\ub85c \u201c\uc5b4\ub5bb\uac8c\u201d \uc774\ud574\ud558\ub294\uc9c0\ub97c \ubc14\uafb8\ub2c8\ub2e4. \ube44\uc2b7\ud558\uac8c \ub4e4\ub9ac\ub294 \ub450 \ud504\ub86c\ud504\ud2b8\uac00 \uc774 \uacf5\uac04\uc5d0\uc11c\ub294 \uba40\ub9ac \ub5a8\uc5b4\uc838 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4 \u2014 \uadf8 \ubc18\ub300\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc785\ub2c8\ub2e4.',
          explainHowTitle: '\uc774 \ub3c4\uad6c\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\ud504\ub86c\ud504\ud2b8 A\uc5d0 \ud14d\uc2a4\ud2b8\ub97c \uc785\ub825\ud558\uc138\uc694 \u2014 \uc774\uac83\uc774 \uae30\ubcf8 \uc18c\ub9ac\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \uc120\ud0dd\uc0ac\ud56d: \ud504\ub86c\ud504\ud2b8 B\ub97c \ubaa9\ud45c \uc9c0\uc810\uc73c\ub85c. \uc54c\ud30c \uc2ac\ub77c\uc774\ub354\uac00 \ubbf9\uc2a4\ub97c \uc870\uc808\ud569\ub2c8\ub2e4: 0\uc774\uba74 A\ub9cc, 1\uc774\uba74 B\ub9cc, 0.5\uc774\uba74 \ube14\ub80c\ub4dc\ub97c \ub4e3\uac8c \ub429\ub2c8\ub2e4. 1 \uc774\uc0c1\uc758 \uac12\uc740 B \ub108\uba38\ub85c \uc678\uc0bd\ud569\ub2c8\ub2e4(\uc18c\ub9ac\uac00 \ub354 \uadf9\ub2e8\uc801\uc73c\ub85c), 0 \ubbf8\ub9cc\uc758 \uac12\uc740 \ubc18\ub300 \ubc29\ud5a5\uc73c\ub85c \uac11\ub2c8\ub2e4. Magnitude\ub294 \uc804\uccb4 \uc784\ubca0\ub529\uc744 \uc2a4\ucf00\uc77c\ud569\ub2c8\ub2e4 \u2014 \ub192\uc740 \uac12\uc740 \ub354 \uac15\ub82c\ud55c \uc18c\ub9ac\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. Noise\ub294 \ubb34\uc791\uc704\uc131\uc744 \uc8fc\uc785\ud558\uc5ec \uc608\uce21 \ubd88\uac00\ub2a5\ud55c \ubcc0\ud654\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. Spectral Strip(Generate \ubc84\ud2bc \uc544\ub798)\uc740 768\uac1c \ucc28\uc6d0 \ubaa8\ub450\ub97c \ub9c9\ub300\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac1c\ubcc4 \ucc28\uc6d0\uc744 \ud074\ub9ad\ud558\uace0 \ub4dc\ub798\uadf8\ud558\uc5ec \uc18c\ub9ac\ub97c \uc9c1\uc811 \uc870\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ud074\ub9ad\uc740 \ucc28\uc6d0\uc744 \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.',
          promptA: '프롬프트 A (기본)',
          promptAPlaceholder: '예: ocean waves',
          promptB: '프롬프트 B (선택, 보간용)',
          promptBPlaceholder: '예: piano melody',
          alpha: '알파 (보간)',
          alphaHint: '0 = A만, 1 = B만, 사이 = 블렌드, >1 또는 <0 = 외삽',
          magnitude: '크기 (스케일링)',
          magnitudeHint: '전역 임베딩 스케일링 (1.0 = 변경 없음)',
          noise: '노이즈',
          noiseHint: '임베딩에 가우시안 노이즈 (0 = 노이즈 없음)',
          duration: '지속 시간 (초)',
          steps: '스텝',
          cfg: 'CFG',
          durationHint: '생성된 오디오 클립의 길이(초)',
          stepsHint: '디노이징 단계. 더 많을수록 = 더 높은 품질.',
          cfgHint: '오디오 생성을 위한 Classifier-Free Guidance',
          seedHint: '-1 = 무작위, 고정값 = 재현 가능한 결과',
          loop: '루프 재생',
          loopOn: '루프 켜기',
          loopOff: '루프 끄기',
          stop: '중지',
          looping: '루프 중',
          playing: '재생 중',
          stopped: '중지됨',
          transpose: '전조 (반음)',
          midiSection: 'MIDI 제어',
          midiUnsupported: '이 브라우저에서는 Web MIDI가 지원되지 않습니다.',
          midiInput: 'MIDI 입력',
          midiNone: '(없음)',
          midiMappings: 'CC 매핑',
          midiNoteC3: '노트 (C3 = 기준)',
          midiGenerate: '생성 + 전조',
          midiPitch: 'C3 대비 피치',
          loopInterval: '루프 구간',
          loopOptimize: '자동 최적화',
          loopPingPong: '핑퐁',
          loopIntervalHint: '루프 영역의 시작/끝 — Stable Audio 페이드아웃을 트리밍하려면 끝을 줄이세요',
          modeLoop: '루프',
          modePingPong: '핑퐁',
          modeWavetable: '웨이브테이블',
          modeRate: '템포 (빠름)',
          modePitch: '피치 (OLA)',
          wavetableScan: '스캔 위치',
          wavetableScanHint: '프레임 간 모프 (0 = 시작, 1 = 끝)',
          wavetableFrames: '{count} 프레임',
          midiScan: '스캔 위치',
          adsrTitle: 'ADSR 엔벨로프',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'MIDI 노트용 엔벨로프 (어택/디케이/서스테인/릴리스)',
          play: '재생',
          normalize: '라우드니스 정규화',
          peak: '피크',
          crossfade: '크로스페이드',
          transposeHint: '반음 단위로 피치를 이동',
          crossfadeHint: '루프 경계에서의 크로스페이드 시간 (ms)',
          normalizeHint: '볼륨을 최대 진폭으로 정규화',
          saveRaw: '원본 저장',
          saveLoop: '루프 저장',
          embeddingStats: '임베딩 통계',
          dimensions: {
            section: '차원 탐색기',
            hint: '바에 드래그 = 오프셋 설정. 가로로 칠하기 = 여러 차원.',
            resetAll: '모두 초기화',
            hoverActivation: '활성화',
            hoverOffset: '오프셋',
            rightClickReset: '우클릭 = 초기화',
            sortDiff: '프롬프트 차이순 정렬',
            sortMagnitude: '활성화순 정렬',
            activeOffsets: '{count}개 오프셋 활성',
            applyAndGenerate: '적용 및 재생성',
            undo: '실행 취소',
            redo: '다시 실행'
          }
        },
        mmaudio: {
          explainWhatTitle: 'MMAudio\ub294 \ubb34\uc5c7\uc744 \ud558\ub098\uc694?',
          explainWhatText: 'MMAudio(Cheng et al., CVPR 2025)\ub294 \ube44\ub514\uc624\uc640 \uc624\ub514\uc624\ub85c \uacf5\ub3d9 \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\ub97c \ud14d\uc2a4\ud2b8\ub85c \ubcc0\ud658\ud55c \ub2e4\uc74c \uc18c\ub9ac\ub85c \ubcc0\ud658\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \uac19\uc740 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \ubcd1\ub82c \uc2e0\ud638\ub85c \ucc98\ub9ac\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc5b4\ub5a4 \uc18c\ub9ac\uac00 \uc5b4\ub5a4 \uc2dc\uac01\uc801 \uc7a5\uba74\uc5d0 \uc18d\ud558\ub294\uc9c0 \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4 \u2014 \uc232\uc740 \uc0c8\uc18c\ub9ac\ub97c, \uac70\ub9ac\ub294 \uad50\ud1b5 \uc18c\uc74c\uc744, \uae30\ud0c0\ub294 \ud604\uc744 \ub5b5\ub294 \uc18c\ub9ac\ub97c \ub9cc\ub4e4\uc5b4\ub0c5\ub2c8\ub2e4.',
          explainHowTitle: '\uc774 \ub3c4\uad6c\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\uc774\ubbf8\uc9c0\ub97c \uc5c5\ub85c\ub4dc\ud558\uace0/\ub610\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \uc785\ub825\ud558\uc138\uc694 \u2014 \ub458 \ub2e4 \ud568\uaed8 \uc0ac\uc6a9\ud558\uba74 \uac00\uc7a5 \ud48d\ubd80\ud55c \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\ub9cc\uc73c\ub85c\ub3c4 \uc2dc\uac01\uc801 \ub0b4\uc6a9\uc5d0 \ub9de\ub294 \uc18c\ub9ac\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub294 \uc18c\ub9ac\ub97c \ucd94\uac00\ub85c \uc870\uc885\ud558\uac70\ub098 \uc774\ubbf8\uc9c0 \uc5c6\uc774 \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub124\uac70\ud2f0\ube0c \ud504\ub86c\ud504\ud2b8\uc5d0\ub294 \ub4e3\uace0 \uc2f6\uc9c0 \uc54a\uc740 \uc18c\ub9ac\ub97c \uc124\uba85\ud558\uc138\uc694(\uc608: \u201c\uc74c\uc131, \uc74c\uc545\u201d). Duration\uc740 \uae38\uc774\ub97c \uc124\uc815\ud569\ub2c8\ub2e4(1-8\ucd08). CFG Strength\ub294 \ubaa8\ub378\uc774 \ud504\ub86c\ud504\ud2b8\ub97c \uc5bc\ub9c8\ub098 \uc5c4\uaca9\ud558\uac8c \ub530\ub974\ub294\uc9c0 \uc81c\uc5b4\ud569\ub2c8\ub2e4 \u2014 \ub0ae\uc740 \uac12(2-3)\uc740 \ub354 \ub2e4\uc591\ud55c \uacb0\uacfc\ub97c, \ub192\uc740 \uac12(6-8)\uc740 \ub354 \ud504\ub86c\ud504\ud2b8\uc5d0 \ucda9\uc2e4\ud55c \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.',
          imageUpload: '이미지 업로드 (선택 사항)',
          prompt: '텍스트 프롬프트 (선택 사항)',
          promptPlaceholder: '예: 타닥타닥 캠프파이어',
          negativePrompt: '네거티브 프롬프트',
          duration: '지속 시간 (초)',
          maxDuration: '최대 8초 (모델 제한)',
          cfg: 'CFG',
          steps: '스텝',
          compareHint: '비교: 텍스트만 vs. 이미지 + 텍스트'
        },
        guidance: {
          explainWhatTitle: 'ImageBind Guidance\ub294 \ubb34\uc5c7\uc744 \ud558\ub098\uc694?',
          explainWhatText: 'ImageBind(Girdhar et al., CVPR 2023)\ub294 \uc5ec\uc12f \uac00\uc9c0 \uac10\uac01 \u2014 \uc774\ubbf8\uc9c0, \uc18c\ub9ac, \ud14d\uc2a4\ud2b8, \uae4a\uc774, \uc5f4, \uc6c0\uc9c1\uc784 \u2014 \uc744 \uacf5\uc720\ub41c \u201c\uc5b8\uc5b4\u201d\ub85c \ud1b5\ud569\ud569\ub2c8\ub2e4. \uc774 \ub3c4\uad6c\ub294 \uadf8 \uacf5\ud1b5 \uae30\ubc18\uc744 \ud65c\uc6a9\ud569\ub2c8\ub2e4: \uc18c\ub9ac\uac00 \ub2e8\uacc4\ubcc4\ub85c \uc0dd\uc131\ub418\ub294 \ub3d9\uc548 \u201c\uc774 \uc18c\ub9ac\uac00 \uc774\ubbf8\uc9c0\uc640 \ub2ee\uc558\ub098?\u201d\ub77c\uace0 \uacc4\uc18d \ubb3c\uc73c\uba70 \ubc29\ud5a5\uc744 \uc218\uc815\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc5d0\uc11c \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\ub294 \uc0dd\uc131\ub41c \uc18c\ub9ac\uac00 \uc774\ubbf8\uc9c0 \ub0b4\uc6a9\uc5d0 \uc5bc\ub9c8\ub098 \uac00\uae4c\uc774 \uc654\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.',
          explainHowTitle: '\uc774 \ub3c4\uad6c\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?',
          explainHowText: '\uc774\ubbf8\uc9c0\ub97c \uc5c5\ub85c\ub4dc\ud558\uc138\uc694 \u2014 \uc774\uac83\uc774 \uc18c\ub9ac\uc758 \ubaa9\ud45c \ubc29\ud5a5\uc785\ub2c8\ub2e4. \uc120\ud0dd\uc0ac\ud56d: \ucd94\uac00 \uc870\uc885\uc744 \uc704\ud55c \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8. \u201c\u03bb Guidance Strength\u201d \uc2ac\ub77c\uc774\ub354\uac00 \uac00\uc7a5 \uc911\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\uc785\ub2c8\ub2e4: \ub0ae\uc740 \uac12(0.01-0.05)\uc740 \uc18c\ub9ac\uc5d0 \ub9ce\uc740 \uc790\uc720\ub97c \uc8fc\uace0, \ub192\uc740 \uac12(0.3-1.0)\uc740 \uc18c\ub9ac\ub97c \uc774\ubbf8\uc9c0\uc5d0 \ubc00\ucc29\uc2dc\ud0b5\ub2c8\ub2e4. \u201cWarmup Steps\u201d\ub294 \uc774\ubbf8\uc9c0 \uac00\uc774\ub358\uc2a4\uac00 \uc2dc\uc791\ub418\ub294 \ub2e8\uacc4\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4 \u2014 \ub0ae\uc740 \uac12\uc740 \uc989\uc2dc \uc2dc\uc791\ud558\uace0, \ub192\uc740 \uac12\uc740 \uae30\ubcf8 \uad6c\uc870\uac00 \uba3c\uc800 \uc790\uc720\ub86d\uac8c \ud615\uc131\ub418\ub3c4\ub85d \ud569\ub2c8\ub2e4. Total Steps\uc640 Duration\uc740 \ud488\uc9c8\uacfc \uae38\uc774\ub97c \uc81c\uc5b4\ud569\ub2c8\ub2e4.',
          referencesTitle: '연구 참고문헌',
          imageUpload: '이미지 업로드',
          prompt: '기본 프롬프트 (선택)',
          promptPlaceholder: '예: 앰비언트 사운드스케이프',
          lambda: '가이던스 강도',
          lambdaHint: '이미지가 오디오 생성을 얼마나 강하게 조종하는지',
          warmupSteps: '워밍업 스텝',
          warmupHint: '그래디언트 가이던스는 처음 N스텝에서만',
          totalSteps: '총 스텝',
          duration: '지속 시간 (초)',
          cfg: 'CFG',
          totalStepsHint: '총 디노이징 단계. 더 많을수록 = 더 높은 품질.',
          durationHint: '생성된 오디오 클립의 길이(초)',
          cosineSimilarity: '코사인 유사도 (이미지-오디오 근접도)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 알고 계셨나요?',
        learnMore: '📚 더 알아보기',
        currentlyHappening: '⚡ 현재 진행 중:',
        energyUsed: '사용된 에너지',
        co2Produced: '생성된 CO₂'
      },
      energy: {
        kids_1: '💡 AI 이미지는 전기가 필요합니다 — 휴대폰을 3시간 충전하는 것만큼!',
        kids_2: '🔌 GPU는 전력을 많이 사용하는 슈퍼 계산기입니다!',
        kids_3: '⚡ 이미지 하나에 LED 조명을 10분 켜는 것만큼의 에너지가 필요합니다!',
        youth_1: '⚡ GPU는 생성 중에 {watts}W를 사용합니다 — 작은 히터 같습니다!',
        youth_2: '🔋 이미지 하나에 약 0.01-0.02 kWh — 적어 보이지만 쌓입니다!',
        youth_3: '🌡️ GPU가 현재 {temp}°C에 도달하고 있습니다 — 그래서 냉각이 필요합니다!',
        expert_1: '📊 실시간: {watts}W, {util}% 활용률 = 현재까지 {kwh} kWh',
        expert_2: '🔥 TDP 한계: {tdp}W | 현재: {watts}W (한계의 {percent}%)',
        expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) — 모델 + 활성화'
      },
      data: {
        kids_1: '🧮 GPU가 지금 100억 번 계산하고 있습니다 — 여러분이 셀 수 있는 것보다 빠릅니다!',
        kids_2: '🎨 이미지는 50개의 작은 단계로 만들어집니다 — 스스로 풀리는 퍼즐처럼!',
        kids_3: '🧩 수백만 개의 숫자가 지금 GPU를 통과하고 있습니다!',
        youth_1: '🔄 각 이미지는 ~50 "디노이징 스텝"을 거칩니다 — 노이즈 제거의 50라운드!',
        youth_2: '📐 80억 개의 매개변수가 조회됩니다 — 이미지당!',
        youth_3: '🧠 AI는 수천 차원의 벡터로 "생각"합니다 — 공간에서의 좌표처럼.',
        expert_1: '🔬 MMDiT: 멀티모달 디퓨전 트랜스포머 — 공동 어텐션 레이어에서 텍스트 + 이미지',
        expert_2: '📈 Self-Attention: O(n²) 복잡도 — 모든 토큰이 다른 모든 토큰을 "봄"',
        expert_3: '⚙️ Classifier-Free Guidance: 프롬프트 영향 vs. 창의성 균형'
      },
      model: {
        kids_1: '🎓 AI 모델은 그림 그리는 법을 배우기 위해 수백만 장의 이미지를 봤습니다!',
        kids_2: '🤖 AI는 본 것을 절대 잊지 않는 예술가와 같습니다!',
        kids_3: '✨ 모델에 80억 개의 연결이 있습니다 — 하늘에서 볼 수 있는 별보다 많습니다!',
        youth_1: '🧠 SD3.5 Large는 80억 개의 매개변수를 가지고 있습니다 — 80억 개의 결정 노드처럼.',
        youth_2: '📚 3개의 텍스트 인코더가 함께 작동합니다: CLIP-L, CLIP-G, T5-XXL',
        youth_3: '🔢 모델은 로드만 하는 데 {vram} GB의 VRAM이 필요합니다!',
        expert_1: '🏗️ 아키텍처: 38개 트랜스포머 블록의 Rectified Flow + MMDiT',
        expert_2: '📊 FP16/FP8 양자화: 정밀도 vs. VRAM 트레이드오프',
        expert_3: '🔗 LoRA: Low-Rank Adaptation — 매개변수의 0.1%만 재학습'
      },
      ethics: {
        kids_1: '🌍 AI는 인터넷의 이미지에서 배웁니다 — 그래서 다른 사람의 예술을 존중하는 것이 중요합니다!',
        kids_2: '⚖️ 모든 예술가에게 AI가 그들에게서 배울 수 있는지 물어보지 않았습니다.',
        kids_3: '🤝 좋은 AI는 사람들의 작업을 존중합니다!',
        youth_1: '📜 학습 데이터는 종종 인터넷에서 옵니다. 예술가들이 논쟁합니다: 공정 사용인가 복사인가?',
        youth_2: '🏛️ EU AI Act는 투명성을 요구합니다: 학습 데이터는 어디서 오나요?',
        youth_3: '💭 질문: AI가 생성한 이미지의 실제 소유자는 누구인가요?',
        expert_1: '⚠️ LAION-5B는 부분적으로 창작자의 동의 없이 만들어졌습니다 — 법적 회색 영역.',
        expert_2: '📋 EU AI Act 52조: AI 생성 콘텐츠에 대한 라벨링 의무',
        expert_3: '🔍 모델 카드 & 데이터시트: ML 투명성을 위한 모범 사례'
      },
      environment: {
        kids_1: '☁️ AI 이미지 하나가 약간의 CO₂를 생성합니다 — 자동차 운전처럼, 하지만 더 적게!',
        kids_2: '🌱 생각해 보세요: 이 이미지가 전기 값을 할 만한가요?',
        kids_3: '🌞 AI를 위한 에너지는 종종 발전소에서 옵니다 — 일부는 깨끗하고, 일부는 그렇지 않습니다.',
        youth_1: '🏭 독일 전력망: kWh당 ~400g CO₂ — 이것은 쌓입니다!',
        youth_2: '📈 이 이미지에 {co2}g CO₂ — 1000개 이미지면 {totalKg} kg이 됩니다!',
        youth_3: '💡 팁: 더 적게 생성하되 더 신중하게 — 에너지와 CO₂를 절약합니다.',
        expert_1: '📊 계산: {watts}W × {seconds}초 ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Scope 2 배출: 데이터 센터 위치가 결정적',
        expert_3: '⚡ PUE (전력 사용 효율성): 냉각을 위한 추가 에너지 오버헤드'
      },
      iceberg: {
        drawPrompt: 'AI 생성은 많은 에너지를 사용합니다. 빙산을 그리고 무슨 일이 일어나는지 보세요...',
        redraw: '다시 그리기',
        startMelting: '녹이기 시작',
        melting: '빙산이 녹고 있습니다...',
        melted: '녹았습니다!',
        meltedMessage: '{co2}g CO₂ 생성됨',
        comparison: '이 CO₂ 양은 약 {volume} cm³의 북극 얼음을 녹입니다.',
        comparisonInfo: '(CO₂ 1톤 = 약 6m³ 해빙 손실)',
        gpuPower: '그래픽 카드 전력 소비',
        gpuTemp: '그래픽 카드 온도',
        co2Info: '전력 소비로 인한 CO₂ 배출 (독일 에너지 믹스 기준)',
        drawAgain: '빙산을 더 그리세요...'
      },
      pixel: {
        grafikkarte: '그래픽 카드',
        energieverbrauch: '에너지 사용량',
        co2Menge: 'CO₂ 양',
        smartphoneComparison: '이 CO₂ 사용량을 상쇄하려면 휴대폰을 {minutes}분 동안 꺼야 합니다!',
        clickToProcess: '미니 이미지를 생성하려면 데이터 픽셀을 클릭하세요!'
      },
      forest: {
        trees: '나무',
        clickToPlant: '나무를 심으려면 클릭하세요! 나무를 심는 곳에서 공장이 사라집니다.',
        gameOver: '숲이 사라졌습니다!',
        treesPlanted: '{count}그루의 나무를 심었습니다.',
        complete: '생성 완료',
        comparison: '평균적인 나무가 이 CO₂ 양을 흡수하는 데 {minutes}분이 걸립니다.'
      },
      rareearth: {
        clickToClean: '독성 슬러지를 제거하려면 호수를 클릭하세요!',
        sludgeRemoved: '슬러지 제거됨',
        environmentHealth: '환경',
        gameOverInactive: '포기했습니다... 채굴은 계속됩니다',
        infoBanner: 'GPU 칩을 위한 희토류 채굴은 독성 슬러지를 남기고 생태계를 파괴합니다. 정화 노력이 추출 속도를 따라잡을 수 없습니다.',
        instructionsCooldown: '⏳ {seconds}초',
        statsGpu: 'GPU',
        statsHealth: '환경',
        statsSludge: '슬러지 제거됨'
      }
    }

  },
  uk: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Креативні ШІ-трансформації'
    },
    form: {
      inputLabel: 'Твій текст',
      inputPlaceholder: 'напр. Квітка на лузі',
      schemaLabel: 'Стиль трансформації',
      executeModeLabel: 'Режим виконання',
      safetyLabel: 'Рівень безпеки',
      generateButton: 'Згенерувати'
    },
    schemas: {
      dada: 'Дада (Випадковий і абсурдний)',
      bauhaus: 'Баухаус (Геометричний)',
      stillepost: 'Зіпсований телефон (Ітеративний)'
    },
    executionModes: {
      eco: 'Еко (Швидко)',
      fast: 'Швидкий (Збалансований)',
      best: 'Найкращий (Якість)'
    },
    safetyLevels: {
      kids: 'Діти',
      youth: 'Підлітки',
      adult: 'Дорослі',
      research: 'Дослідження'
    },
    stages: {
      pipeline_starting: 'Конвеєр запускається',
      translation_and_safety: 'Переклад і безпека',
      interception: 'Трансформація',
      pre_output_safety: 'Безпека виводу',
      media_generation: 'Генерація зображення',
      completed: 'Завершено'
    },
    status: {
      idle: 'Готово',
      executing: 'Конвеєр працює...',
      connectionSlow: 'З\'єднання повільне, повторна спроба...',
      completed: 'Конвеєр завершено!',
      error: 'Виникла помилка'
    },
    entities: {
      input: 'Введення',
      translation: 'Переклад',
      safety: 'Перевірка безпеки',
      interception: 'Трансформація',
      safety_pre_output: 'Безпека виводу',
      media: 'Згенероване зображення'
    },
    properties: {
      chill: 'спокійний',
      chaotic: 'дикий',
      narrative: 'розповідати історії',
      algorithmic: 'за правилами',
      historical: 'історія',
      contemporary: 'сучасність',
      explore: 'тестувати ШІ',
      create: 'створювати мистецтво',
      playful: 'грайливий',
      serious: 'серйозний'
    },
    phase2: {
      title: 'Введення промпту',
      userInput: 'Твоє введення',
      yourInput: 'Твоє введення',
      yourIdea: 'Твоя ідея: ПРО ЩО це має бути?',
      rules: 'Твої правила: ЯК реалізувати твою ідею?',
      yourInstructions: 'Твої інструкції',
      what: 'ЩО',
      how: 'ЯК',
      userInputPlaceholder: 'напр. Квітка на лузі',
      inputPlaceholder: 'Тут з\'явиться твій текст...',
      metaPrompt: 'Мистецька інструкція',
      instruction: 'Інструкція',
      transformation: 'Мистецька трансформація',
      metaPromptPlaceholder: 'Опиши трансформацію...',
      result: 'Результат',
      expectedResult: 'Очікуваний результат',
      execute: 'Запустити конвеєр',
      executing: 'Виконується...',
      transforming: 'LLM трансформує...',
      startTransformation: 'Почати трансформацію',
      letsGo: 'Добре, поїхали!',
      modified: 'Змінено',
      reset: 'Скинути',
      loadingConfig: 'Завантаження конфігурації...',
      loadingMetaPrompt: 'Завантаження мета-промпту...',
      errorLoadingConfig: 'Помилка завантаження конфігурації',
      errorLoadingMetaPrompt: 'Помилка завантаження мета-промпту',
      threeForces: '3 сили працюють разом',
      twoForces: 'ЩО + ЯК → LLM → Результат',
      yourPrompt: 'Твій промпт:',
      writeYourText: 'Напиши свій текст...',
      examples: 'Приклади',
      estimatedTime: '~12 секунд',
      stage12Time: '~5-10 секунд',
      willAppearAfterExecution: 'З\'явиться після виконання...',
      back: 'Назад',
      retry: 'Спробувати знову',
      transformedPrompt: 'Трансформований промпт',
      notYetTransformed: 'Ще не трансформовано...',
      transform: 'Трансформувати',
      reTransform: 'Спробувати інакше',
      startAI: 'ШІ, обробити моє введення',
      aiWorking: 'ШІ працює...',
      continueToMedia: 'Продовжити до генерації зображення',
      readyForMedia: 'Готово до генерації зображення',
      stage1: 'Етап 1: Переклад + безпека...',
      stage2: 'Етап 2: Трансформація...',
      selectMedia: 'Обери свій медіум:',
      mediaImage: 'Зображення',
      mediaAudio: 'Аудіо',
      mediaVideo: 'Відео',
      media3D: '3D',
      comingSoon: 'Незабаром',
      generateMedia: 'Старт!'
    },
    phase3: {
      generating: 'Зображення генерується...',
      generatingHint: '~30 секунд'
    },
    common: {
      back: 'Назад',
      loading: 'Завантаження...',
      error: 'Помилка',
      retry: 'Повторити',
      cancel: 'Скасувати',
      checkingSafety: 'Перевірка...'
    },
    gallery: {
      title: 'Обране',
      empty: 'Поки що нічого в обраному',
      favorite: 'Додати до обраного',
      unfavorite: 'Видалити з обраного',
      continue: 'Продовжити редагування',
      restore: 'Відновити сесію',
      viewMine: 'Моє обране',
      viewAll: 'Усе обране'
    },
    settings: {
      authRequired: 'Потрібна автентифікація',
      authPrompt: 'Будь ласка, введіть пароль для доступу до налаштувань:',
      passwordPlaceholder: 'Введіть пароль...',
      authenticate: 'Увійти',
      authenticating: 'Автентифікація...',
      title: 'Адміністрування',
      tabs: {
        export: 'Дослідницькі дані',
        config: 'Конфігурація',
        demos: 'Демо мінігри',
        matrix: 'Матриця моделей'
      },
      loading: 'Завантаження налаштувань...',
      presets: {
        title: 'Набори моделей',
        help: 'Використовуйте вкладку <strong>Матриця моделей</strong>, щоб переглянути всі доступні набори та застосувати їх одним кліком.',
        openMatrix: 'Відкрити матрицю моделей'
      },
      testingTools: {
        title: 'Інструменти тестування для педагогів',
        help: 'Тестуйте та досліджуйте педагогічні мініігри та анімації перед використанням з учнями.',
        openPreview: 'Відкрити попередній перегляд мініігор',
        pixelEditor: 'Піксельний редактор шаблонів',
        includes: 'Включає: Піксельну анімацію, Танення айсберга, Гру з лісом, Рідкісні землі'
      },
      general: {
        title: 'Загальна конфігурація',
        uiMode: 'Режим інтерфейсу',
        uiModeHelp: 'Рівень складності інтерфейсу',
        kids: 'Діти (8–12)',
        youth: 'Підлітки (13–17)',
        expert: 'Експерт',
        safetyLevel: 'Рівень безпеки',
        defaultLanguage: 'Мова за замовчуванням',
        germanDe: 'Німецька (de)',
        englishEn: 'Англійська (en)',
        turkishTr: 'Турецька (tr)',
        koreanKo: '한국어 (ko)',
        ukrainianUk: 'Українська (uk)',
        frenchFr: 'Français (fr)'
      },
      safety: {
        kidsTitle: 'Діти (8–12)',
        kidsDesc: 'Усі фільтри активні: §86a, DSGVO, захист дітей (вікові параметри), VLM-перевірка зображень',
        youthTitle: 'Підлітки (13–17)',
        youthDesc: 'Усі фільтри активні: §86a, DSGVO, захист молоді (параметри для підлітків), VLM-перевірка зображень',
        adultTitle: 'Дорослі',
        adultDesc: '§86a + DSGVO активні. Без захисту молоді, без VLM-перевірки зображень.',
        researchTitle: 'Режим дослідження',
        researchDesc: 'ЖОДНИХ фільтрів безпеки не активовано. Дозволено лише для використання дослідницькими установами в контексті наукових проєктів.'
      },
      safetyModels: {
        title: 'Локальні моделі безпеки',
        help: 'Локально через Ollama — імена людей і перевірки безпеки ніколи не залишають систему',
        safetyModel: 'Модель безпеки',
        safetyModelHelp: 'Guard-модель для безпеки контенту (§86a, захист молоді)',
        dsgvoModel: 'DSGVO-модель верифікації',
        dsgvoModelHelp: 'Модель загального призначення для верифікації NER за DSGVO (не guard-модель)',
        vlmModel: 'VLM-модель безпеки',
        vlmModelHelp: 'Візуальна модель для перевірки безпеки зображень після генерації (діти/підлітки)',
        fast: 'швидко, мінімально',
        recommended: 'рекомендовано'
      },
      dsgvo: {
        title: 'Попередження DSGVO',
        notCompliant: 'Наступні моделі <strong>НЕ відповідають DSGVO</strong> (дані обробляються за межами ЄС):',
        compliantHint: 'Варіанти, що відповідають DSGVO:'
      },
      models: {
        title: 'Конфігурація моделей',
        help: 'Ідентифікатори моделей з префіксом провайдера: local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: 'Рекомендується використовувати матрицю моделей. Однак ви можете вільно налаштовувати тут.',
        ollamaAvailable: '{count} моделей Ollama доступно (введіть або оберіть зі списку)',
        stage1Text: 'Етап 1 - Текстова модель',
        stage1Vision: 'Етап 1 - Модель зору',
        stage2Interception: 'Етап 2 - Модель перехоплення',
        stage2Optimization: 'Етап 2 - Модель оптимізації',
        stage3: 'Етап 3 - Модель перекладу/безпеки',
        stage4Legacy: 'Етап 4 - Застаріла модель',
        chatHelper: 'Модель чат-помічника',
        imageAnalysis: 'Модель аналізу зображень',
        coding: 'Генерація коду (Tone.js, p5.js)'
      },
      api: {
        title: 'Конфігурація API',
        llmProvider: 'Провайдер LLM',
        localFramework: 'Локальний LLM-фреймворк',
        externalProvider: 'Зовнішній провайдер LLM',
        cloudProvider: 'Хмарний провайдер LLM — потрібен API-ключ',
        noneLocal: 'Без (тільки локально, DSGVO)',
        mistralEu: 'Mistral AI (базується в ЄС, DSGVO)',
        anthropicDirect: 'Anthropic Direct API (НЕ DSGVO)',
        openaiDirect: 'OpenAI Direct API (НЕ DSGVO)',
        openrouterDirect: 'OpenRouter (НЕ DSGVO, маршрутизація через ЄС доступна)',
        mistralInfo: 'Mistral AI (базується в ЄС)',
        mistralDsgvo: 'Відповідає DSGVO (інфраструктура ЄС)',
        anthropicInfo: 'Anthropic Direct API',
        anthropicNotDsgvo: 'НЕ відповідає DSGVO',
        anthropicWarning: 'Дані обробляються за межами ЄС. Використовуйте лише в необосвітніх контекстах.',
        openaiInfo: 'OpenAI Direct API',
        openaiNotDsgvo: 'НЕ відповідає DSGVO (базується в США)',
        openaiWarning: 'Дані обробляються у Сполучених Штатах. Використовуйте лише в необосвітніх контекстах.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: 'НЕ відповідає DSGVO (компанія зі США)',
        openrouterWarning: 'Маршрутизацію через сервери ЄС можна налаштувати в OpenRouter, але компанія знаходиться в США.',
        storedIn: 'Збережено у',
        currentKey: 'Поточний'
      },
      save: {
        saveApply: 'Зберегти і застосувати',
        saving: 'Збереження...',
        applying: 'Застосування...',
        success: 'Налаштування збережено та застосовано',
        presetApplied: 'Застосовано набір: {preset}'
      }
    },
    pipeline: {
      yourInput: 'Твоє введення',
      result: 'Результат',
      generatedMedia: 'Згенероване зображення'
    },
    landing: {
      subtitlePrefix: 'Педагогічно-мистецька експериментальна платформа',
      subtitleSuffix: 'для дослідницького використання генеративного ШІ в культурно-естетичній медіаосвіті',
      research: '',
      features: {
        textTransformation: {
          title: 'Текстова трансформація',
          description: 'Зміна перспективи через ШІ — твій промпт трансформується через мистецько-педагогічні лінзи в зображення, відео та звук.'
        },
        imageTransformation: {
          title: 'Трансформація зображень',
          description: 'Трансформуй зображення через різні моделі та перспективи в нові зображення та відео.'
        },
        multiImage: {
          title: 'Злиття зображень',
          description: 'Комбінуй декілька зображень та зливай їх у нові композиції через моделі ШІ.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Візуальна композиція робочих процесів — з\'єднуй модулі через перетягування у власні ШІ-конвеєри.'
        },
        music: {
          title: 'Генерація музики',
          description: 'Створення музики за допомогою ШІ з текстами пісень, тегами та стильовим контролем.'
        },
        latentLab: {
          title: 'Латентна лабораторія',
          description: 'Дослідження векторного простору — сюрреалізація, елімінація вимірів, інтерполяція вкладень.'
        }
      }
    },
    research: {
      locked: 'Доступно лише в режимі дослідження',
      lockedHint: 'Потрібен рівень безпеки "Дорослі" або "Дослідження" (config.py)',
      complianceTitle: 'Повідомлення про режим дослідження',
      complianceWarning: 'У режимі дослідження фільтри безпеки не активні для промптів або згенерованих зображень. Можуть виникнути неочікувані або невідповідні результати.',
      complianceAge: 'Цей режим не рекомендується для осіб молодше 16 років.',
      complianceConfirm: 'Я підтверджую, що зрозумів(ла) повідомлення',
      complianceCancel: 'Скасувати',
      complianceProceed: 'Продовжити'
    },
    presetOverlay: {
      title: 'Обери перспективу',
      close: 'Закрити'
    },
    imageUpload: {
      clickHere: 'Натисни тут',
      orDragImage: 'або перетягни зображення сюди',
      formatHint: 'PNG, JPG, WEBP (макс. 10МБ)',
      invalidFormat: 'Невірний формат файлу. Дозволені лише PNG, JPG та WEBP.',
      fileTooLarge: 'Файл занадто великий. Максимум: {max}МБ',
      uploadFailed: 'Помилка завантаження',
      infoOriginal: 'Оригінал:',
      infoSize: 'Розмір:'
    },
    mediaInput: {
      choosePreset: 'Обери перспективу',
      translateToEnglish: 'Перекласти англійською',
      copy: 'Копіювати',
      paste: 'Вставити',
      delete: 'Видалити',
      loading: 'Завантаження...',
      contentBlocked: 'Контент заблоковано'
    },
    nav: {
      about: 'Про нас',
      impressum: 'Імпресум',
      privacy: 'Конфіденційність',
      docs: 'Документація',
      language: 'Змінити мову',
      settings: 'Налаштування',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'Новий робочий процес',
      importWorkflow: 'Імпорт',
      exportWorkflow: 'Експорт',
      execute: 'Виконати',
      ready: 'Готово',
      errors: 'помилки',
      discardWorkflow: 'Відкинути поточний робочий процес?',
      importError: 'Не вдалося імпортувати файл',
      selectTransformation: 'Обрати трансформацію',
      selectOutput: 'Обрати вихідну модель',
      search: 'Пошук...',
      noResults: 'Нічого не знайдено',
      dragHint: 'Натисни або перетягни модулі на полотно',
      editNameHint: '(подвійний клік для редагування)',
      modules: 'Модулі',
      toggleSidebar: 'Перемкнути бічну панель',
      dsgvoTooltip: 'Canvas-процеси можуть використовувати зовнішні LLM API. Відповідність DSGVO — відповідальність користувача.',
      batchExecute: 'Пакетне виконання',
      batchExecution: 'Пакетне виконання',
      batchAbort: 'Перервати пакет',
      abort: 'Перервати',
      cancel: 'Скасувати',
      loading: 'Завантаження...',
      executingWorkflow: 'Виконання робочого процесу...',
      starting: 'Запуск...',
      nodes: 'вузли',
      batchRunCount: 'Кількість запусків',
      batchUseSeed: 'Використати базовий Seed',
      batchBaseSeed: 'Базовий Seed',
      batchSeedHint: 'Кожний запуск: seed + індекс',
      batchStart: 'Почати пакет',
      stage: {
        configSelectPlaceholder: 'Обрати...',
        evaluationCriteriaFallback: 'Критерії оцінювання...',
        feedbackInputTitle: 'Введення зворотного зв\'язку',
        deleteTitle: 'Видалити',
        selectLlmPlaceholder: 'Обрати LLM...',
        resizeTitle: 'Змінити розмір',
        input: {
          promptPlaceholder: 'Твій промпт...'
        },
        imageInput: {
          uploadLabel: 'Завантажити зображення'
        },
        interception: {
          contextPromptLabel: 'Контекстний промпт',
          contextPromptPlaceholder: 'Інструкції трансформації...'
        },
        translation: {
          translationPromptLabel: 'Промпт перекладу',
          translationPromptPlaceholder: 'Інструкції перекладу...'
        },
        modelAdaption: {
          targetModelLabel: 'Цільова модель',
          noAdaptionOption: 'Без адаптації',
          videoModelsOption: 'Відео-моделі',
          audioModelsOption: 'Аудіо-моделі'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Критерії порівняння',
          criteriaPlaceholder: 'напр. Порівняти за оригінальністю, чіткістю, деталізацією...',
          infoText: 'Підключіть до 3 текстових виходів'
        },
        seed: {
          modeLabel: 'Режим',
          modeFixed: 'Фіксований',
          modeRandom: 'Випадковий',
          valueLabel: 'Значення',
          baseLabel: 'Базове'
        },
        resolution: {
          customOption: 'Власний',
          widthLabel: 'Ширина',
          heightLabel: 'Висота'
        },
        collector: {
          emptyText: 'Очікування виконання...'
        },
        evaluation: {
          typeLabel: 'Тип оцінювання',
          typeCreativity: 'Креативність',
          typeQuality: 'Якість',
          typeCustom: 'Власний',
          criteriaLabel: 'Критерії оцінювання',
          outputTypeLabel: 'Тип виводу',
          outputCommentary: 'Коментар + Бінарний',
          outputScore: 'Коментар + Бал + Бінарний',
          outputAll: 'Усе',
          evalPassTitle: 'Прийнято (вперед)',
          evalFailTitle: 'Зворотний зв\'язок (назад)',
          evalCommentaryTitle: 'Коментар (вперед)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Обрати Vision-модель...',
          frameworkLabel: 'Рамка аналізу',
          frameworkPanofsky: 'Мистецтвознавча (Панофський)',
          frameworkEducational: 'Освітня теорія',
          frameworkEthical: 'Етична',
          frameworkCritical: 'Критична/деколоніальна',
          frameworkCustom: 'Власна',
          customPromptLabel: 'Промпт аналізу',
          customPromptPlaceholder: 'Опишіть, як слід аналізувати зображення...'
        },
        display: {
          imageAlt: 'Попередній перегляд',
          emptyText: 'Попередній перегляд (після виконання)'
        }
      }
    },
    about: {
      title: 'Про UCDCAE AI LAB',
      intro: 'UCDCAE AI LAB — це педагогічно-мистецька експериментальна платформа Кафедри ЮНЕСКО з цифрової культури та мистецтва в освіті для дослідницького використання генеративного штучного інтелекту в культурно-естетичній медіаосвіті. Вона була розроблена в рамках проєктів AI4ArtsEd та COMeARTS.',
      project: {
        title: 'Проєкт',
        description: 'ШІ трансформує суспільство та світ праці; він дедалі більше стає предметом освіти. Проєкт досліджує можливості, умови та межі педагогічного використання штучного інтелекту (ШІ) в умовах культурної різноманітності культурної освіти.',
        paragraph2: 'У трьох підпроєктах — Загальна педагогіка (TPap), Інформатика (TPinf) та Мистецька освіта (TPkp) — тісно переплітаються орієнтоване на креативність педагогічне дослідження практики ШІ та інформатична концепція й програмування ШІ. Проєкт від початку систематично залучає мистецько-педагогічних практиків до процесу проєктування.',
        paragraph3: 'Партисипативний процес проєктування тривалістю близько двох років має на меті створити технологію ШІ з відкритим вихідним кодом, яка досліджує, наскільки системи ШІ вже можуть враховувати мистецько-педагогічні принципи на структурному рівні за сприятливих реальних умов.',
        paragraph4: 'Фокус зосереджено на: a) майбутній застосовності та доданій вартості високоінноваційних технологій для культурної освіти, b) обсязі та межах ШІ-грамотності вчителів і учнів, та c) загальному питанні оцінюваності та оцінки трансформації педагогічних умов складними нелюдськими акторами з точки зору педагогічної етики та оцінки технологій.',
        moreInfo: 'Більше інформації:'
      },
      subproject: {
        title: 'Підпроєкт «Загальна педагогіка»',
        description: 'Підпроєкт «Загальна педагогіка» досліджує можливості та межі мистецько-педагогічного процесу проєктування ШІ на основі партисипативного дослідження практики в рамках спільного дослідницького питання. З цією метою у першому проєктному році проводяться серії досліджень, аналізів, експертних семінарів та відкритих просторів.'
      },
      team: {
        title: 'Команда',
        projectLead: 'Керівник проєкту',
        leadName: 'Проф. д-р Бенджамін Йоріссен',
        leadInstitute: 'Інститут педагогіки',
        leadChair: 'Кафедра педагогіки з фокусом на культуру та естетичну освіту',
        leadUnesco: 'Кафедра ЮНЕСКО з цифрової культури та мистецтва в освіті',
        researcher: 'Наукова співробітниця',
        researcherName: 'Ванесса Бауманн',
        researcherInstitute: 'Інститут педагогіки',
        researcherChair: 'Кафедра педагогіки з фокусом на культуру та естетичну освіту',
        researcherUnesco: 'Кафедра ЮНЕСКО з цифрової культури та мистецтва в освіті'
      },
      funding: {
        title: 'За підтримки'
      }
    },
    legal: {
      impressum: {
        title: 'Імпресум',
        publisher: 'Видавець',
        represented: 'Представлений президентом',
        responsible: 'Відповідальний за зміст',
        authority: 'Наглядовий орган',
        moreInfo: 'Додаткова інформація',
        moreInfoText: 'Повний імпресум FAU:',
        funding: 'За підтримки'
      },
      privacy: {
        title: 'Політика конфіденційності',
        notice: 'Примітка: Згенерований контент зберігається на сервері для дослідницьких цілей. Жодні дані користувачів або IP-адреси не збираються. Завантажені зображення не зберігаються.',
        usage: 'Використання цієї платформи дозволено виключно для зареєстрованих партнерів UCDCAE AI LAB. Діють домовленості про захист даних, укладені в цьому контексті. Якщо у вас є питання, зверніться до vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Документація та посібник',
      intro: {
        title: 'Ласкаво просимо',
        content: 'Креативні експерименти з ШІ-трансформаціями.'
      },
      gettingStarted: {
        title: 'Початок роботи',
        step1: 'Обери властивості з квадрантів',
        step2: 'Введи текст або зображення',
        step3: 'Запусти трансформацію'
      },
      modes: {
        title: 'Режими',
        mode1: { name: 'Прямий', desc: 'Швидкі експерименти' },
        mode2: { name: 'Текст', desc: 'Текстові трансформації' },
        mode3: { name: 'Зображення', desc: 'Процедури на основі зображень' }
      },
      support: {
        title: 'Підтримка',
        content: 'Для запитань:'
      },
      wikipedia: {
        title: 'Дослідження Wikipedia',
        subtitle: 'Знання про світ як частина мистецьких процесів',
        feature: 'Мистецькі процеси потребують не лише естетичних знань, а й знань про факти у світі. ШІ досліджує Wikipedia під час трансформації, щоб знайти фактичну інформацію.',
        languages: 'Підтримується понад 70 мов',
        languagesDesc: 'ШІ автоматично обирає відповідну мовну Wikipedia для кожної теми:',
        examples: {
          nigeria: 'Тема про Нігерію → хауса, йоруба, ігбо або англійська',
          india: 'Тема про Індію → гінді, тамільська, бенгальська або інші регіональні мови',
          indigenous: 'Корінні культури → кечуа, маорі, інуктитут тощо'
        },
        why: 'Прозорість: Що знає ШІ?',
        whyDesc: 'Система показує всі спроби дослідження: як знайдені статті (у вигляді посилань), так і терміни, для яких нічого не знайдено. Це робить видимим, що ШІ вважає відомим — і що ні.',
        culturalRespect: 'Запрошення дослідити самостійно',
        culturalRespectDesc: 'Показані посилання Wikipedia — це запрошення дізнатися більше самостійно. Натисніть на посилання, щоб перевірити джерела та розширити власні знання.',
        limitations: 'Дослідження ШІ — це допомога, а не заміна власного занурення в тему.'
      }
    },
    multiImage: {
      image1Label: 'Зображення 1',
      image2Label: 'Зображення 2 (необов\'язково)',
      image3Label: 'Зображення 3 (необов\'язково)',
      contextLabel: 'Опишіть, що ви хочете зробити з зображеннями',
      contextPlaceholder: 'напр. Вставте будинок із зображення 2 і коня із зображення 3 у зображення 1. Збережіть кольори та стиль із зображення 1.',
      modeTitle: 'Кілька зображень → Зображення',
      selectConfig: 'Обери модель:',
      generating: 'Зображення зливаються...'
    },
    imageTransform: {
      imageLabel: 'Твоє зображення',
      contextLabel: 'Опиши, що ти хочеш змінити в зображенні',
      contextPlaceholder: 'напр. Перетвори в олійний живопис... Зроби яскравішим... Додай захід сонця...'
    },
    videoGeneration: {
      promptLabel: 'Твоя ідея для відео',
      promptPlaceholder: 'напр. Повітряна куля пливе над гірським пейзажем на заході сонця...',
      modelLabel: 'Обери відео-модель:',
      generating: 'Відео генерується...'
    },
    textTransform: {
      inputLabel: 'Твоя ідея = ЩО?',
      inputTooltip: 'Введи, про що має бути твій витвір.',
      inputPlaceholder: 'напр. Свято на моїй вулиці: ...',
      contextLabel: 'Твої правила = ЯК?',
      contextTooltip: 'Введи, як має бути представлена твоя ідея, або натисни на значок кола!',
      contextPlaceholder: 'напр. Опиши все так, як це сприймають птахи на деревах!',
      resultLabel: 'Ідея + Правила = Промпт',
      resultPlaceholder: 'Промпт з\'явиться після натискання старт (або введи свій текст)',
      optimizedLabel: 'Оптимізований для моделі промпт',
      optimizedPlaceholder: 'Оптимізований промпт з\'явиться після вибору моделі.'
    },
    training: {
      info: {
        title: 'Про навчання LoRA',
        studioDescription: 'Навчайте власні LoRA-моделі для Stable Diffusion 3.5 Large зі своїми зображеннями.',
        description: 'Це вбудоване навчання призначене для швидких тестів.',
        limitations: 'Обмеження',
        limitationDuration: 'Навчання триває 1-3 години',
        limitationBlocking: 'Блокує генерацію зображень під час навчання',
        limitationConfig: 'Обмежені можливості конфігурації',
        showMore: 'Дізнатися більше',
        showLess: 'Показати менше'
      },
      placeholders: {
        projectName: 'напр. Наша шкільна будівля',
        triggerWords: 'напр. наша_школа, шкільний_двір, клас'
      },
      labels: {
        projectName: 'Назва проєкту',
        triggerWords: 'Тригерні слова',
        triggerHelp: 'Теги через кому. Перший = основний тригер, решта = додаткові теги для кожного зображення.',
        images: 'Навчальні зображення (рекомендовано 10–50)',
        dropZone: 'Натисніть або перетягніть зображення сюди',
        imagesSelected: '{count} зображень обрано',
        logs: 'Журнал навчання',
        waiting: 'Очікування початку навчання...'
      },
      buttons: {
        start: 'Почати навчання',
        stop: 'Зупинити',
        inProgress: 'Навчання виконується...',
        delete: 'Видалити файли проєкту (DSGVO)',
        cancel: 'Скасувати'
      },
      vram: {
        title: 'Перевірка VRAM GPU',
        checking: 'Перевірка VRAM...',
        used: 'використано',
        free: 'вільно',
        notEnough: 'Недостатньо вільної VRAM для навчання (потрібно {gb} ГБ).',
        clearQuestion: 'Очистити VRAM для продовження?',
        enough: 'Достатньо VRAM для навчання.',
        clearing: 'Очищення VRAM...',
        newFree: 'Нове вільне',
        clearBtn: 'Очистити VRAM ComfyUI + Ollama'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Віковий фільтр',
      dsgvo_ner: 'DSGVO',
      dsgvo_llm: 'DSGVO',
      translation: '\u2192 EN',
      fast_filter: 'Контент',
      llm_context_check: 'Контент (LLM)',
      llm_safety_check: 'Захист молоді',
      llm_check_failed: 'Перевірка не вдалася',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Твій промпт був нормальний, але згенероване зображення було позначене як невідповідне ШІ-аналізом зображень. Таке трапляється — генерація зображень не завжди передбачувана. Просто спробуй ще раз, кожна генерація інша!',
      para86a: 'Твій промпт було заблоковано, бо він містить символи або терміни, заборонені за німецьким законодавством (§86a КК). Це правило захищає нас усіх від ненависті та насильства. Спробуй іншу тему!',
      dsgvo: 'Твій промпт було заблоковано, бо він містить щось схоже на ім\'я людини. Це захищено Загальним регламентом захисту даних (DSGVO). Використовуй описи, як «дівчинка» або «старий чоловік», замість імен.',
      kids: 'Твій промпт було заблоковано фільтром безпеки для дітей. Деякі терміни не підходять для дітей, бо можуть лякати або бентежити. Спробуй описати свою ідею дружнішими словами!',
      youth: 'Твій промпт було заблоковано фільтром захисту молоді. Деякий контент не підходить і для підлітків. Спробуй переформулювати свою ідею!',
      generic: 'Твій промпт було заблоковано системою безпеки. Система захищає тебе від невідповідного контенту. Спробуй інше формулювання!',
      inputImage: 'Завантажене зображення було позначене як невідповідне ШІ-аналізом зображень. Будь ласка, використай інше зображення.',
      vlmSaw: 'ШІ побачив',
      systemUnavailable: 'Система безпеки (Ollama) не відповідає, тому подальша обробка неможлива. Зверніться до системного адміністратора.',
      suggestionLoading: 'Зачекай, у мене є ідея...',
      suggestionError: 'Зараз не вдалося згенерувати пропозицію. Просто спробуй ще раз іншими словами!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine — Семантичне векторне злиття',
      infoDescription: 'Цей робочий процес зливає два промпти на рівні семантичних векторів. Результат — не просте змішування, а глибше математичне з\'єднання просторів значень.',
      purposeTitle: 'Педагогічна мета',
      purposeText: 'Досліджуй, як моделі ШІ представляють значення у вигляді числових просторів. Що відбувається, коли ми математично зливаємо різні концепції?',
      techTitle: 'Технічні деталі',
      techText: 'Модель: SD3.5 Large | Кодувальник: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Часткова елімінація — Деконструкція вектора',
      infoDescription: 'Цей робочий процес цілеспрямовано маніпулює частинами семантичного вектора. Елімінуючи певні виміри, ми можемо спостерігати, які аспекти значення втрачаються.',
      purposeTitle: 'Педагогічна мета',
      purposeText: 'Зрозумій, як значення кодується через різні виміри векторного простору. Що залишається, коли ми «вимикаємо» частини?',
      techTitle: 'Технічні деталі',
      techText: 'Модель: SD3.5 Large | Кодувальник: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Текстовий кодувальник',
      modeLabel: 'Режим елімінації',
      dimensionRange: 'Діапазон вимірів',
      selected: 'Обрано',
      dimensions: 'Виміри',
      emptyTitle: 'Очікування генерації...',
      emptySubtitle: 'Результати з\'являться тут',
      referenceLabel: 'Еталонне зображення',
      referenceDesc: 'Без маніпуляцій (оригінал)',
      innerLabel: 'Внутрішній діапазон елімінований',
      outerLabel: 'Зовнішній діапазон елімінований'
    },
    surrealizer: {
      infoTitle: 'Сюрреалізатор — Екстраполяція за межі відомого',
      infoDescription: 'Два ШІ-«мозки» читають твій текст: CLIP-L розуміє мову через зображення, T5 — суто лінгвістично. Повзунок не просто змішує між ними — він штовхає зображення далеко за межі того, що T5 сам по собі створив би. ШІ має інтерпретувати вектори, яких він ніколи не зустрічав під час навчання. Результат: ШІ-галюцинації — зображення, які жоден промпт не міг би створити безпосередньо.',
      purposeTitle: 'Повзунок',
      purposeText: 'α < 0: CLIP-L підсилений, T5 інвертований — верхні 3328 вимірів (де CLIP-L заповнений нулями) отримують інвертовані вектори T5. Паттерни крос-уваги в трансформері перевертаються: візуально зумовлені галюцинації. ◆ α = 0: чистий CLIP-L — нормальне зображення. ◆ α = 1: чистий T5-XXL — ще нормально, але інша якість. ◆ α > 1: екстраполяція за T5. При α = 20 формула штовхає вкладення у 19× далі за T5 у незвіданий векторний простір — лінгвістично зумовлені галюцинації. ◆ Оптимальна зона: α = 15–35.',
      techTitle: 'Як це працює',
      techText: 'Твій промпт відправляється через два кодувальники окремо: CLIP-L (візуально навчений, 77 токенів, 768 вимірів → доповнений до 4096) та T5-XXL (лінгвістично навчений, 512 токенів, 4096 вимірів). Перші 77 позицій токенів зливаються: (1-α)·CLIP-L + α·T5. Решта токенів T5 (78–512) залишаються без змін як семантичний якір — вони прив\'язують зображення до тексту незалежно від екстремальності α. При α > 1 це не змішування, а екстраполяція: вектори, яких жодне навчання ніколи не створювало. При α < 0 T5 інвертується, а CLIP-L підсилюється — якісно інші галюцинації, тому що паттерни крос-уваги в трансформері інвертовані.',
      sliderLabel: 'Екстраполяція (α)',
      sliderNormal: 'нормально',
      sliderWeird: 'дивно',
      sliderCrazy: 'божевільно',
      sliderExtremeWeird: 'супер дивно',
      sliderExtremeCrazy: 'супер божевільно',
      sliderHint: "α<0: за CLIP {'|'} α=0: чистий CLIP {'|'} α=1: чистий T5 {'|'} α>1: за T5",
      expandLabel: 'Розширити промпт для T5',
      expandSuggest: 'Виявлено короткий промпт — розширення T5 значно покращує результати з малою кількістю слів.',
      expandHint: 'Твій промпт має мало слів (~{count} токенів CLIP). Для оптимальних галюцинацій ШІ може наративно розширити контекст T5.',
      expandActive: 'Розширення промпту...',
      expandResultLabel: 'Розширення T5 (лише кодувальник T5)',
      advancedLabel: 'Розширені налаштування',
      negativeLabel: 'Негативний промпт',
      negativeHint: 'Екстрапольований з тим самим α. Визначає, від чого зображення екстраполюється ГЕТЬ — різні негативні промпти створюють принципово різну естетику.',
      cfgLabel: 'CFG Scale',
      cfgHint: 'Classifier-Free Guidance: сила впливу промпту. Вище = сильніший ефект, менше варіацій.'
    },
    musicGeneration: {
      infoTitle: 'Генерація музики',
      infoDescription: 'Створюй музику з тексту та стильових тегів. ШІ генерує мелодії, ритми та гармонії на основі твоїх текстів та жанрових вказівок.',
      purposeTitle: 'Педагогічна мета',
      purposeText: 'Досліджуй, як ШІ інтерпретує музичні концепції. Як вибір слів у тексті впливає на мелодію?',
      lyricsLabel: 'Текст пісні',
      lyricsPlaceholder: '[Verse]\nТвій текст тут...\n\n[Chorus]\nПриспів...',
      tagsLabel: 'Стильові теги',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: 'Обери музичну модель:',
      generate: 'Згенерувати музику',
      generating: 'Генерація музики...'
    },
    musicGen: {
      simpleMode: 'Простий',
      advancedMode: 'Розширений',
      lyricsLabel: 'Текст',
      lyricsPlaceholder: 'Напиши текст пісні з маркерами структури, такими як [Verse], [Chorus], [Bridge]...\n\nПриклад:\n[Verse]\nду ду ду ду\nбла бла бла бла\n\n[Chorus]\nце все що я хочу тобі заспівати',
      tagsLabel: 'Стильові теги',
      tagsPlaceholder: 'Жанр, настрій, інструменти...\n\nПриклад: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: 'Покращити текст і теги',
      refinedLyricsLabel: 'Покращений текст',
      refinedLyricsPlaceholder: 'Покращений текст з\'явиться тут...',
      refiningLyricsMessage: 'ШІ покращує текст пісні...',
      refinedTagsLabel: 'Покращені теги',
      refinedTagsPlaceholder: 'Покращені стильові теги з\'являться тут...',
      refiningTagsMessage: 'ШІ генерує відповідні стильові теги...',
      selectModel: 'Обери музичну модель',
      generateButton: 'Згенерувати музику',
      quality: 'Якість'
    },
    musicGenV2: {
      lyricsWorkshop: 'Майстерня текстів',
      lyricsInput: 'Твій текст',
      lyricsPlaceholder: 'Напиши текст, тему, ключові слова або настрій...',
      themeToLyrics: 'Ключові слова → Текст пісні',
      refineLyrics: 'Структурувати текст пісні',
      resultLabel: 'Результат',
      resultPlaceholder: 'Текст пісні з\'явиться тут...',
      expandingTheme: 'ШІ пише текст пісні з твоїх ключових слів...',
      refiningLyrics: 'ШІ структурує текст пісні...',
      soundExplorer: 'Звуковий дослідник',
      suggestFromLyrics: 'Запропонувати з тексту',
      suggestingTags: 'ШІ аналізує текст...',
      mostImportant: 'найважливіше',
      dimGenre: 'Жанр',
      dimTimbre: 'Тембр',
      dimGender: 'Голос',
      dimMood: 'Настрій',
      dimInstrument: 'Інструменти',
      dimScene: 'Сцена',
      dimRegion: 'Регіон (ЮНЕСКО)',
      dimTopic: 'Тема',
      audioLength: 'Тривалість аудіо',
      generateButton: 'Згенерувати музику',
      selectModel: 'Модель',
      customTags: 'Власні теги',
      customTagsPlaceholder: 'напр. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: 'Лабораторія зображень',
        textlab: 'Латентна текстова лабораторія',
        crossmodal: 'Крос-модальна лабораторія'
      },
      imageLab: {
        headerTitle: 'Лабораторія зображень — Візуальне дослідження векторного простору',
        headerSubtitle: 'П\'ять інструментів для дослідження того, як дифузійні моделі генерують зображення з тексту: від шумоочищення через увагу та злиття до векторної арифметики.',
        tabs: {
          archaeology: {
            label: 'Археологія шумоочищення',
            short: 'Спостерігайте за роботою моделі'
          },
          attention: {
            label: 'Картографія уваги',
            short: 'Подивіться, куди дивиться модель'
          },
          fusion: {
            label: 'Злиття кодувальників',
            short: 'Сюрреалістичне змішування'
          },
          probing: {
            label: 'Зондування ознак',
            short: 'Аналіз на рівні вимірів'
          },
          algebra: {
            label: 'Алгебра концептів',
            short: 'Векторна арифметика'
          }
        }
      },
      comingSoon: 'Цей інструмент буде реалізований у майбутній версії.',
      shared: {
        negativeHint: 'Терміни, яких модель має активно уникати (напр. "розмите, текст")',
        stepsHint: 'Більше кроків = вища якість, але довший час генерації',
        cfgHint: 'Classifier-Free Guidance: вище = сильніша відповідність промпту, менше варіацій',
        seedHint: '-1 = випадково, фіксоване значення = відтворюваний результат',
        recordingActive: 'Запис активний',
        recordingCount: '{count} запис | {count} записів',
        recordingTooltip: 'Дослідницькі дані зберігаються автоматично',
      },
      attention: {
        headerTitle: 'Картографія уваги — Яке слово керує якою частиною зображення?',
        headerSubtitle: 'Для кожного слова промпту теплова карта на згенерованому зображенні показує, ДЕ саме це слово мало найбільший вплив. Це розкриває, як модель просторово розподіляє семантичні концепції.',
        explanationToggle: 'Показати детальне пояснення',
        explainWhatTitle: 'Що показує цей інструмент?',
        explainWhatText: 'Коли дифузійна модель генерує зображення, вона не читає промпт слово за словом як набір інструкцій. Натомість механізм «увага» розподіляє вплив кожного слова по різних областях зображення. Слово «будинок» переважно впливає на область, де з\'являється будинок — але також на сусідні ділянки, бо модель розуміє контекст усієї сцени. Цей інструмент робить цей розподіл видимим: натисни на слово і подивись, які частини зображення підсвічуються.',
        explainHowTitle: 'Як читати теплову карту?',
        explainHowText: 'Яскравий, насичений колір = сильний вплив слова на цю область. Темний або відсутній колір = слабкий вплив. При виборі кількох слів вони з\'являються в різних кольорах. Зауваж: карти НЕ мають ідеально чітких меж — це не помилка, а показує, що модель обробляє концепції контекстуально, а не ізольовано.',
        explainReadTitle: 'Що розкривають два повзунки?',
        explainReadText: 'Повзунок кроку шумоочищення показує, КОЛИ у 25-кроковому процесі генерації ви переглядаєте увагу. Ранні кроки показують планування грубої композиції, пізні — присвоєння деталей. Селектор глибини мережі показує, ДЕ в трансформері вимірюється увага: мілкі шари (ближче до входу) показують планування глобальної композиції, середні — семантичне присвоєння, глибокі — тонке налаштування.',
        techTitle: 'Технічні деталі',
        techText: 'SD3.5 використовує MMDiT (Мультимодальний дифузійний трансформер) зі спільною увагою: токени зображення та тексту взаємодіють через 24 блоки трансформера. Ми замінюємо стандартний SDPA-процесор на ручний softmax(QK^T/√d) процесор у 3 обраних блоках для вилучення підматриці уваги текст→зображення. Карти мають роздільність 64x64 (сітка патчів), збільшену до роздільності зображення через білінійну інтерполяцію.',
        referencesTitle: 'Наукові джерела',
        promptLabel: 'Промпт',
        promptPlaceholder: 'напр. Будинок стоїть у ландшафті, оточений сільськогосподарськими угіддями, природою та тваринами. Видно кількох людей.',
        generate: 'Згенерувати + Аналізувати',
        generating: 'Генерація зображення та вилучення уваги...',
        emptyHint: 'Введи промпт і натисни Згенерувати, щоб візуалізувати карти уваги моделі.',
        advancedLabel: 'Розширені налаштування',
        negativeLabel: 'Негативний промпт',
        stepsLabel: 'Кроки',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Токени',
        tokensHint: 'Натисни на одне або більше слів. Підслівні токени (напр. «Бу»+«ди»+«нок») об\'єднуються автоматично. Декілька слів з\'являються різними кольорами.',
        timestepLabel: 'Крок шумоочищення',
        timestepHint: 'Дифузійні моделі генерують зображення за 25 кроків від шуму до зображення. Ранні кроки встановлюють грубу структуру, пізні — уточнюють деталі. Цей повзунок показує, на що модель звертає увагу на кожному кроці.',
        step: 'Крок',
        layerLabel: 'Глибина мережі',
        layerHint: 'На кожному кроці шумоочищення сигнал проходить через усі 24 шари трансформера. Мілкі шари (ближче до входу) фіксують глобальну композицію, середні — семантичне присвоєння, глибокі (ближче до виходу) — дрібні деталі.',
        layerEarly: 'Мілкий (Композиція)',
        layerMid: 'Середній (Семантика)',
        layerLate: 'Глибокий (Деталі)',
        opacityLabel: 'Теплова карта',
        opacityHint: 'Інтенсивність кольорового накладення на зображення.',
        baseImageLabel: 'Базове зображення',
        baseColor: 'Колір',
        baseBW: 'Ч/Б',
        baseOff: 'Вимк.',
        baseImageHint: 'Колір показує оригінальне зображення. Ч/Б знебарвлює його, щоб кольори теплової карти виділялися. Вимк. ховає зображення повністю і показує лише карту уваги.',
        encoderLabel: 'Текстовий кодувальник',
        encoderClipL: 'CLIP-L (77 токенів)',
        encoderT5: 'T5-XXL (512 токенів)',
        encoderHint: 'SD3.5 використовує два текстових кодувальники з різною токенізацією. CLIP-L використовує BPE (Byte-Pair Encoding), T5-XXL — SentencePiece. Порівняйте, як обидва кодувальники обробляють один промпт і якими областями зображення кожен керує.',
        download: 'Завантажити зображення'
      },
      probing: {
        headerTitle: 'Зондування ознак — Які виміри що кодують?',
        headerSubtitle: 'Порівняйте два промпти і дізнайтеся, які виміри вкладень кодують семантичну різницю. Вибірково переносіть окремі виміри, щоб побачити їх вплив на зображення.',
        explanationToggle: 'Показати детальне пояснення',
        explainWhatTitle: 'Що показує цей інструмент?',
        explainWhatText: 'Кожне слово перетворюється текстовим кодувальником у високовимірний вектор (напр. 4096 вимірів для T5). Коли ви змінюєте слово в промпті — напр. «червоний» на «синій» — певні виміри змінюються більше за інші. Цей інструмент показує, ЯКІ виміри змінюються найбільше, і дозволяє вибірково переносити окремі виміри з промпту B в промпт A.',
        explainHowTitle: 'Як працює перенесення?',
        explainHowText: 'Стовпчаста діаграма показує всі виміри, відсортовані за величиною різниці. Використовуйте елементи управління діапазоном рангів (Від/До), щоб обрати вікно — напр. лише топ-100 або конкретно ранги 880–920. Натискання «Перенести» регенерує зображення з тими ж налаштуваннями (той самий seed!) — але з обраними вимірами від промпту B.',
        explainReadTitle: 'Як читати стовпчасту діаграму?',
        explainReadText: 'Кожна стовпчик представляє один вимір вкладення. Довжина показує, наскільки цей вимір відрізняється між промптами A і B. Виміри з великими різницями — найімовірніші носії семантичної зміни. Але зауважте: вкладення розподілені — часто для видимої зміни потрібно декілька вимірів одночасно.',
        techTitle: 'Технічні деталі',
        techText: 'SD3.5 використовує три текстових кодувальники: CLIP-L (768 вимірів), CLIP-G (1280 вимірів) та T5-XXL (4096 вимірів). Можна зондувати кожен окремо. Різниця обчислюється як середнє абсолютне відхилення по всіх позиціях токенів.',
        referencesTitle: 'Наукові джерела',
        promptALabel: 'Промпт A (Оригінал)',
        promptBLabel: 'Промпт B (Порівняння)',
        promptAPlaceholder: 'напр. Червоний будинок біля озера',
        promptBPlaceholder: 'напр. Синій будинок біля озера',
        encoderLabel: 'Кодувальник',
        encoderAll: 'Усі (рекомендовано)',
        encoderClipL: 'CLIP-L (768 вимірів)',
        encoderClipG: 'CLIP-G (1280 вимірів)',
        encoderT5: 'T5-XXL (4096 вимірів)',
        analyzeBtn: 'Аналізувати',
        analyzing: 'Кодування та порівняння промптів...',
        transferBtn: 'Перенести обрані векторні виміри з промпту B у згенероване зображення',
        transferring: 'Генерація зображення зі зміненим вкладенням...',
        rankFromLabel: 'Від рангу',
        rankToLabel: 'До рангу',
        sliderLabel: 'Обрати виміри з промпту B',
        range1Label: 'Діапазон 1',
        range2Label: 'Діапазон 2',
        addRange: 'Додати діапазон',
        selectionDesc: '{count} вимірів з промпту B обрано (ранг {ranges} з {total})',
        listTitle: '{count} вимірів з промпту B з найбільшою різницею до промпту A',
        sortAsc: 'За зростанням',
        sortDesc: 'За спаданням',
        originalLabel: 'Оригінал (промпт A)',
        modifiedLabel: 'Змінений (перенесення з промпту B)',
        modifiedHint: 'Оберіть діапазон рангів нижче та натисніть «Перенести» — тут з\'явиться промпт A з перенесеними вимірами з B (той самий seed).',
        noDifference: 'Вкладення ідентичні — змініть промпт B.',
        advancedLabel: 'Розширені налаштування',
        negativeLabel: 'Негативний промпт',
        stepsLabel: 'Кроки',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'Усі',
        selectNone: 'Жоден',
        encoderHint: 'Усі = всі енкодери разом. CLIP-L/CLIP-G/T5 = ізолює один енкодер для аналізу.',
        sliderHint: 'Виберіть діапазон рангів найважливіших вимірів ембедингу (відсортовано за різницею між A та B).',
        transferHint: 'Переносить обрані виміри з промпту B на промпт A та генерує нове зображення.',
        downloadOriginal: 'Завантажити оригінал',
        downloadModified: 'Завантажити змінений'
      },
      algebra: {
        headerTitle: 'Алгебра концептів — Векторна арифметика на вкладеннях зображень',
        headerSubtitle: 'Застосування відомої аналогії word2vec до генерації зображень: Король \u2212 Чоловік + Жінка \u2248 Королева. Три промпти кодуються та алгебраїчно комбінуються.',
        explanationToggle: 'Показати детальне пояснення',
        explainWhatTitle: 'Що показує цей інструмент?',
        explainWhatText: 'У 2013 році Міколов показав, що вкладення слів кодують семантичні зв\'язки як лінійні напрямки: вектор «Король» мінус «Чоловік» плюс «Жінка» дає вектор, близький до «Королева». Цей інструмент застосовує цю ідею до текстових кодувальників SD3.5.',
        explainHowTitle: 'Як працює алгебра — і чому не просто негативний промпт?',
        explainHowText: 'Ви вводите три промпти: A (база), B (відняти) та C (додати). Формула: Результат = A \u2212 Масштаб\u2081\u00d7B + Масштаб\u2082\u00d7C. Повзунки масштабу контролюють інтенсивність. Негативний промпт робить щось принципово інше: він керує процесом шумоочищення ГЕТЬ від B на КОЖНОМУ з 25 кроків. Алгебра концептів натомість обчислює новий вектор ДО генерації зображення: віднімання відбувається у просторі вкладень, а не в процесі дифузії.',
        explainReadTitle: 'Що означають результати?',
        explainReadText: 'Зліва — еталонне зображення (лише промпт A, той самий seed). Справа — результат алгебри. Якщо аналогія працює, праве зображення має показувати концепт A, але зі семантичною зміною B→C.',
        techTitle: 'Технічні деталі',
        techText: 'Алгебра виконується на вкладеннях обраного кодувальника: CLIP-L (768 вимірів), CLIP-G (1280 вимірів), T5-XXL (4096 вимірів) або усі разом. Та сама операція також застосовується до об\'єднаних вкладень (2048 вимірів). Обидва зображення використовують той самий seed для справедливого порівняння.',
        referencesTitle: 'Наукові джерела',
        promptALabel: 'Промпт A (База)',
        promptAPlaceholder: 'напр. Захід сонця на пляжі з пальмами',
        promptBLabel: 'Промпт B (Відняти)',
        promptBPlaceholder: 'напр. Пляж з пальмами',
        promptCLabel: 'Промпт C (Додати)',
        promptCPlaceholder: 'напр. Засніжені гори',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Кодувальник',
        encoderAll: 'Усі (рекомендовано)',
        encoderClipL: 'CLIP-L (768 вимірів)',
        encoderClipG: 'CLIP-G (1280 вимірів)',
        encoderT5: 'T5-XXL (4096 вимірів)',
        generateBtn: 'Обчислити',
        generating: 'Обчислення вкладень та генерація зображень...',
        referenceLabel: 'Еталон (промпт A)',
        resultLabel: 'Результат (A \u2212 B + C)',
        l2Label: 'L2 відстань від оригіналу',
        advancedLabel: 'Розширені налаштування',
        negativeLabel: 'Негативний промпт',
        stepsLabel: 'Кроки',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Масштаб віднімання',
        scaleAddLabel: 'Масштаб додавання',
        encoderHint: 'Усі = всі енкодери разом. CLIP-L/CLIP-G/T5 = ізолює один енкодер для арифметики.',
        scaleSubHint: 'Вага віднімання (B). Вище = сильніше видалення концепту B.',
        scaleAddHint: 'Вага додавання (C). Вище = сильніше додавання концепту C.',
        l2Hint: 'Евклідова відстань у просторі ембедингів. Менше = більш схожі, більше = більш різні.',
        downloadReference: 'Завантажити еталон',
        downloadResult: 'Завантажити результат',
        resultHint: 'Введіть три промпти та натисніть Обчислити — результат векторної арифметики з\'явиться тут.'
      },
      archaeology: {
        headerTitle: 'Археологія шумоочищення — Як шум стає зображенням?',
        headerSubtitle: 'Спостерігай кожен крок шумоочищення. Дифузійні моделі не малюють зліва направо — вони працюють скрізь одночасно, від грубих форм до дрібних деталей.',
        explanationToggle: 'Показати детальне пояснення',
        explainWhatTitle: 'Що показує цей інструмент?',
        explainWhatText: 'Дифузійна модель створює зображення, поступово видаляючи шум. На відміну від малювання зліва направо, модель працює над УСІМА областями зображення одночасно. На перших кроках виникають грубі структури: де верх, де низ? Де горизонт? На середніх кроках з\'являється семантичний зміст: об\'єкти, форми, кольори. Останні кроки уточнюють текстури та деталі.',
        explainHowTitle: 'Як користуватися цим інструментом?',
        explainHowText: 'Введи промпт і натисни Згенерувати. Модель створить 25 проміжних зображень (одне на крок шумоочищення). Вони з\'являться як стрічка знизу. Натисни на мініатюру або використай повзунок часової шкали для перегляду кожного кроку у повному розмірі.',
        explainReadTitle: 'Що розкривають три фази?',
        explainReadText: 'Ранні кроки (1–8): Глобальна композиція — базова структура, розподіл кольорів, планування макету. Середні кроки (9–17): Семантичне виникнення — об\'єкти стають розпізнаваними, форми кристалізуються. Пізні кроки (18–25): Уточнення деталей — текстури, краї, дрібні візерунки.',
        techTitle: 'Технічні деталі',
        techText: 'SD3.5 Large використовує Rectified Flow як планувальник з 25 кроками за замовчуванням. На кожному кроці поточні латентні вектори декодуються через VAE (1024\u00d71024 JPEG). VAE перетворює математичний латентний простір у пікселі.',
        referencesTitle: 'Наукові джерела',
        promptLabel: 'Промпт',
        promptPlaceholder: 'напр. Ринок у середньовічному місті з людьми, будівлями та фонтаном',
        generate: 'Згенерувати',
        generating: 'Генерація зображення — запис кожного кроку...',
        emptyHint: 'Введи промпт і натисни Згенерувати, щоб візуалізувати процес шумоочищення.',
        advancedLabel: 'Розширені налаштування',
        negativeLabel: 'Негативний промпт',
        stepsLabel: 'Кроки',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Стрічка шумоочищення',
        timelineLabel: 'Крок',
        phaseEarly: 'Композиція',
        phaseMid: 'Семантика',
        phaseLate: 'Деталі',
        phaseEarlyDesc: 'Виникає глобальна структура та розподіл кольорів',
        phaseMidDesc: 'Об\'єкти та форми стають розпізнаваними',
        phaseLateDesc: 'Текстури та дрібні деталі уточнюються',
        finalImageLabel: 'Кінцеве зображення (повна роздільність)',
        timelineHint: 'Прокручує крізь кроки видалення шуму — показує, як зображення виникає від шуму до фінальної композиції.',
        download: 'Завантажити зображення'
      },
      textLab: {
        headerTitle: 'Латентна текстова лабораторія — Наукова деконструкція LLM',
        headerSubtitle: 'Representation Engineering, порівняльна модельна археологія та систематичний аналіз упередженості: три дослідницькі інструменти для вивчення мовних моделей.',
        explanationToggle: '\u041f\u043e\u043a\u0430\u0437\u0430\u0442\u0438 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u043d\u044f',
        modelPanel: {
          presetLabel: 'Набір',
          presetNone: 'Без набору (власний ID)',
          customModelLabel: 'ID моделі HuggingFace',
          customModelPlaceholder: 'напр. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Квантизація',
          quantAuto: 'Авто',
          quantizationHint: 'bf16 = повна якість, int8 = половина VRAM, int4 = мінімум VRAM але найнижча якість',
        },
        temperatureHint: 'Випадковість генерації тексту. Низько = детерміновано, високо = креативніше.',
        maxTokensHint: 'Максимальна кількість згенерованих токенів (частин слів).',
        textSeedHint: '-1 = випадково, фіксоване значення = відтворюваний результат',
        tabs: {
          repeng: { label: '\u0406\u043d\u0436\u0435\u043d\u0435\u0440\u0456\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044c', short: '\u041f\u043e\u0448\u0443\u043a \u043a\u0435\u0440\u0443\u044e\u0447\u0438\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u0456\u0432 \u0443 LLM' },
          compare: { label: '\u041f\u043e\u0440\u0456\u0432\u043d\u044f\u043d\u043d\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439', short: '\u041f\u043e\u0440\u0456\u0432\u043d\u044f\u043d\u043d\u044f \u0434\u0432\u043e\u0445 LLM \u043f\u043e\u0448\u0430\u0440\u043e\u0432\u043e' },
          bias: { label: '\u0410\u0440\u0445\u0435\u043e\u043b\u043e\u0433\u0456\u044f \u0443\u043f\u0435\u0440\u0435\u0434\u0436\u0435\u043d\u044c', short: '\u0412\u0438\u044f\u0432\u043b\u0435\u043d\u043d\u044f \u043f\u0440\u0438\u0445\u043e\u0432\u0430\u043d\u0438\u0445 \u0443\u043f\u0435\u0440\u0435\u0434\u0436\u0435\u043d\u044c \u0443 LLM' },
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: 'Знайдіть напрямки концептів у просторі активацій та керуйте генерацією',
          explainWhatTitle: '\u0429\u043e \u043f\u043e\u043a\u0430\u0437\u0443\u0454 \u0446\u0435\u0439 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442?',
          explainWhatText: '\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0456 Zou \u0442\u0430 \u0456\u043d. (2023) \u00abRepresentation Engineering\u00bb \u0442\u0430 Li \u0442\u0430 \u0456\u043d. (2024). LLM \u043a\u043e\u0434\u0443\u044e\u0442\u044c \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u0456 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0438 \u044f\u043a \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0438 \u0443 \u043f\u0440\u043e\u0441\u0442\u043e\u0440\u0456 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0456\u0439. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: '\u042f\u043a \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0442\u0438\u0441\u044f?',
          explainHowText: '\u0426\u0435\u0439 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442 \u0432\u0438\u0442\u044f\u0433\u0443\u0454 \u00ab\u043d\u0430\u043f\u0440\u044f\u043c\u043e\u043a \u0456\u0441\u0442\u0438\u043d\u0438\u00bb \u0437 \u043c\u043e\u0434\u0435\u043b\u0456. \u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u044f: \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u0456 \u043f\u0440\u043e\u043c\u043f\u0442\u0438 \u043f\u0440\u0430\u0446\u044e\u044e\u0442\u044c \u0437\u043d\u0430\u0447\u043d\u043e \u043a\u0440\u0430\u0449\u0435.',
          referencesTitle: 'Наукові джерела',
          expectedResults: 'Очікувані результати: При α = 0 (базовий) модель генерує правильну відповідь. При α = -1 (інверсія) має з\'явитися неправильна відповідь. При α = +1 мало що змінюється. За межами |α| > 2 домінують артефакти.',
          pairsTitle: 'Контрастні пари',
          pairsSubtitle: 'Рекомендовано щонайменше 3 пари. Кожна пара має відрізнятися лише в цільовому концепті (правда vs. брехня).',
          positiveLabel: 'Позитивне (правда)',
          negativeLabel: 'Негативне (брехня)',
          positivePlaceholder: 'напр. The capital of France is Paris',
          negativePlaceholder: 'напр. The capital of France is Berlin',
          addPair: 'Додати пару',
          removePair: 'Видалити',
          targetLayerLabel: 'Цільовий шар',
          targetLayerHint: 'Який шар трансформера отримує вектор керування. Різні шари впливають на різні аспекти генерації тексту.',
          targetLayerAuto: 'Останній шар',
          findDirection: 'Знайти напрямок',
          finding: 'Обчислення напрямку концепту...',
          directionFound: 'Напрямок концепту знайдено',
          varianceLabel: 'Пояснена дисперсія',
          dimLabel: 'Виміри',
          projectionsTitle: 'Проєкції контрастних пар',
          testTitle: 'Тест + Маніпуляція',
          testSubtitle: 'Введіть речення та керуйте генерацією вздовж напрямку концепту',
          testPromptLabel: 'Тестовий промпт',
          testPromptPlaceholder: 'напр. The capital of Germany is',
          alphaLabel: 'Сила маніпуляції (α)',
          alphaHint: 'Сила вектора керування. 0 = без ефекту, вище = сильніший вплив контрастних пар.',
          temperatureLabel: 'Температура',
          maxTokensLabel: 'Макс. токенів',
          seedLabel: 'Seed (-1 = випадковий)',
          generateBtn: 'Згенерувати з маніпуляцією',
          generating: 'Виконання маніпульованої генерації...',
          baselineLabel: 'Базовий (без маніпуляції)',
          manipulatedLabel: 'Маніпульований (α = {alpha})',
          projectionLabel: 'Проєкція на напрямок концепту',
          interpretationTitle: 'Інтерпретація',
          interpreting: 'Аналіз результатів...',
          interpretationError: 'Не вдалося згенерувати інтерпретацію'
        },
        compare: {
          title: 'Порівняльна модельна археологія',
          subtitle: 'Завантажте дві моделі та систематично порівняйте їх внутрішні представлення',
          explainWhatTitle: '\u0429\u043e \u043f\u043e\u043a\u0430\u0437\u0443\u0454 \u0446\u0435\u0439 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442?',
          explainWhatText: '\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0456 Belinkov (2022) \u0442\u0430 Olsson \u0442\u0430 \u0456\u043d. (2022). \u0422\u0435\u043f\u043b\u043e\u0432\u0430 \u043a\u0430\u0440\u0442\u0430 \u043f\u043e\u043a\u0430\u0437\u0443\u0454 CKA \u043c\u0456\u0436 \u0448\u0430\u0440\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: '\u042f\u043a \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0442\u0438\u0441\u044f?',
          explainHowText: '\u041c\u043e\u0434\u0435\u043b\u044c A \u2014 \u0430\u043a\u0442\u0438\u0432\u043d\u0438\u0439 \u043d\u0430\u0431\u0456\u0440. \u041e\u0431\u0435\u0440\u0456\u0442\u044c \u0434\u0440\u0443\u0433\u0443 \u043c\u043e\u0434\u0435\u043b\u044c (B) \u0442\u0430 \u0437\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0442\u0435. \u0412\u0432\u0435\u0434\u0456\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0442\u0430 \u043d\u0430\u0442\u0438\u0441\u043d\u0456\u0442\u044c \u00ab\u041f\u043e\u0440\u0456\u0432\u043d\u044f\u0442\u0438\u00bb.',
          referencesTitle: 'Наукові джерела',
          modelATitle: 'Модель A (з вибору набору)',
          modelAHint: 'Змініть через випадне меню набору вгорі',
          modelBTitle: 'Модель B (друга модель)',
          modelBPresetLabel: 'Набір',
          modelBCustomLabel: 'ID моделі HuggingFace',
          modelBCustomPlaceholder: 'напр. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Завантажити модель B',
          modelBLoaded: 'Модель B завантажена',
          modelBNone: 'Модель B не завантажена',
          promptLabel: 'Промпт',
          promptPlaceholder: 'напр. The cat sat on the mat and watched the birds',
          seedLabel: 'Seed',
          temperatureLabel: 'Температура',
          maxTokensLabel: 'Макс. токенів',
          compareBtn: 'Порівняти',
          comparing: 'Порівняння моделей...',
          heatmapTitle: 'Вирівнювання шарів (CKA)',
          heatmapAxisA: 'Модель A — Шари',
          heatmapAxisB: 'Модель B — Шари',
          heatmapExplain: 'Яскраві комірки = висока подібність представлень. Діагональні патерни показують, що моделі обробляють інформацію у подібному порядку.',
          attentionTitle: 'Порівняння уваги (останній шар)',
          modelALabel: 'Модель A',
          modelBLabel: 'Модель B',
          generationTitle: 'Порівняння генерації (той самий seed)',
          layerStatsTitle: 'Статистика шарів',
          interpretationTitle: 'Інтерпретація',
          interpreting: 'Аналіз результатів...',
          interpretationError: 'Не вдалося згенерувати інтерпретацію'
        },
        bias: {
          title: 'Археологія упередженості',
          subtitle: 'Систематичні експерименти з упередженістю через контрольовану маніпуляцію токенами',
          explainWhatTitle: '\u0429\u043e \u043f\u043e\u043a\u0430\u0437\u0443\u0454 \u0446\u0435\u0439 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442?',
          explainWhatText: '\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0456 Zou \u0442\u0430 \u0456\u043d. (2023) \u0442\u0430 Bricken \u0442\u0430 \u0456\u043d. (2023). \u0426\u0435\u0439 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043e\u0441\u043b\u0456\u0434\u0436\u0443\u0454 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u0456 \u0443\u043f\u0435\u0440\u0435\u0434\u0436\u0435\u043d\u043d\u044f. \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: '\u042f\u043a \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0442\u0438\u0441\u044f?',
          explainHowText: '\u041e\u0431\u0435\u0440\u0456\u0442\u044c \u0442\u0438\u043f \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0443. \u0412\u0432\u0435\u0434\u0456\u0442\u044c \u043f\u0440\u043e\u043c\u043f\u0442 \u0434\u043b\u044f \u043f\u0440\u043e\u0434\u043e\u0432\u0436\u0435\u043d\u043d\u044f. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0438 \u043f\u043e\u043a\u0430\u0437\u0443\u044e\u0442\u044c \u0431\u0430\u0437\u043e\u0432\u0456 \u0442\u0430 \u043c\u0430\u043d\u0456\u043f\u0443\u043b\u044c\u043e\u0432\u0430\u043d\u0456 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0456\u0457.',
          referencesTitle: 'Наукові джерела',
          presetLabel: 'Тип експерименту',
          presetGender: 'Гендер — Пригнічення гендерованих займенників',
          presetSentiment: 'Сентимент — Підсилення позитивного/негативного',
          presetDomain: 'Домен — Підсилення наукового/поетичного',
          presetCustom: 'Власний експеримент',
          promptLabel: 'Промпт',
          promptPlaceholder: 'напр. The doctor said to the patient',
          customBoostLabel: 'Підсилити токени (через кому)',
          customBoostPlaceholder: 'напр. dark,shadow,night',
          customSuppressLabel: 'Пригнічити токени (через кому)',
          customSuppressPlaceholder: 'напр. light,sun,bright',
          numSamplesLabel: 'Зразків на умову',
          temperatureLabel: 'Температура',
          maxTokensLabel: 'Макс. токенів',
          seedLabel: 'Базовий seed',
          runBtn: 'Запустити експеримент',
          running: 'Виконання експерименту з упередженістю...',
          baselineTitle: 'Базовий (без маніпуляції)',
          groupTitle: 'Група: {name}',
          modeSuppress: 'пригнічено',
          modeBoost: 'підсилено',
          tokensLabel: 'Токени',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Пригнічує всі гендеровані займенники та спостерігає, які значення за замовчуванням обирає модель.',
          sentimentDesc: 'Підсилює позитивні або негативні слова та вимірює, наскільки сильно це впливає на весь текстовий потік.',
          domainDesc: 'Підсилює наукову або поетичну лексику та спостерігає зміни регістру.',
          interpretationTitle: 'Інтерпретація',
          interpreting: 'Аналіз результатів...',
          interpretationError: 'Не вдалося згенерувати інтерпретацію'
        },
        error: {
          gpuUnreachable: 'GPU-сервіс недоступний. Чи він запущений?',
          loadFailed: 'Не вдалося завантажити модель.',
          operationFailed: 'Операція не вдалася.'
        }
      },
      crossmodal: {
        headerTitle: 'Крос-модальна лабораторія',
        headerSubtitle: 'Звук з латентних просторів: маніпуляція вкладеннями T5, генерація аудіо з зображень, крос-модальний перенос',
        explanationToggle: 'Показати детальне пояснення',
        generate: 'Згенерувати',
        generating: 'Генерація...',
        result: 'Результат',
        seed: 'Seed',
        generationTime: 'Час генерації',
        tabs: {
          synth: {
            label: 'Латентний аудіосинтезатор',
            short: 'Маніпуляція вкладеннями T5',
            title: 'Латентний аудіосинтезатор',
            description: 'Пряма маніпуляція простором кондиціонування T5 Stable Audio (768 вимірів). Інтерполяція між промптами, екстраполяція за межі промпту, масштабування вкладень та ін\'єкція шуму.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Зображення/текст у аудіо (CVPR 2025)',
            title: 'MMAudio — Відео/зображення у аудіо',
            description: 'Зображення та текст надходять в одну мережу як окремі сигнали — зображення не перекладається у мову, обидва одночасно спрямовують генерацію звуку. Модель навчалась спільно на відео та аудіо, засвоюючи прямі зв\'язки між видимим та чутним. До 8с, 44,1кГц, ~1,2с обчислень. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'ImageBind Guidance',
            short: 'Градієнтне керування зображенням',
            title: 'ImageBind Gradient Guidance',
            description: 'Градієнтне керування під час процесу шумоочищення Stable Audio. ImageBind надає спільний 1024-вимірний простір для зображення та аудіо — градієнт косинусної подібності спрямовує генерацію аудіо до вкладення зображення.'
          }
        },
        synth: {
          explainWhatTitle: '\u0429\u043e \u0440\u043e\u0431\u0438\u0442\u044c \u041b\u0430\u0442\u0435\u043d\u0442\u043d\u0438\u0439 \u0430\u0443\u0434\u0456\u043e\u0441\u0438\u043d\u0442\u0435\u0437\u0430\u0442\u043e\u0440?',
          explainWhatText: 'Stable Audio \u0433\u0435\u043d\u0435\u0440\u0443\u0454 \u0437\u0432\u0443\u043a \u0456\u0437 \u0442\u0435\u043a\u0441\u0442\u0443. \u0422\u0435\u043a\u0441\u0442 \u043f\u0435\u0440\u0435\u0442\u0432\u043e\u0440\u044e\u0454\u0442\u044c\u0441\u044f \u043a\u043e\u0434\u0443\u0432\u0430\u043b\u044c\u043d\u0438\u043a\u043e\u043c T5 \u0443 \u0447\u0438\u0441\u043b\u043e\u0432\u0438\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0456\u0437 768 \u0432\u0438\u043c\u0456\u0440\u0430\u043c\u0438 \u2014 \u0441\u0430\u043c\u0435 \u0446\u0435\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u0438 \u0442\u0443\u0442 \u043c\u0430\u043d\u0456\u043f\u0443\u043b\u044e\u0454\u0442\u0435. \u0417\u0430\u043c\u0456\u0441\u0442\u044c \u0437\u043c\u0456\u043d\u0438 \u043b\u0438\u0448\u0435 \u0442\u043e\u0433\u043e, \u201c\u0449\u043e\u201d \u043c\u043e\u0434\u0435\u043b\u044c \u0433\u0435\u043d\u0435\u0440\u0443\u0454 (\u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u043c\u043f\u0442), \u0432\u0438 \u0437\u043c\u0456\u043d\u044e\u0454\u0442\u0435 \u201c\u044f\u043a\u201d \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u043d\u0443\u0442\u0440\u0456\u0448\u043d\u044c\u043e \u0440\u043e\u0437\u0443\u043c\u0456\u0454 \u0442\u0435\u043a\u0441\u0442. \u0414\u0432\u0430 \u043f\u0440\u043e\u043c\u043f\u0442\u0438, \u0449\u043e \u0437\u0432\u0443\u0447\u0430\u0442\u044c \u0441\u0445\u043e\u0436\u0435, \u043c\u043e\u0436\u0443\u0442\u044c \u0431\u0443\u0442\u0438 \u0434\u0430\u043b\u0435\u043a\u043e \u043e\u0434\u0438\u043d \u0432\u0456\u0434 \u043e\u0434\u043d\u043e\u0433\u043e \u0432 \u0446\u044c\u043e\u043c\u0443 \u043f\u0440\u043e\u0441\u0442\u043e\u0440\u0456 \u2014 \u0456 \u043d\u0430\u0432\u043f\u0430\u043a\u0438.',
          explainHowTitle: '\u042f\u043a \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0446\u0435\u0439 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442?',
          explainHowText: '\u0412\u0432\u0435\u0434\u0456\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0443 \u041f\u0440\u043e\u043c\u043f\u0442 A \u2014 \u0446\u0435 \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454 \u0431\u0430\u0437\u043e\u0432\u0438\u0439 \u0437\u0432\u0443\u043a. \u041d\u0435\u043e\u0431\u043e\u0432\u2019\u044f\u0437\u043a\u043e\u0432\u043e: \u041f\u0440\u043e\u043c\u043f\u0442 B \u044f\u043a \u0446\u0456\u043b\u044c\u043e\u0432\u0430 \u0442\u043e\u0447\u043a\u0430. \u041f\u043e\u0432\u0437\u0443\u043d\u043e\u043a \u0410\u043b\u044c\u0444\u0430 \u043a\u0435\u0440\u0443\u0454 \u043c\u0456\u043a\u0441\u043e\u043c: \u043f\u0440\u0438 0 \u0432\u0438 \u0447\u0443\u0454\u0442\u0435 \u043b\u0438\u0448\u0435 A, \u043f\u0440\u0438 1 \u043b\u0438\u0448\u0435 B, \u043f\u0440\u0438 0.5 \u0441\u0443\u043c\u0456\u0448. \u0417\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u0432\u0438\u0449\u0435 1 \u0435\u043a\u0441\u0442\u0440\u0430\u043f\u043e\u043b\u044e\u044e\u0442\u044c \u0437\u0430 B (\u0437\u0432\u0443\u043a \u0441\u0442\u0430\u0454 \u0435\u043a\u0441\u0442\u0440\u0435\u043c\u0430\u043b\u044c\u043d\u0456\u0448\u0438\u043c), \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u043d\u0438\u0436\u0447\u0435 0 \u0439\u0434\u0443\u0442\u044c \u0443 \u043f\u0440\u043e\u0442\u0438\u043b\u0435\u0436\u043d\u043e\u043c\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0443. Magnitude \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0443\u0454 \u0432\u0441\u0435 \u0432\u043a\u043b\u0430\u0434\u0435\u043d\u043d\u044f \u2014 \u0432\u0438\u0449\u0456 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u0434\u0430\u044e\u0442\u044c \u0456\u043d\u0442\u0435\u043d\u0441\u0438\u0432\u043d\u0456\u0448\u0456 \u0437\u0432\u0443\u043a\u0438. Noise \u0434\u043e\u0434\u0430\u0454 \u0432\u0438\u043f\u0430\u0434\u043a\u043e\u0432\u0456\u0441\u0442\u044c \u0456 \u0441\u0442\u0432\u043e\u0440\u044e\u0454 \u043d\u0435\u043f\u0435\u0440\u0435\u0434\u0431\u0430\u0447\u0443\u0432\u0430\u043d\u0456 \u0432\u0430\u0440\u0456\u0430\u0446\u0456\u0457. Spectral Strip (\u043f\u0456\u0434 \u043a\u043d\u043e\u043f\u043a\u043e\u044e Generate) \u043f\u043e\u043a\u0430\u0437\u0443\u0454 \u0432\u0441\u0456 768 \u0432\u0438\u043c\u0456\u0440\u0456\u0432 \u044f\u043a \u0441\u0442\u043e\u0432\u043f\u0447\u0438\u043a\u0438. \u0412\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0441\u0443\u0432\u0430\u0442\u0438 \u043e\u043a\u0440\u0435\u043c\u0456 \u0432\u0438\u043c\u0456\u0440\u0438 \u043f\u0435\u0440\u0435\u0442\u044f\u0433\u0443\u0432\u0430\u043d\u043d\u044f\u043c, \u0431\u0435\u0437\u043f\u043e\u0441\u0435\u0440\u0435\u0434\u043d\u044c\u043e \u043c\u0430\u043d\u0456\u043f\u0443\u043b\u044e\u044e\u0447\u0438 \u0437\u0432\u0443\u043a\u043e\u043c. \u041f\u0440\u0430\u0432\u0438\u0439 \u043a\u043b\u0456\u043a \u0441\u043a\u0438\u0434\u0430\u0454 \u0432\u0438\u043c\u0456\u0440.',
          promptA: 'Промпт A (База)',
          promptAPlaceholder: 'напр. океанські хвилі',
          promptB: 'Промпт B (Необов\'язково, для інтерполяції)',
          promptBPlaceholder: 'напр. мелодія фортепіано',
          alpha: 'Альфа (Інтерполяція)',
          alphaHint: '0 = лише A, 1 = лише B, між = суміш, >1 або <0 = екстраполяція',
          magnitude: 'Магнітуда (Масштабування)',
          magnitudeHint: 'Глобальне масштабування вкладення (1.0 = без змін)',
          noise: 'Шум',
          noiseHint: 'Гауссів шум на вкладенні (0 = без шуму)',
          duration: 'Тривалість (с)',
          steps: 'Кроки',
          cfg: 'CFG',
          durationHint: 'Тривалість згенерованого аудіокліпу в секундах',
          stepsHint: 'Кроки видалення шуму. Більше = вища якість.',
          cfgHint: 'Classifier-Free Guidance для генерації аудіо',
          seedHint: '-1 = випадково, фіксоване значення = відтворюваний результат',
          loop: 'Циклічне відтворення',
          loopOn: 'Цикл увімк.',
          loopOff: 'Цикл вимк.',
          stop: 'Стоп',
          looping: 'Циклічне',
          playing: 'Відтворення',
          stopped: 'Зупинено',
          transpose: 'Транспонування (півтони)',
          midiSection: 'Управління MIDI',
          midiUnsupported: 'Web MIDI не підтримується цим браузером.',
          midiInput: 'MIDI-вхід',
          midiNone: '(немає)',
          midiMappings: 'CC-маппінги',
          midiNoteC3: 'Нота (C3 = еталон)',
          midiGenerate: 'Згенерувати + Транспонувати',
          midiPitch: 'Висота відн. C3',
          loopInterval: 'Інтервал циклу',
          loopOptimize: 'Автооптимізація',
          loopPingPong: 'Пінг-понг',
          loopIntervalHint: 'Початок/кінець області циклу — скоротіть кінець, щоб обрізати затухання Stable Audio',
          modeLoop: 'Цикл',
          modePingPong: 'Пінг-Понг',
          modeWavetable: 'Хвильова таблиця',
          modeRate: 'Темп (швидко)',
          modePitch: 'Висота (OLA)',
          wavetableScan: 'Позиція сканування',
          wavetableScanHint: 'Морфінг між фреймами (0 = початок, 1 = кінець)',
          wavetableFrames: '{count} фреймів',
          midiScan: 'Позиція сканування',
          adsrTitle: 'Огинаюча ADSR',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Огинаюча для MIDI-нот (Attack/Decay/Sustain/Release)',
          play: 'Відтворити',
          normalize: 'Нормалізувати гучність',
          peak: 'Пік',
          crossfade: 'Крос-фейд',
          transposeHint: 'Зсуває висоту тону на півтони',
          crossfadeHint: 'Час крос-фейду на межі циклу (мс)',
          normalizeHint: 'Нормалізує гучність до максимальної амплітуди',
          saveRaw: 'Зберегти raw',
          saveLoop: 'Зберегти цикл',
          embeddingStats: 'Статистика вкладень',
          dimensions: {
            section: 'Дослідник вимірів',
            hint: 'Перетягніть стовпчики = встановити зсув. Горизонтальний мазок = кілька вимірів.',
            resetAll: 'Скинути все',
            hoverActivation: 'Активація',
            hoverOffset: 'Зсув',
            rightClickReset: 'ПКМ = скинути',
            sortDiff: 'Сортовано за різницею промптів',
            sortMagnitude: 'Сортовано за активацією',
            activeOffsets: '{count} зсувів активних',
            applyAndGenerate: 'Застосувати та перегенерувати',
            undo: 'Скасувати',
            redo: 'Повторити'
          }
        },
        mmaudio: {
          explainWhatTitle: '\u0429\u043e \u0440\u043e\u0431\u0438\u0442\u044c MMAudio?',
          explainWhatText: 'MMAudio (Cheng \u0442\u0430 \u0456\u043d., CVPR 2025) \u043d\u0430\u0432\u0447\u0430\u0432\u0441\u044f \u0441\u043f\u0456\u043b\u044c\u043d\u043e \u043d\u0430 \u0432\u0456\u0434\u0435\u043e \u0442\u0430 \u0430\u0443\u0434\u0456\u043e. \u0412\u0456\u043d \u043d\u0435 \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0430\u0454 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0432 \u0442\u0435\u043a\u0441\u0442, \u0430 \u043f\u043e\u0442\u0456\u043c \u0443 \u0437\u0432\u0443\u043a \u2014 \u0430 \u043e\u0431\u0440\u043e\u0431\u043b\u044f\u0454 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0442\u0430 \u0442\u0435\u043a\u0441\u0442 \u044f\u043a \u043f\u0430\u0440\u0430\u043b\u0435\u043b\u044c\u043d\u0456 \u0441\u0438\u0433\u043d\u0430\u043b\u0438 \u0432 \u043e\u0434\u043d\u0456\u0439 \u043c\u0435\u0440\u0435\u0436\u0456. \u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0430\u0432\u0447\u0438\u043b\u0430\u0441\u044f, \u044f\u043a\u0456 \u0437\u0432\u0443\u043a\u0438 \u043d\u0430\u043b\u0435\u0436\u0430\u0442\u044c \u044f\u043a\u0438\u043c \u0432\u0456\u0437\u0443\u0430\u043b\u044c\u043d\u0438\u043c \u0441\u0446\u0435\u043d\u0430\u043c \u2014 \u043b\u0456\u0441 \u0441\u0442\u0432\u043e\u0440\u044e\u0454 \u043f\u0442\u0430\u0448\u0438\u043d\u0438\u0439 \u0441\u043f\u0456\u0432, \u0432\u0443\u043b\u0438\u0446\u044f \u2014 \u0448\u0443\u043c \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\u0443, \u0433\u0456\u0442\u0430\u0440\u0430 \u2014 \u0437\u0432\u0443\u043a\u0438 \u0449\u0438\u043f\u043a\u0456\u0432.',
          explainHowTitle: '\u042f\u043a \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0446\u0435\u0439 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442?',
          explainHowText: '\u0417\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0442\u0435 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0442\u0430/\u0430\u0431\u043e \u0432\u0432\u0435\u0434\u0456\u0442\u044c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0439 \u043f\u0440\u043e\u043c\u043f\u0442 \u2014 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u043e\u0431\u043e\u0445 \u0440\u0430\u0437\u043e\u043c \u0434\u0430\u0454 \u043d\u0430\u0439\u0431\u0430\u0433\u0430\u0442\u0448\u0456 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0438. \u0417\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0441\u0430\u043c\u043e \u043f\u043e \u0441\u043e\u0431\u0456 \u0433\u0435\u043d\u0435\u0440\u0443\u0454 \u0437\u0432\u0443\u043a\u0438, \u0449\u043e \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0430\u044e\u0442\u044c \u0432\u0456\u0437\u0443\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0432\u043c\u0456\u0441\u0442\u0443. \u0422\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0439 \u043f\u0440\u043e\u043c\u043f\u0442 \u043c\u043e\u0436\u0435 \u0434\u043e\u0434\u0430\u0442\u043a\u043e\u0432\u043e \u0441\u043f\u0440\u044f\u043c\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0437\u0432\u0443\u043a \u0430\u0431\u043e \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438\u0441\u044c \u043e\u043a\u0440\u0435\u043c\u043e \u0431\u0435\u0437 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f. \u0423 \u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e\u043c\u0443 \u043f\u0440\u043e\u043c\u043f\u0442\u0456 \u043e\u043f\u0438\u0448\u0456\u0442\u044c \u0437\u0432\u0443\u043a\u0438, \u044f\u043a\u0438\u0445 \u041d\u0415 \u0445\u043e\u0447\u0435\u0442\u0435 \u0447\u0443\u0442\u0438 (\u043d\u0430\u043f\u0440. \u201c\u043c\u043e\u0432\u0430, \u043c\u0443\u0437\u0438\u043a\u0430\u201d). Duration \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454 \u0442\u0440\u0438\u0432\u0430\u043b\u0456\u0441\u0442\u044c (1-8 \u0441\u0435\u043a\u0443\u043d\u0434). CFG Strength \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044e\u0454, \u043d\u0430\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0441\u0443\u0432\u043e\u0440\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043e\u0442\u0440\u0438\u043c\u0443\u0454\u0442\u044c\u0441\u044f \u043f\u0440\u043e\u043c\u043f\u0442\u0443 \u2014 \u043d\u0438\u0437\u044c\u043a\u0456 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f (2-3) \u0434\u0430\u044e\u0442\u044c \u0440\u0456\u0437\u043d\u043e\u043c\u0430\u043d\u0456\u0442\u043d\u0456\u0448\u0456, \u0432\u0438\u0441\u043e\u043a\u0456 (6-8) \u2014 \u0431\u0456\u043b\u044c\u0448 \u043f\u0440\u043e\u043c\u043f\u0442-\u0432\u0456\u0440\u043d\u0456 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0438.',
          imageUpload: 'Завантажити зображення (необов\'язково)',
          prompt: 'Текстовий промпт (необов\'язково)',
          promptPlaceholder: 'напр. потріскування вогнища',
          negativePrompt: 'Негативний промпт',
          duration: 'Тривалість (с)',
          maxDuration: 'Макс. 8с (обмеження моделі)',
          cfg: 'CFG',
          steps: 'Кроки',
          compareHint: 'Порівняти: лише текст vs. зображення + текст'
        },
        guidance: {
          explainWhatTitle: '\u0429\u043e \u0440\u043e\u0431\u0438\u0442\u044c ImageBind Guidance?',
          explainWhatText: 'ImageBind (Girdhar \u0442\u0430 \u0456\u043d., CVPR 2023) \u043e\u0431\u2019\u0454\u0434\u043d\u0443\u0454 \u0448\u0456\u0441\u0442\u044c \u0432\u0456\u0434\u0447\u0443\u0442\u0442\u0456\u0432 \u2014 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f, \u0437\u0432\u0443\u043a, \u0442\u0435\u043a\u0441\u0442, \u0433\u043b\u0438\u0431\u0438\u043d\u0443, \u0442\u0435\u043f\u043b\u043e, \u0440\u0443\u0445 \u2014 \u0443 \u0441\u043f\u0456\u043b\u044c\u043d\u0443 \u201c\u043c\u043e\u0432\u0443\u201d. \u0426\u0435\u0439 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454 \u0446\u044e \u0441\u043f\u0456\u043b\u044c\u043d\u0456\u0441\u0442\u044c: \u043f\u043e\u043a\u0438 \u0437\u0432\u0443\u043a \u0441\u0442\u0432\u043e\u0440\u044e\u0454\u0442\u044c\u0441\u044f \u043a\u0440\u043e\u043a \u0437\u0430 \u043a\u0440\u043e\u043a\u043e\u043c, \u0432\u0456\u043d \u043f\u043e\u0441\u0442\u0456\u0439\u043d\u043e \u0437\u0430\u043f\u0438\u0442\u0443\u0454 \u201c\u0427\u0438 \u0437\u0432\u0443\u0447\u0438\u0442\u044c \u0446\u0435 \u0432\u0436\u0435 \u044f\u043a \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f?\u201d \u0456 \u043a\u043e\u0440\u0438\u0433\u0443\u0454 \u043d\u0430\u043f\u0440\u044f\u043c\u043e\u043a. \u041a\u043e\u0441\u0438\u043d\u0443\u0441\u043d\u0430 \u043f\u043e\u0434\u0456\u0431\u043d\u0456\u0441\u0442\u044c \u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0456 \u043f\u043e\u043a\u0430\u0437\u0443\u0454, \u043d\u0430\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0431\u043b\u0438\u0437\u044c\u043a\u043e \u0437\u0433\u0435\u043d\u0435\u0440\u043e\u0432\u0430\u043d\u0438\u0439 \u0437\u0432\u0443\u043a \u043f\u0456\u0434\u0456\u0439\u0448\u043e\u0432 \u0434\u043e \u0432\u043c\u0456\u0441\u0442\u0443 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f.',
          explainHowTitle: '\u042f\u043a \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0446\u0435\u0439 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442?',
          explainHowText: '\u0417\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0442\u0435 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u2014 \u0446\u0435 \u0446\u0456\u043b\u044c\u043e\u0432\u0438\u0439 \u043d\u0430\u043f\u0440\u044f\u043c\u043e\u043a \u0434\u043b\u044f \u0437\u0432\u0443\u043a\u0443. \u041d\u0435\u043e\u0431\u043e\u0432\u2019\u044f\u0437\u043a\u043e\u0432\u043e: \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0439 \u043f\u0440\u043e\u043c\u043f\u0442 \u0434\u043b\u044f \u0434\u043e\u0434\u0430\u0442\u043a\u043e\u0432\u043e\u0433\u043e \u0441\u043f\u0440\u044f\u043c\u0443\u0432\u0430\u043d\u043d\u044f. \u041f\u043e\u0432\u0437\u0443\u043d\u043e\u043a \u201c\u03bb Guidance Strength\u201d \u2014 \u043d\u0430\u0439\u0432\u0430\u0436\u043b\u0438\u0432\u0456\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440: \u043d\u0438\u0437\u044c\u043a\u0456 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f (0.01-0.05) \u0434\u0430\u044e\u0442\u044c \u0437\u0432\u0443\u043a\u0443 \u0431\u0430\u0433\u0430\u0442\u043e \u0441\u0432\u043e\u0431\u043e\u0434\u0438, \u0432\u0438\u0441\u043e\u043a\u0456 (0.3-1.0) \u0442\u0456\u0441\u043d\u043e \u043f\u0440\u0438\u0432\u2019\u044f\u0437\u0443\u044e\u0442\u044c \u0439\u043e\u0433\u043e \u0434\u043e \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f. \u201cWarmup Steps\u201d \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454, \u0437 \u044f\u043a\u043e\u0433\u043e \u043a\u0440\u043e\u043a\u0443 \u043f\u043e\u0447\u0438\u043d\u0430\u0454\u0442\u044c\u0441\u044f \u043a\u0435\u0440\u0443\u0432\u0430\u043d\u043d\u044f \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f\u043c \u2014 \u043d\u0438\u0437\u044c\u043a\u0456 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u043f\u043e\u0447\u0438\u043d\u0430\u044e\u0442\u044c \u043e\u0434\u0440\u0430\u0437\u0443, \u0432\u0438\u0441\u043e\u043a\u0456 \u0434\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442\u044c \u0431\u0430\u0437\u043e\u0432\u0456\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0456 \u0441\u043f\u0435\u0440\u0448\u0443 \u0441\u0444\u043e\u0440\u043c\u0443\u0432\u0430\u0442\u0438\u0441\u044f \u0431\u0435\u0437 \u043a\u0435\u0440\u0443\u0432\u0430\u043d\u043d\u044f. Total Steps \u0456 Duration \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044e\u044e\u0442\u044c \u044f\u043a\u0456\u0441\u0442\u044c \u0456 \u0442\u0440\u0438\u0432\u0430\u043b\u0456\u0441\u0442\u044c.',
          referencesTitle: 'Наукові джерела',
          imageUpload: 'Завантажити зображення',
          prompt: 'Базовий промпт (необов\'язково)',
          promptPlaceholder: 'напр. ambient soundscape',
          lambda: 'Сила керування',
          lambdaHint: 'Наскільки сильно зображення керує генерацією аудіо',
          warmupSteps: 'Кроки прогріву',
          warmupHint: 'Градієнтне керування лише протягом перших N кроків',
          totalSteps: 'Загальні кроки',
          duration: 'Тривалість (с)',
          cfg: 'CFG',
          totalStepsHint: 'Загальні кроки видалення шуму. Більше = вища якість.',
          durationHint: 'Тривалість згенерованого аудіокліпу в секундах',
          cosineSimilarity: 'Косинусна подібність (близькість зображення-аудіо)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: 'Чи знаєш ти?',
        learnMore: 'Дізнатися більше',
        currentlyHappening: 'Зараз відбувається:',
        energyUsed: 'Використано енергії',
        co2Produced: 'Вироблено CO\u2082'
      },
      energy: {
        kids_1: 'ШІ-зображення потребують електрики — стільки ж, як зарядка телефону протягом 3 годин!',
        kids_2: 'GPU — це як суперкалькулятор, який споживає багато енергії!',
        kids_3: 'Кожне зображення потребує стільки енергії, як LED-лампочка протягом 10 хвилин!',
        youth_1: 'GPU споживає {watts} Вт під час генерації — як невеликий обігрівач!',
        youth_2: 'Одне зображення використовує приблизно 0.01-0.02 кВт·год — звучить мало, але накопичується!',
        youth_3: 'GPU зараз нагрівається до {temp}°C — тому йому потрібне охолодження!',
        expert_1: 'Реальний час: {watts} Вт при {util}% завантаженні = {kwh} кВт·год досі',
        expert_2: 'Ліміт TDP: {tdp} Вт | Поточне: {watts} Вт ({percent}% від ліміту)',
        expert_3: 'VRAM: {used}/{total} ГБ ({percent}%) — модель + активації'
      },
      data: {
        kids_1: 'GPU зараз обчислює 10 мільярдів разів — швидше, ніж ти зможеш порахувати!',
        kids_2: 'Зображення створюється за 50 маленьких кроків — як пазл, що складається сам!',
        kids_3: 'Мільйони чисел зараз летять через GPU!',
        youth_1: 'Кожне зображення проходить ~50 «кроків шумоочищення» — 50 раундів видалення шуму!',
        youth_2: '8 мільярдів параметрів опитуються — на кожне зображення!',
        youth_3: 'ШІ «думає» у векторах з тисячами вимірів — як координати у просторі.',
        expert_1: 'MMDiT: Мультимодальний дифузійний трансформер — текст + зображення у спільних шарах уваги',
        expert_2: 'Self-Attention: O(n\u00b2) складність — кожен токен «бачить» усі інші',
        expert_3: 'Classifier-Free Guidance: баланс впливу промпту vs. креативності'
      },
      model: {
        kids_1: 'ШІ-модель переглянула мільйони зображень, щоб навчитися малювати!',
        kids_2: 'ШІ — як художник, який ніколи не забуває побачене!',
        kids_3: '8 мільярдів з\'єднань у моделі — більше, ніж зірок, які можна побачити на небі!',
        youth_1: 'SD3.5 Large має 8 мільярдів параметрів — як 8 мільярдів вузлів прийняття рішень.',
        youth_2: '3 текстових кодувальники працюють разом: CLIP-L, CLIP-G та T5-XXL',
        youth_3: 'Модель потребує {vram} ГБ VRAM лише для завантаження!',
        expert_1: 'Архітектура: Rectified Flow + MMDiT з 38 блоками трансформера',
        expert_2: 'FP16/FP8 квантизація: компроміс між точністю та VRAM',
        expert_3: 'LoRA: Low-Rank Adaptation — перенавчається лише 0.1% параметрів'
      },
      ethics: {
        kids_1: 'ШІ вчиться з зображень в інтернеті — тому важливо бути справедливим до чужого мистецтва!',
        kids_2: 'Не всіх художників запитали, чи може ШІ вчитися на їхніх роботах.',
        kids_3: 'Хороший ШІ поважає роботу людей!',
        youth_1: 'Навчальні дані часто беруться з інтернету. Художники дискутують: Fair Use чи копіювання?',
        youth_2: 'EU AI Act вимагає прозорості: Звідки беруться навчальні дані?',
        youth_3: 'Питання: Кому насправді належить ШІ-згенероване зображення?',
        expert_1: 'LAION-5B був частково створений без згоди авторів — правова сіра зона.',
        expert_2: 'EU AI Act Ст. 52: Вимога маркування ШІ-згенерованого контенту',
        expert_3: 'Model Cards & Datasheets: Найкращі практики прозорості ML'
      },
      environment: {
        kids_1: 'Кожне ШІ-зображення виробляє трохи CO\u2082 — як поїздка на автомобілі, але менше!',
        kids_2: 'Подумай: Чи варте це зображення електрики?',
        kids_3: 'Енергія для ШІ часто надходить з електростанцій — деякі чисті, деякі ні.',
        youth_1: 'Німецька електромережа: ~400 г CO\u2082 на кВт·год — це накопичується!',
        youth_2: '{co2} г CO\u2082 за це зображення — при 1000 зображеннях це було б {totalKg} кг!',
        youth_3: 'Порада: Генеруй менше зображень, але обдуманіше — економить енергію та CO\u2082.',
        expert_1: 'Розрахунок: {watts} Вт \u00d7 {seconds} с \u00f7 3600 \u00d7 400 г/кВт·год = {co2} г CO\u2082',
        expert_2: 'Scope 2 емісії: місцезнаходження дата-центру є вирішальним',
        expert_3: 'PUE (Power Usage Effectiveness): Додаткові витрати енергії на охолодження'
      },
      iceberg: {
        drawPrompt: 'ШІ-генерація використовує багато енергії. Намалюй айсберги і подивись, що станеться...',
        redraw: 'Перемалювати',
        startMelting: 'Почати танення',
        melting: 'Айсберг тане...',
        melted: 'Розтанув!',
        meltedMessage: '{co2} г CO\u2082 вироблено',
        comparison: 'Ця кількість CO\u2082 розтоплює приблизно {volume} см\u00b3 арктичного льоду.',
        comparisonInfo: '(Кожна тонна CO\u2082 = приблизно 6 м\u00b3 втрати морського льоду)',
        gpuPower: 'Споживання відеокарти',
        gpuTemp: 'Температура відеокарти',
        co2Info: 'Емісії CO\u2082 від споживання електроенергії (на основі німецького енергомікса)',
        drawAgain: 'Намалюй ще айсбергів...'
      },
      pixel: {
        grafikkarte: 'Відеокарта',
        energieverbrauch: 'Споживання енергії',
        co2Menge: 'Кількість CO\u2082',
        smartphoneComparison: 'Тобі довелося б тримати телефон вимкненим {minutes} хвилин, щоб компенсувати це CO\u2082!',
        clickToProcess: 'Натисни на піксель даних, щоб згенерувати мінізображення!'
      },
      forest: {
        trees: 'Дерева',
        clickToPlant: 'Натисни, щоб посадити дерева! Де ти посадиш дерево, там зникне фабрика.',
        gameOver: 'Ліс загублений!',
        treesPlanted: 'Ти посадив(ла) {count} дерев.',
        complete: 'Генерація завершена',
        comparison: 'Середньому дереву потрібно {minutes} хвилин, щоб поглинути цю кількість CO\u2082.'
      },
      rareearth: {
        clickToClean: 'Натисни на озеро, щоб видалити токсичний шлам!',
        sludgeRemoved: 'Шлам видалено',
        environmentHealth: 'Довкілля',
        gameOverInactive: 'Ти здався(лась)... видобуток продовжується',
        infoBanner: 'Видобуток рідкісних земель для чіпів GPU залишає токсичний шлам та руйнує екосистеми. Твої зусилля з очищення не можуть встигнути за швидкістю видобутку.',
        instructionsCooldown: '\u23f3 {seconds} с',
        statsGpu: 'GPU',
        statsHealth: 'Довкілля',
        statsSludge: 'Шлам видалено'
      }
    }
  },
  fr: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Transformations créatives par IA'
    },
    form: {
      inputLabel: 'Votre texte',
      inputPlaceholder: 'p. ex. Une fleur dans la prairie',
      schemaLabel: 'Style de transformation',
      executeModeLabel: 'Mode d\'exécution',
      safetyLabel: 'Niveau de sécurité',
      generateButton: 'Générer'
    },
    schemas: {
      dada: 'Dada (Aléatoire & Absurde)',
      bauhaus: 'Bauhaus (Géométrique)',
      stillepost: 'Téléphone arabe (Itératif)'
    },
    executionModes: {
      eco: 'Éco (Rapide)',
      fast: 'Fast (Équilibré)',
      best: 'Best (Qualité)'
    },
    safetyLevels: {
      kids: 'Enfants',
      youth: 'Jeunes',
      adult: 'Adultes',
      research: 'Recherche'
    },
    stages: {
      pipeline_starting: 'Démarrage du pipeline',
      translation_and_safety: 'Traduction & Sécurité',
      interception: 'Transformation',
      pre_output_safety: 'Sécurité de sortie',
      media_generation: 'Génération d\'image',
      completed: 'Terminé'
    },
    status: {
      idle: 'Prêt',
      executing: 'Pipeline en cours...',
      connectionSlow: 'Connexion lente, nouvelle tentative...',
      completed: 'Pipeline terminé !',
      error: 'Erreur survenue'
    },
    entities: {
      input: 'Entrée',
      translation: 'Traduction',
      safety: 'Vérification de sécurité',
      interception: 'Transformation',
      safety_pre_output: 'Sécurité de sortie',
      media: 'Image générée'
    },
    properties: {
      chill: 'tranquille',
      chaotic: 'sauvage',
      narrative: 'raconter des histoires',
      algorithmic: 'suivre des règles',
      historical: 'histoire',
      contemporary: 'présent',
      explore: 'tester l\'IA',
      create: 'créer de l\'art',
      playful: 'ludique',
      serious: 'sérieux'
    },
    phase2: {
      title: 'Saisie du prompt',
      userInput: 'Votre saisie',
      yourInput: 'Votre saisie',
      yourIdea: 'Votre idée : DE QUOI cela devrait-il parler ?',
      rules: 'Vos règles : COMMENT votre idée doit-elle être réalisée ?',
      yourInstructions: 'Vos instructions',
      what: 'QUOI',
      how: 'COMMENT',
      userInputPlaceholder: 'p. ex. Une fleur dans la prairie',
      inputPlaceholder: 'Votre texte apparaît ici...',
      metaPrompt: 'Instruction artistique',
      instruction: 'Instruction',
      transformation: 'Transformation artistique',
      metaPromptPlaceholder: 'Décrivez la transformation...',
      result: 'Résultat',
      expectedResult: 'Résultat attendu',
      execute: 'Exécuter le pipeline',
      executing: 'En cours...',
      transforming: 'Transformation LLM en cours...',
      startTransformation: 'Lancer la transformation',
      letsGo: 'C\'est parti !',
      modified: 'Modifié',
      reset: 'Réinitialiser',
      loadingConfig: 'Chargement de la configuration...',
      loadingMetaPrompt: 'Chargement du méta-prompt...',
      errorLoadingConfig: 'Erreur lors du chargement de la configuration',
      errorLoadingMetaPrompt: 'Erreur lors du chargement du méta-prompt',
      threeForces: '3 forces en action',
      twoForces: 'QUOI + COMMENT → LLM → Résultat',
      yourPrompt: 'Votre prompt :',
      writeYourText: 'Écrivez votre texte...',
      examples: 'Exemples',
      estimatedTime: '~12 secondes',
      stage12Time: '~5-10 secondes',
      willAppearAfterExecution: 'Apparaîtra après l\'exécution...',
      back: 'Retour',
      retry: 'Réessayer',
      transformedPrompt: 'Prompt transformé',
      notYetTransformed: 'Pas encore transformé...',
      transform: 'Transformer',
      reTransform: 'Réessayer autrement',
      startAI: 'IA, traite ma saisie',
      aiWorking: 'L\'IA travaille...',
      continueToMedia: 'Continuer vers la génération d\'image',
      readyForMedia: 'Prêt pour la génération d\'image',
      stage1: 'Étape 1 : Traduction + Sécurité...',
      stage2: 'Étape 2 : Transformation...',
      selectMedia: 'Choisissez votre média :',
      mediaImage: 'Image',
      mediaAudio: 'Audio',
      mediaVideo: 'Vidéo',
      media3D: '3D',
      comingSoon: 'Bientôt disponible',
      generateMedia: 'Démarrer !'
    },
    phase3: {
      generating: 'L\'image est en cours de génération...',
      generatingHint: '~30 secondes'
    },
    common: {
      back: 'Retour',
      loading: 'Chargement...',
      error: 'Erreur',
      retry: 'Réessayer',
      cancel: 'Annuler',
      checkingSafety: 'Vérification...'
    },
    gallery: {
      title: 'Favoris',
      empty: 'Pas encore de favoris',
      favorite: 'Ajouter aux favoris',
      unfavorite: 'Retirer des favoris',
      continue: 'Continuer l\'édition',
      restore: 'Restaurer la session',
      viewMine: 'Mes favoris',
      viewAll: 'Tous les favoris'
    },
    settings: {
      authRequired: 'Authentification requise',
      authPrompt: 'Veuillez entrer le mot de passe pour accéder aux paramètres :',
      passwordPlaceholder: 'Entrer le mot de passe...',
      authenticate: 'Se connecter',
      authenticating: 'Authentification...',
      title: 'Administration',
      tabs: {
        export: 'Données de recherche',
        config: 'Configuration',
        demos: 'Démo minijeu',
        matrix: 'Matrice de modèles'
      },
      loading: 'Chargement des paramètres...',
      presets: {
        title: 'Préréglages de modèles',
        help: 'Utilisez l\'onglet <strong>Matrice de modèles</strong> pour voir tous les préréglages disponibles et les appliquer en un clic.',
        openMatrix: 'Ouvrir la matrice de modèles'
      },
      testingTools: {
        title: 'Outils de test pour les enseignants',
        help: 'Testez et explorez les mini-jeux et animations pédagogiques avant de les utiliser avec les apprenants.',
        openPreview: 'Ouvrir l\'aperçu des mini-jeux',
        pixelEditor: 'Éditeur de modèles pixel',
        includes: 'Inclut : Animation pixel, Fonte des icebergs, Jeu de la forêt, Terres rares'
      },
      general: {
        title: 'Configuration générale',
        uiMode: 'Mode d\'interface',
        uiModeHelp: 'Niveau de complexité de l\'interface',
        kids: 'Enfants (8–12)',
        youth: 'Jeunes (13–17)',
        expert: 'Expert',
        safetyLevel: 'Niveau de sécurité',
        defaultLanguage: 'Langue par défaut',
        germanDe: 'Allemand (de)',
        englishEn: 'Anglais (en)',
        turkishTr: 'Turc (tr)',
        koreanKo: '한국어 (ko)',
        ukrainianUk: 'Українська (uk)',
        frenchFr: 'Français (fr)'
      },
      safety: {
        kidsTitle: 'Enfants (8–12)',
        kidsDesc: 'Tous les filtres actifs : §86a, RGPD, Protection de la jeunesse (paramètres adaptés à l\'âge), Vérification VLM des images',
        youthTitle: 'Jeunes (13–17)',
        youthDesc: 'Tous les filtres actifs : §86a, RGPD, Protection de la jeunesse (paramètres jeunesse), Vérification VLM des images',
        adultTitle: 'Adultes',
        adultDesc: '§86a + RGPD actifs. Pas de protection de la jeunesse, pas de vérification VLM des images.',
        researchTitle: 'Mode recherche',
        researchDesc: 'AUCUN filtre de sécurité actif. Uniquement autorisé pour les institutions de recherche dans le cadre de projets de recherche scientifique.'
      },
      safetyModels: {
        title: 'Modèles de sécurité locaux',
        help: 'Locaux via Ollama — les noms de personnes et vérifications de sécurité ne quittent jamais le système',
        safetyModel: 'Modèle de sécurité',
        safetyModelHelp: 'Modèle garde pour la sécurité du contenu (§86a, protection de la jeunesse)',
        dsgvoModel: 'Modèle de vérification RGPD',
        dsgvoModelHelp: 'Modèle généraliste pour la vérification NER RGPD (pas un modèle garde)',
        vlmModel: 'Modèle VLM de sécurité',
        vlmModelHelp: 'Modèle de vision pour la vérification de sécurité des images après génération (enfants/jeunes)',
        fast: 'rapide, minimal',
        recommended: 'recommandé'
      },
      dsgvo: {
        title: 'Avertissement RGPD',
        notCompliant: 'Les modèles suivants ne sont <strong>PAS conformes au RGPD</strong> (données traitées hors UE) :',
        compliantHint: 'Options conformes au RGPD :'
      },
      models: {
        title: 'Configuration des modèles',
        help: 'Identifiants de modèles avec préfixe de fournisseur : local/, mistral/, anthropic/, openai/, openrouter/',
        matrixAdvised: 'L\'utilisation de la matrice de modèles est recommandée. Vous pouvez toutefois configurer vos paramètres librement ici.',
        ollamaAvailable: '{count} modèles Ollama disponibles (saisissez ou sélectionnez dans la liste)',
        stage1Text: 'Étape 1 - Modèle texte',
        stage1Vision: 'Étape 1 - Modèle vision',
        stage2Interception: 'Étape 2 - Modèle d\'interception',
        stage2Optimization: 'Étape 2 - Modèle d\'optimisation',
        stage3: 'Étape 3 - Modèle traduction/sécurité',
        stage4Legacy: 'Étape 4 - Modèle hérité',
        chatHelper: 'Modèle assistant de chat',
        imageAnalysis: 'Modèle d\'analyse d\'image',
        coding: 'Génération de code (Tone.js, p5.js)'
      },
      api: {
        title: 'Configuration API',
        llmProvider: 'Fournisseur LLM',
        localFramework: 'Framework LLM local',
        externalProvider: 'Fournisseur LLM externe',
        cloudProvider: 'Fournisseur LLM cloud — nécessite une clé API',
        noneLocal: 'Aucun (local uniquement, RGPD)',
        mistralEu: 'Mistral AI (basé en UE, RGPD)',
        anthropicDirect: 'API Anthropic directe (NON conforme RGPD)',
        openaiDirect: 'API OpenAI directe (NON conforme RGPD)',
        openrouterDirect: 'OpenRouter (NON conforme RGPD, routage UE disponible)',
        mistralInfo: 'Mistral AI (basé en UE)',
        mistralDsgvo: 'Conforme au RGPD (infrastructure UE)',
        anthropicInfo: 'API Anthropic directe',
        anthropicNotDsgvo: 'NON conforme au RGPD',
        anthropicWarning: 'Données traitées hors UE. À utiliser uniquement dans des contextes non éducatifs.',
        openaiInfo: 'API OpenAI directe',
        openaiNotDsgvo: 'NON conforme au RGPD (basé aux États-Unis)',
        openaiWarning: 'Données traitées aux États-Unis. À utiliser uniquement dans des contextes non éducatifs.',
        openrouterInfo: 'OpenRouter',
        openrouterNotDsgvo: 'NON conforme au RGPD (entreprise américaine)',
        openrouterWarning: 'Routage serveur UE configurable dans les paramètres OpenRouter, mais l\'entreprise est basée aux États-Unis.',
        storedIn: 'Stocké dans',
        currentKey: 'Actuelle'
      },
      save: {
        saveApply: 'Enregistrer et appliquer',
        saving: 'Enregistrement...',
        applying: 'Application...',
        success: 'Paramètres enregistrés et appliqués',
        presetApplied: 'Préréglage appliqué : {preset}'
      }
    },
    pipeline: {
      yourInput: 'Votre saisie',
      result: 'Résultat',
      generatedMedia: 'Image générée'
    },
    landing: {
      subtitlePrefix: 'Plateforme d\'expérimentation pédagogique-artistique de la',
      subtitleSuffix: 'pour l\'utilisation exploratoire de l\'IA générative dans l\'éducation médiatique culturelle-esthétique',
      research: '',
      features: {
        textTransformation: {
          title: 'Transformation de texte',
          description: 'Changement de perspective par l\'IA — votre prompt est transformé à travers des prismes artistiques-pédagogiques en image, vidéo et son.'
        },
        imageTransformation: {
          title: 'Transformation d\'image',
          description: 'Transformez des images à travers différents modèles et perspectives en nouvelles images et vidéos.'
        },
        multiImage: {
          title: 'Fusion d\'images',
          description: 'Combinez plusieurs images et fusionnez-les en nouvelles compositions visuelles grâce aux modèles d\'IA.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Composition visuelle de workflows — connectez des modules par glisser-déposer en pipelines IA personnalisés.'
        },
        music: {
          title: 'Génération musicale',
          description: 'Création musicale par IA avec paroles, tags et contrôle stylistique.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Recherche en espace vectoriel — surréalisation, élimination de dimensions, interpolation d\'embeddings.'
        }
      }
    },
    research: {
      locked: 'Uniquement disponible en mode recherche',
      lockedHint: 'Nécessite le niveau de sécurité « Adulte » ou « Recherche » (config.py)',
      complianceTitle: 'Avis mode recherche',
      complianceWarning: 'En mode recherche, aucun filtre de sécurité n\'est actif pour les prompts ou les images générées. Des résultats inattendus ou inappropriés peuvent survenir.',
      complianceAge: 'Ce mode n\'est pas recommandé pour les personnes de moins de 16 ans.',
      complianceConfirm: 'Je confirme avoir compris les avis',
      complianceCancel: 'Annuler',
      complianceProceed: 'Continuer'
    },
    presetOverlay: {
      title: 'Choisir une perspective',
      close: 'Fermer'
    },
    imageUpload: {
      clickHere: 'Cliquez ici',
      orDragImage: 'ou glissez une image ici',
      formatHint: 'PNG, JPG, WEBP (max 10 Mo)',
      invalidFormat: 'Format de fichier invalide. Seuls PNG, JPG et WEBP sont acceptés.',
      fileTooLarge: 'Fichier trop volumineux. Maximum : {max} Mo',
      uploadFailed: 'Échec du téléchargement',
      infoOriginal: 'Original :',
      infoSize: 'Taille :'
    },
    mediaInput: {
      choosePreset: 'Choisir une perspective',
      translateToEnglish: 'Traduire en anglais',
      copy: 'Copier',
      paste: 'Coller',
      delete: 'Supprimer',
      loading: 'Chargement...',
      contentBlocked: 'Contenu bloqué'
    },
    nav: {
      about: 'À propos',
      impressum: 'Mentions légales',
      privacy: 'Confidentialité',
      docs: 'Documentation',
      language: 'Changer de langue',
      settings: 'Paramètres',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'Nouveau workflow',
      importWorkflow: 'Importer',
      exportWorkflow: 'Exporter',
      execute: 'Exécuter',
      ready: 'Prêt',
      errors: 'erreurs',
      discardWorkflow: 'Abandonner le workflow actuel ?',
      importError: 'Échec de l\'importation du fichier',
      selectTransformation: 'Sélectionner une transformation',
      selectOutput: 'Sélectionner un modèle de sortie',
      search: 'Rechercher...',
      noResults: 'Aucun résultat trouvé',
      dragHint: 'Cliquez ou glissez des modules sur le canvas',
      editNameHint: '(double-cliquez pour modifier)',
      modules: 'Modules',
      toggleSidebar: 'Basculer la barre latérale',
      dsgvoTooltip: 'Les workflows Canvas peuvent utiliser des API LLM externes. La conformité au RGPD relève de la responsabilité de l\'utilisateur.',
      batchExecute: 'Exécution par lot',
      batchExecution: 'Exécution par lot',
      batchAbort: 'Interrompre le lot',
      abort: 'Interrompre',
      cancel: 'Annuler',
      loading: 'Chargement...',
      executingWorkflow: 'Exécution du workflow...',
      starting: 'Démarrage...',
      nodes: 'nœuds',
      batchRunCount: 'Nombre d\'exécutions',
      batchUseSeed: 'Utiliser un seed de base',
      batchBaseSeed: 'Seed de base',
      batchSeedHint: 'Chaque exécution : seed + index',
      batchStart: 'Lancer le lot',
      stage: {
        configSelectPlaceholder: 'Sélectionner...',
        evaluationCriteriaFallback: 'Critères d\'évaluation...',
        feedbackInputTitle: 'Entrée de retour',
        deleteTitle: 'Supprimer',
        selectLlmPlaceholder: 'Sélectionner un LLM...',
        resizeTitle: 'Redimensionner',
        input: {
          promptPlaceholder: 'Votre prompt...'
        },
        imageInput: {
          uploadLabel: 'Télécharger une image'
        },
        interception: {
          contextPromptLabel: 'Prompt de contexte',
          contextPromptPlaceholder: 'Instructions de transformation...'
        },
        translation: {
          translationPromptLabel: 'Prompt de traduction',
          translationPromptPlaceholder: 'Instructions de traduction...'
        },
        modelAdaption: {
          targetModelLabel: 'Modèle cible',
          noAdaptionOption: 'Pas d\'adaptation',
          videoModelsOption: 'Modèles vidéo',
          audioModelsOption: 'Modèles audio'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Critères de comparaison',
          criteriaPlaceholder: 'p. ex. Comparer par originalité, clarté, détail...',
          infoText: 'Connectez jusqu\'à 3 sorties texte'
        },
        seed: {
          modeLabel: 'Mode',
          modeFixed: 'Fixe',
          modeRandom: 'Aléatoire',
          valueLabel: 'Valeur',
          baseLabel: 'Base'
        },
        resolution: {
          customOption: 'Personnalisé',
          widthLabel: 'Largeur',
          heightLabel: 'Hauteur'
        },
        collector: {
          emptyText: 'En attente de l\'exécution...'
        },
        evaluation: {
          typeLabel: 'Type d\'évaluation',
          typeCreativity: 'Créativité',
          typeQuality: 'Qualité',
          typeCustom: 'Personnalisé',
          criteriaLabel: 'Critères d\'évaluation',
          outputTypeLabel: 'Type de sortie',
          outputCommentary: 'Commentaire + Binaire',
          outputScore: 'Commentaire + Score + Binaire',
          outputAll: 'Tout',
          evalPassTitle: 'Réussi (en avant)',
          evalFailTitle: 'Retour (en arrière)',
          evalCommentaryTitle: 'Commentaire (en avant)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Sélectionner un modèle de vision...',
          frameworkLabel: 'Cadre d\'analyse',
          frameworkPanofsky: 'Histoire de l\'art (Panofsky)',
          frameworkEducational: 'Théorie éducative',
          frameworkEthical: 'Éthique',
          frameworkCritical: 'Critique/Décolonial',
          frameworkCustom: 'Personnalisé',
          customPromptLabel: 'Prompt d\'analyse',
          customPromptPlaceholder: 'Décrivez comment l\'image doit être analysée...'
        },
        display: {
          imageAlt: 'Aperçu',
          emptyText: 'Aperçu (après exécution)'
        }
      }
    },
    about: {
      title: 'À propos du UCDCAE AI LAB',
      intro: 'Le UCDCAE AI LAB est une plateforme d\'expérimentation pédagogique-artistique de la Chaire UNESCO en Culture numérique et Arts dans l\'Éducation pour l\'utilisation exploratoire de l\'intelligence artificielle générative dans l\'éducation médiatique culturelle-esthétique. Elle a été développée dans le cadre des projets AI4ArtsEd et COMeARTS.',
      project: {
        title: 'Le projet',
        description: 'L\'IA transforme la société et le monde du travail ; elle devient de plus en plus un sujet d\'éducation. Le projet explore les opportunités, les conditions et les limites de l\'utilisation pédagogique de l\'intelligence artificielle (IA) dans des contextes d\'éducation culturelle sensibles à la diversité.',
        paragraph2: 'Dans trois sous-projets — Pédagogie générale (TPap), Informatique (TPinf) et Éducation artistique (TPkp) — la recherche pratique pédagogique orientée créativité et la conception et programmation informatique de l\'IA s\'entrelacent en étroite coopération. Dès le départ, le projet implique systématiquement des praticiens artistiques-pédagogiques dans le processus de conception ; il fait le pont entre la mise en œuvre pédagogique-pratique professionnelle (liée à la qualité, esthétique, éthique et aux valeurs) d\'une part et le processus de mise en œuvre et de formation du sous-projet informatique d\'autre part.',
        paragraph3: 'Un processus de conception participatif d\'environ deux ans vise à produire une technologie IA open source qui explore dans quelle mesure les systèmes d\'IA peuvent déjà intégrer des principes artistiques-pédagogiques à leur niveau structurel dans des conditions réelles favorables.',
        paragraph4: 'L\'accent est mis sur a) l\'applicabilité future et la valeur ajoutée des technologies hautement innovantes pour l\'éducation culturelle, b) la portée et les limites de la littératie IA chez les enseignants et les apprenants, et c) la question transversale de l\'évaluabilité et de l\'évaluation de la transformation des cadres pédagogiques par des acteurs non humains complexes en termes d\'éthique pédagogique et d\'évaluation technologique.',
        moreInfo: 'Plus d\'informations :'
      },
      subproject: {
        title: 'Sous-projet « Pédagogie générale »',
        description: 'Le sous-projet « Pédagogie générale » étudie les possibilités et les limites d\'un processus de conception IA artistique-pédagogique basé sur la recherche-action participative dans le cadre de la question de recherche conjointe du projet collaboratif. À cette fin, il mène une série de recherches, analyses, ateliers d\'experts et espaces ouverts au cours de la première année de projet. La phase suivante du projet, conçue comme une boucle de rétroaction en plusieurs cycles, explore l\'utilisation d\'un prototype avec des praticiens pédagogiques et des artistes-éducateurs, en particulier dans l\'éducation culturelle non formelle, comme processus éducatif transformatif relationnel et collectif.'
      },
      team: {
        title: 'Équipe',
        projectLead: 'Responsable du projet',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Institut d\'Éducation',
        leadChair: 'Chaire d\'Éducation avec spécialisation en Culture et Éducation esthétique',
        leadUnesco: 'Chaire UNESCO en Culture numérique et Arts dans l\'Éducation',
        researcher: 'Collaborateur/trice de recherche',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Institut d\'Éducation',
        researcherChair: 'Chaire d\'Éducation avec spécialisation en Culture et Éducation esthétique',
        researcherUnesco: 'Chaire UNESCO en Culture numérique et Arts dans l\'Éducation'
      },
      funding: {
        title: 'Financé par'
      }
    },
    legal: {
      impressum: {
        title: 'Mentions légales',
        publisher: 'Éditeur',
        represented: 'Représenté par le Président',
        responsible: 'Responsable du contenu',
        authority: 'Autorité de tutelle',
        moreInfo: 'Informations complémentaires',
        moreInfoText: 'Mentions légales complètes de la FAU :',
        funding: 'Financé par'
      },
      privacy: {
        title: 'Politique de confidentialité',
        notice: 'Avis : Le contenu généré est stocké sur le serveur à des fins de recherche. Aucune donnée utilisateur ou IP n\'est collectée. Les images téléchargées ne sont pas stockées.',
        usage: 'L\'utilisation de cette plateforme est exclusivement réservée aux partenaires de coopération enregistrés du UCDCAE AI LAB. Les accords de protection des données conclus dans ce cadre s\'appliquent. Pour toute question, veuillez contacter vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Documentation & Guide',
      intro: {
        title: 'Bienvenue',
        content: 'Expériences créatives avec les transformations IA.'
      },
      gettingStarted: {
        title: 'Premiers pas',
        step1: 'Sélectionnez des propriétés dans les quadrants',
        step2: 'Entrez du texte ou une image',
        step3: 'Lancez la transformation'
      },
      modes: {
        title: 'Modes',
        mode1: { name: 'Direct', desc: 'Expériences rapides' },
        mode2: { name: 'Texte', desc: 'Transformations textuelles' },
        mode3: { name: 'Image', desc: 'Procédés basés sur l\'image' }
      },
      support: {
        title: 'Support',
        content: 'Pour toute question :'
      },
      wikipedia: {
        title: 'Recherche Wikipédia',
        subtitle: 'Le savoir sur le monde comme partie des processus artistiques',
        feature: 'Les processus artistiques nécessitent non seulement un savoir esthétique, mais aussi des connaissances factuelles sur le monde. L\'IA recherche sur Wikipédia pendant la transformation pour trouver des informations factuelles.',
        languages: 'Plus de 70 langues sont prises en charge',
        languagesDesc: 'L\'IA choisit automatiquement la Wikipédia dans la langue appropriée pour chaque sujet :',
        examples: {
          nigeria: 'Sujet sur le Nigeria → Haoussa, Yorouba, Igbo ou anglais',
          india: 'Sujet sur l\'Inde → Hindi, tamoul, bengali ou autres langues régionales',
          indigenous: 'Cultures autochtones → Quechua, Māori, Inuktitut, etc.'
        },
        why: 'Transparence : Que sait l\'IA ?',
        whyDesc: 'Le système affiche toutes les tentatives de recherche : les articles trouvés (sous forme de liens cliquables) et les termes pour lesquels rien n\'a été trouvé. Cela rend visible ce que l\'IA pense savoir — et ce qu\'elle ne sait pas.',
        culturalRespect: 'Invitation à faire vos propres recherches',
        culturalRespectDesc: 'Les liens Wikipédia affichés sont une invitation à en apprendre davantage par vous-même. Cliquez sur les liens pour vérifier les sources et enrichir vos propres connaissances.',
        limitations: 'La recherche par IA est une aide, pas un substitut à votre propre engagement avec le sujet.'
      }
    },
    multiImage: {
      image1Label: 'Image 1',
      image2Label: 'Image 2 (optionnelle)',
      image3Label: 'Image 3 (optionnelle)',
      contextLabel: 'Décrivez ce que vous voulez faire avec les images',
      contextPlaceholder: 'p. ex. Insérer la maison de l\'image 2 et le cheval de l\'image 3 dans l\'image 1. Conserver les couleurs et le style de l\'image 1.',
      modeTitle: 'Plusieurs images → Image',
      selectConfig: 'Choisissez votre modèle :',
      generating: 'Les images sont en cours de fusion...'
    },
    imageTransform: {
      imageLabel: 'Votre image',
      contextLabel: 'Décrivez ce que vous voulez changer dans l\'image',
      contextPlaceholder: 'p. ex. Transformer en peinture à l\'huile... Rendre plus coloré... Ajouter un coucher de soleil...'
    },
    videoGeneration: {
      promptLabel: 'Votre id\u00E9e vid\u00E9o',
      promptPlaceholder: 'p. ex. Une montgolfi\u00E8re survolant un paysage montagneux au coucher du soleil...',
      modelLabel: 'Choisissez un mod\u00E8le vid\u00E9o :',
      generating: 'G\u00E9n\u00E9ration de la vid\u00E9o en cours...'
    },
    textTransform: {
      inputLabel: 'Votre idée = QUOI ?',
      inputTooltip: 'Entrez le sujet de votre création.',
      inputPlaceholder: 'p. ex. Une fête dans ma rue : ...',
      contextLabel: 'Vos règles = COMMENT ?',
      contextTooltip: 'Entrez comment votre idée doit être présentée, ou cliquez sur l\'icône cercle !',
      contextPlaceholder: 'p. ex. Décrivez tout tel que les oiseaux dans les arbres le perçoivent !',
      resultLabel: 'Idée + Règles = Prompt',
      resultPlaceholder: 'Le prompt apparaîtra après avoir cliqué sur démarrer (ou entrez votre propre texte)',
      optimizedLabel: 'Prompt optimisé pour le modèle',
      optimizedPlaceholder: 'Le prompt optimisé apparaîtra après la sélection du modèle.'
    },
    training: {
      info: {
        title: 'À propos de l\'entraînement LoRA',
        studioDescription: 'Entraînez des modèles LoRA personnalisés pour Stable Diffusion 3.5 Large avec vos propres images.',
        description: 'Cet entraînement intégré est conçu pour des tests rapides.',
        limitations: 'Limitations',
        limitationDuration: 'L\'entraînement dure 1 à 3 heures',
        limitationBlocking: 'Bloque la génération d\'images pendant l\'entraînement',
        limitationConfig: 'Options de configuration limitées',
        showMore: 'En savoir plus',
        showLess: 'Afficher moins'
      },
      placeholders: {
        projectName: 'p. ex. Notre bâtiment scolaire',
        triggerWords: 'p. ex. notre_ecole, cour_de_recreation, salle_de_classe'
      },
      labels: {
        projectName: 'Nom du projet',
        triggerWords: 'Mots déclencheurs',
        triggerHelp: 'Tags séparés par des virgules. Le premier = déclencheur principal, le reste = tags supplémentaires par image.',
        images: 'Images d\'entraînement (10–50 recommandées)',
        dropZone: 'Cliquez ou déposez des images ici',
        imagesSelected: '{count} images sélectionnées',
        logs: 'Journaux d\'entraînement',
        waiting: 'En attente du démarrage de l\'entraînement...'
      },
      buttons: {
        start: 'Lancer l\'entraînement',
        stop: 'Arrêter',
        inProgress: 'Entraînement en cours...',
        delete: 'Supprimer les fichiers du projet (RGPD)',
        cancel: 'Annuler'
      },
      vram: {
        title: 'Vérification VRAM GPU',
        checking: 'Vérification de la VRAM...',
        used: 'utilisée',
        free: 'libre',
        notEnough: 'Pas assez de VRAM libre pour l\'entraînement (besoin de {gb} Go).',
        clearQuestion: 'Libérer la VRAM pour continuer ?',
        enough: 'Assez de VRAM disponible pour l\'entraînement.',
        clearing: 'Libération de la VRAM...',
        newFree: 'Nouvelle VRAM libre',
        clearBtn: 'Libérer la VRAM ComfyUI + Ollama'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Filtre d\'âge',
      dsgvo_ner: 'RGPD',
      dsgvo_llm: 'RGPD',
      translation: '\u2192 EN',
      fast_filter: 'Contenu',
      llm_context_check: 'Contenu (LLM)',
      llm_safety_check: 'Protection de la jeunesse',
      llm_check_failed: 'Vérification échouée',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Votre prompt était correct, mais l\'image générée a été signalée comme inappropriée par une IA d\'analyse d\'image. Cela peut arriver — la génération d\'images n\'est pas toujours prévisible. Réessayez, chaque génération est différente !',
      para86a: 'Votre prompt a été bloqué car il contient des symboles ou termes interdits par la loi allemande (§86a StGB). Cette règle nous protège tous de la haine et de la violence. Essayez un autre sujet !',
      dsgvo: 'Votre prompt a été bloqué car il contient ce qui ressemble à un nom de personne. Ceci est protégé par le Règlement général sur la protection des données (RGPD). Utilisez des descriptions comme « une fille » ou « un vieil homme » au lieu de noms.',
      kids: 'Votre prompt a été bloqué par le filtre de sécurité enfants. Certains termes ne conviennent pas aux enfants car ils peuvent être effrayants ou perturbants. Essayez de décrire votre idée avec des mots plus amicaux !',
      youth: 'Votre prompt a été bloqué par le filtre de protection de la jeunesse. Certains contenus ne conviennent pas non plus aux adolescents. Essayez de reformuler votre idée !',
      generic: 'Votre prompt a été bloqué par le système de sécurité. Le système vous protège des contenus inappropriés. Essayez une formulation différente !',
      inputImage: 'L\'image téléchargée a été signalée comme inappropriée par une IA d\'analyse d\'image. Veuillez utiliser une autre image.',
      vlmSaw: 'L\'IA d\'image a vu',
      systemUnavailable: 'Le système de sécurité (Ollama) ne répond pas, aucun traitement supplémentaire n\'est possible. Veuillez contacter l\'administrateur système.',
      suggestionLoading: 'Attendez, j\'ai une idée...',
      suggestionError: 'Je n\'ai pas pu générer de suggestion pour le moment. Réessayez avec d\'autres mots !'
    },
    splitCombine: {
      infoTitle: 'Split & Combine — Fusion sémantique de vecteurs',
      infoDescription: 'Ce workflow fusionne deux prompts au niveau des vecteurs sémantiques. Le résultat n\'est pas un simple mélange, mais une connexion mathématique plus profonde des espaces de sens.',
      purposeTitle: 'Objectif pédagogique',
      purposeText: 'Explorez comment les modèles d\'IA représentent le sens sous forme d\'espaces numériques. Que se passe-t-il lorsque nous fusionnons mathématiquement des concepts différents ?',
      techTitle: 'Détails techniques',
      techText: 'Modèle : SD3.5 Large | Encodeur : DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Élimination partielle — Déconstruction vectorielle',
      infoDescription: 'Ce workflow manipule spécifiquement des parties du vecteur sémantique. En éliminant certaines dimensions, nous pouvons observer quels aspects du sens sont perdus.',
      purposeTitle: 'Objectif pédagogique',
      purposeText: 'Comprenez comment le sens est encodé à travers différentes dimensions de l\'espace vectoriel. Que reste-t-il quand on « éteint » des parties ?',
      techTitle: 'Détails techniques',
      techText: 'Modèle : SD3.5 Large | Encodeur : TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Encodeur de texte',
      modeLabel: 'Mode d\'élimination',
      dimensionRange: 'Plage de dimensions',
      selected: 'Sélectionné',
      dimensions: 'Dimensions',
      emptyTitle: 'En attente de la génération...',
      emptySubtitle: 'Les résultats apparaîtront ici',
      referenceLabel: 'Image de référence',
      referenceDesc: 'Sortie non manipulée (original)',
      innerLabel: 'Plage intérieure éliminée',
      outerLabel: 'Plage extérieure éliminée'
    },
    surrealizer: {
      infoTitle: 'Surréalisateur — Extrapolation au-delà du connu',
      infoDescription: 'Deux « cerveaux » d\'IA lisent votre texte : CLIP-L comprend le langage à travers les images, T5 le comprend de manière purement linguistique. Le curseur ne fait pas simplement un mélange — il pousse l\'image bien au-delà de ce que T5 seul produirait. L\'IA doit alors interpréter des vecteurs qu\'elle n\'a jamais rencontrés pendant l\'entraînement. Le résultat : des hallucinations d\'IA — des images qu\'aucun prompt ne pourrait produire directement.',
      purposeTitle: 'Le curseur',
      purposeText: 'α < 0 : CLIP-L amplifié, T5 inversé — les 3328 dimensions supérieures (où CLIP-L est rempli de zéros) reçoivent des vecteurs T5 inversés. Les patterns d\'attention croisée dans le transformer s\'inversent : hallucinations pilotées visuellement. ◆ α = 0 : CLIP-L pur — image normale. ◆ α = 1 : T5-XXL pur — encore normal, mais qualité différente. ◆ α > 1 : extrapolation au-delà de T5. À α = 20, la formule pousse l\'embedding 19× au-delà de T5 dans un espace vectoriel inexploré — hallucinations pilotées linguistiquement. ◆ Sweet spot : α = 15–35.',
      techTitle: 'Comment ça fonctionne',
      techText: 'Votre prompt est envoyé séparément à travers deux encodeurs : CLIP-L (entraîné visuellement, 77 tokens, 768 dims → rempli à 4096) et T5-XXL (entraîné linguistiquement, 512 tokens, 4096 dims). Les 77 premières positions de tokens sont fusionnées : (1-α)·CLIP-L + α·T5. Les tokens T5 restants (78–512) restent inchangés comme ancre sémantique — ils maintiennent l\'image liée à votre texte quel que soit l\'extrémité de α. À α > 1, ce n\'est pas un mélange mais une extrapolation : des vecteurs qu\'aucun entraînement n\'a jamais produits. À α < 0, T5 est inversé et CLIP-L amplifié — des hallucinations qualitativement différentes car les patterns d\'attention croisée dans le transformer sont inversés.',
      sliderLabel: 'Extrapolation (α)',
      sliderNormal: 'normal',
      sliderWeird: 'étrange',
      sliderCrazy: 'fou',
      sliderExtremeWeird: 'super étrange',
      sliderExtremeCrazy: 'super fou',
      sliderHint: "α<0 : au-delà de CLIP {'|'} α=0 : CLIP pur {'|'} α=1 : T5 pur {'|'} α>1 : au-delà de T5",
      expandLabel: 'Enrichir le prompt pour T5',
      expandSuggest: 'Prompt court détecté — l\'expansion T5 améliore significativement les résultats avec peu de mots.',
      expandHint: 'Votre prompt contient peu de mots (~{count} tokens CLIP). Pour des hallucinations optimales, l\'IA peut enrichir narrativement le contexte T5.',
      expandActive: 'Enrichissement du prompt...',
      expandResultLabel: 'Expansion T5 (encodeur T5 uniquement)',
      advancedLabel: 'Paramètres avancés',
      negativeLabel: 'Prompt négatif',
      negativeHint: 'Extrapolé avec le même α. Détermine de quoi l\'image s\'éloigne par extrapolation — des négatifs différents produisent des esthétiques fondamentalement différentes.',
      cfgLabel: 'Échelle CFG',
      cfgHint: 'Classifier-Free Guidance : force d\'influence du prompt. Plus élevé = effet plus fort, moins de variation.'
    },
    musicGeneration: {
      infoTitle: 'Génération musicale',
      infoDescription: 'Créez de la musique à partir de texte et de tags de style. L\'IA génère des mélodies, des rythmes et des harmonies à partir de vos paroles et spécifications de genre.',
      purposeTitle: 'Objectif pédagogique',
      purposeText: 'Explorez comment l\'IA interprète les concepts musicaux. Comment le choix des mots dans les paroles affecte-t-il la mélodie ?',
      lyricsLabel: 'Paroles (Texte)',
      lyricsPlaceholder: '[Couplet]\nVos paroles ici...\n\n[Refrain]\nRefrain...',
      tagsLabel: 'Tags de style',
      tagsPlaceholder: 'pop, piano, entraînant, voix féminine, 120bpm',
      selectModel: 'Choisissez un modèle musical :',
      generate: 'Générer de la musique',
      generating: 'Génération de la musique...'
    },
    musicGen: {
      simpleMode: 'Simple',
      advancedMode: 'Avancé',
      lyricsLabel: 'Paroles',
      lyricsPlaceholder: 'Écrivez les paroles de votre chanson avec des marqueurs de structure comme [Couplet], [Refrain], [Pont]...\n\nExemple :\n[Couplet]\ndo ré mi fa sol\nla si do ré mi\n\n[Refrain]\nc\'est tout ce que je veux te chanter',
      tagsLabel: 'Tags de style',
      tagsPlaceholder: 'Genre, ambiance, instruments...\n\nExemple : ska, agressif, entraînant, haute définition, trio basse et saxophone',
      refineButton: 'Affiner paroles et tags',
      refinedLyricsLabel: 'Paroles affinées',
      refinedLyricsPlaceholder: 'Vos paroles affinées apparaîtront ici...',
      refiningLyricsMessage: 'L\'IA affine vos paroles...',
      refinedTagsLabel: 'Tags affinés',
      refinedTagsPlaceholder: 'Les tags de style affinés apparaîtront ici...',
      refiningTagsMessage: 'L\'IA génère des tags de style correspondants...',
      selectModel: 'Choisir un modèle musical',
      generateButton: 'Générer de la musique',
      quality: 'Qualité'
    },
    musicGenV2: {
      lyricsWorkshop: 'Atelier paroles',
      lyricsInput: 'Votre texte',
      lyricsPlaceholder: 'Écrivez des paroles, un thème, des mots-clés ou une ambiance...',
      themeToLyrics: 'Mots-clés vers paroles de chanson',
      refineLyrics: 'Structurer les paroles',
      resultLabel: 'Résultat',
      resultPlaceholder: 'Vos paroles apparaîtront ici...',
      expandingTheme: 'L\'IA écrit des paroles à partir de vos mots-clés...',
      refiningLyrics: 'L\'IA structure vos paroles...',
      soundExplorer: 'Explorateur sonore',
      suggestFromLyrics: 'Suggérer à partir des paroles',
      suggestingTags: 'L\'IA analyse vos paroles...',
      mostImportant: 'le plus important',
      dimGenre: 'Genre',
      dimTimbre: 'Timbre',
      dimGender: 'Voix',
      dimMood: 'Ambiance',
      dimInstrument: 'Instruments',
      dimScene: 'Scène',
      dimRegion: 'Région (UNESCO)',
      dimTopic: 'Thème',
      audioLength: 'Durée audio',
      generateButton: 'Générer de la musique',
      selectModel: 'Modèle',
      customTags: 'Tags personnalisés',
      customTagsPlaceholder: 'p. ex. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        image: 'Laboratoire d\'images',
        textlab: 'Latent Text Lab',
        crossmodal: 'Laboratoire crossmodal'
      },
      imageLab: {
        headerTitle: 'Laboratoire d\'images — Recherche visuelle en espace vectoriel',
        headerSubtitle: 'Cinq outils pour explorer comment les modèles de diffusion génèrent des images à partir de texte : du débruitage à l\'attention et la fusion, jusqu\'à l\'arithmétique vectorielle.',
        tabs: {
          archaeology: {
            label: 'Archéologie du débruitage',
            short: 'Observer le modèle travailler'
          },
          attention: {
            label: 'Cartographie d\'attention',
            short: 'Voir où le modèle regarde'
          },
          fusion: {
            label: 'Fusion d\'encodeurs',
            short: 'Mélange surréaliste'
          },
          probing: {
            label: 'Sondage de caractéristiques',
            short: 'Analyse dimensionnelle'
          },
          algebra: {
            label: 'Algèbre de concepts',
            short: 'Arithmétique vectorielle'
          }
        }
      },
      comingSoon: 'Cet outil sera implémenté dans une version future.',
      shared: {
        negativeHint: 'Termes que le modèle doit activement éviter (ex. "flou, texte")',
        stepsHint: 'Plus d\'étapes = meilleure qualité mais temps de génération plus long',
        cfgHint: 'Classifier-Free Guidance : plus élevé = plus fidèle au prompt, moins de variation',
        seedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
        recordingActive: 'Enregistrement actif',
        recordingCount: '{count} enregistrement | {count} enregistrements',
        recordingTooltip: 'Les données de recherche sont sauvegardées automatiquement',
      },
      attention: {
        headerTitle: 'Cartographie d\'attention — Quel mot dirige quelle région de l\'image ?',
        headerSubtitle: 'Pour chaque mot du prompt, une heatmap superposée à l\'image générée montre OÙ dans l\'image ce mot a eu le plus d\'influence. Cela révèle comment le modèle distribue spatialement les concepts sémantiques.',
        explanationToggle: 'Afficher l\'explication détaillée',
        explainWhatTitle: 'Que montre cet outil ?',
        explainWhatText: 'Lorsqu\'un modèle de diffusion génère une image, il ne lit pas le prompt mot à mot comme un ensemble d\'instructions. Au lieu de cela, un mécanisme appelé « attention » distribue l\'influence de chaque mot à travers différentes régions de l\'image. Le mot « maison » influence principalement la région où apparaît la maison — mais aussi les zones voisines, car le modèle comprend le contexte de toute la scène. Cet outil rend cette distribution visible : cliquez sur un mot et voyez quelles régions de l\'image s\'illuminent.',
        explainHowTitle: 'Comment lire la heatmap ?',
        explainHowText: 'Couleur vive et intense = forte influence du mot sur cette région. Couleur sombre ou absente = peu d\'influence. Si vous sélectionnez plusieurs mots, ils apparaissent en couleurs différentes. Note : les cartes ne sont PAS parfaitement délimitées — ce n\'est pas un bug, mais cela montre que le modèle traite les concepts contextuellement, pas de manière isolée. Une « maison » dans une scène de ferme a aussi une certaine influence sur les animaux et les champs, car le modèle comprend la scène dans son ensemble.',
        explainReadTitle: 'Que révèlent les deux curseurs ?',
        explainReadText: 'Le curseur d\'étape de débruitage montre QUAND dans le processus de génération en 25 étapes vous observez l\'attention. Les premières étapes montrent la planification grossière de la mise en page, les dernières étapes l\'attribution des détails. Le sélecteur de profondeur réseau montre OÙ dans le transformer l\'attention est mesurée : les couches superficielles (près de l\'entrée) montrent la planification de la composition globale, les couches médianes l\'attribution sémantique, les couches profondes le peaufinage. Les deux axes sont indépendants — il vaut la peine d\'explorer systématiquement différentes combinaisons.',
        techTitle: 'Détails techniques',
        techText: 'SD3.5 utilise un MMDiT (Multimodal Diffusion Transformer) avec attention jointe : les tokens image et texte s\'attendent mutuellement à travers 24 blocs transformer. Nous remplaçons le processeur SDPA par défaut par un processeur softmax(QK^T/√d) manuel à 3 blocs sélectionnés pour extraire la sous-matrice d\'attention texte→image. Les cartes sont en résolution 64x64 (grille de patches), mises à l\'échelle par interpolation bilinéaire. SD3.5 utilise deux encodeurs de texte : CLIP-L (BPE, 77 tokens) et T5-XXL (SentencePiece, 512 tokens). Les deux peuvent être activés ici pour voir comment différentes stratégies de tokenisation affectent l\'attention.',
        referencesTitle: 'Références scientifiques',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ex. Une maison se dresse dans un paysage, entourée de terres agricoles, de nature et d\'animaux. Quelques personnes sont visibles.',
        generate: 'Générer + Analyser',
        generating: 'Génération de l\'image et extraction de l\'attention...',
        emptyHint: 'Entrez un prompt et cliquez sur Générer pour visualiser les cartes d\'attention du modèle.',
        advancedLabel: 'Paramètres avancés',
        negativeLabel: 'Prompt négatif',
        stepsLabel: 'Étapes',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Tokens',
        tokensHint: 'Cliquez sur un ou plusieurs mots. Les sous-tokens (p. ex. « Mai » + « son ») sont automatiquement combinés. Plusieurs mots apparaissent en couleurs différentes.',
        timestepLabel: 'Étape de débruitage',
        timestepHint: 'Les modèles de diffusion génèrent des images en 25 étapes, du bruit à l\'image. Les premières étapes établissent la structure grossière, les dernières affinent les détails. Ce curseur montre ce à quoi le modèle prête attention à chaque étape.',
        step: 'Étape',
        layerLabel: 'Profondeur réseau',
        layerHint: 'À chaque étape de débruitage, le signal traverse les 24 couches transformer. Les couches superficielles (près de l\'entrée) capturent la composition globale, les couches médianes l\'attribution sémantique, les couches profondes (près de la sortie) les détails fins. Les deux contrôles sont indépendants : étape = quand dans le processus, profondeur = où dans le réseau.',
        layerEarly: 'Superficiel (Composition)',
        layerMid: 'Médian (Sémantique)',
        layerLate: 'Profond (Détail)',
        opacityLabel: 'Heatmap',
        opacityHint: 'Intensité de la superposition colorée sur l\'image.',
        baseImageLabel: 'Image de base',
        baseColor: 'Couleur',
        baseBW: 'N/B',
        baseOff: 'Désactivé',
        baseImageHint: 'Couleur affiche l\'image originale. N/B la désature pour que les couleurs de la heatmap ressortent. Désactivé masque entièrement l\'image et n\'affiche que la carte d\'attention.',
        encoderLabel: 'Encodeur de texte',
        encoderClipL: 'CLIP-L (77 Tokens)',
        encoderT5: 'T5-XXL (512 Tokens)',
        encoderHint: 'SD3.5 utilise deux encodeurs de texte avec une tokenisation différente. CLIP-L utilise BPE (Byte-Pair Encoding), T5-XXL utilise SentencePiece. Comparez comment les deux encodeurs traitent le même prompt et quelles régions de l\'image chacun dirige.',
        download: 'Télécharger l\'image'
      },
      probing: {
        headerTitle: 'Sondage de caractéristiques — Quelles dimensions encodent quoi ?',
        headerSubtitle: 'Comparez deux prompts et découvrez quelles dimensions d\'embedding encodent la différence sémantique. Transférez sélectivement des dimensions individuelles pour voir comment elles affectent l\'image.',
        explanationToggle: 'Afficher l\'explication détaillée',
        explainWhatTitle: 'Que montre cet outil ?',
        explainWhatText: 'Chaque mot est converti par l\'encodeur de texte en un vecteur de haute dimension (p. ex. 4096 dimensions pour T5). Lorsque vous changez un mot dans le prompt — p. ex. « rouge » en « bleu » — certaines dimensions changent plus que d\'autres. Cet outil vous montre QUELLES dimensions changent le plus et vous permet de transférer sélectivement des dimensions individuelles du prompt B vers le prompt A.',
        explainHowTitle: 'Comment fonctionne le transfert ?',
        explainHowText: 'Le diagramme à barres montre toutes les dimensions triées par magnitude de différence. Utilisez les contrôles de plage de rang (De/À) pour sélectionner une fenêtre — p. ex. juste les 100 premières ou spécifiquement les rangs 880–920. Cliquer sur « Transférer » régénère l\'image avec les mêmes paramètres (même seed !) — mais avec les dimensions sélectionnées du prompt B. Cela vous permet de voir exactement ce que ces dimensions « encodent ».',
        explainReadTitle: 'Comment lire le diagramme à barres ?',
        explainReadText: 'Chaque barre représente une dimension d\'embedding. La longueur montre combien cette dimension diffère entre le prompt A et B. Les dimensions avec de grandes différences sont les porteurs les plus probables du changement sémantique. Mais attention : les embeddings sont distribués — souvent plusieurs dimensions ensemble sont nécessaires pour produire un changement visible.',
        techTitle: 'Détails techniques',
        techText: 'SD3.5 utilise trois encodeurs de texte : CLIP-L (768d), CLIP-G (1280d) et T5-XXL (4096d). Vous pouvez sonder chacun individuellement. La différence est calculée comme déviation absolue moyenne sur toutes les positions de tokens : mean(abs(B-A), dim=tokens). Le transfert remplace les dimensions sélectionnées sur toutes les positions de tokens simultanément.',
        referencesTitle: 'Références scientifiques',
        promptALabel: 'Prompt A (Original)',
        promptBLabel: 'Prompt B (Comparaison)',
        promptAPlaceholder: 'p. ex. Une maison rouge au bord du lac',
        promptBPlaceholder: 'p. ex. Une maison bleue au bord du lac',
        encoderLabel: 'Encodeur',
        encoderAll: 'Tous (recommandé)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analyser',
        analyzing: 'Encodage et comparaison des prompts...',
        transferBtn: 'Transférer les dimensions vectorielles sélectionnées du Prompt B dans l\'image générée',
        transferring: 'Génération de l\'image avec l\'embedding modifié...',
        rankFromLabel: 'Du rang',
        rankToLabel: 'Au rang',
        sliderLabel: 'Sélectionner des dimensions du Prompt B',
        range1Label: 'Plage 1',
        range2Label: 'Plage 2',
        addRange: 'Ajouter une plage',
        selectionDesc: '{count} dimensions du Prompt B sélectionnées (rang {ranges} sur {total})',
        listTitle: 'Les {count} dimensions du Prompt B avec la plus grande différence par rapport au Prompt A',
        sortAsc: 'Croissant',
        sortDesc: 'Décroissant',
        originalLabel: 'Original (Prompt A)',
        modifiedLabel: 'Modifié (Transfert du Prompt B)',
        modifiedHint: 'Sélectionnez une plage de rangs ci-dessous et cliquez sur « Transférer » — ceci affichera le prompt A avec les dimensions transférées de B (même seed).',
        noDifference: 'Les embeddings sont identiques — modifiez le prompt B.',
        advancedLabel: 'Paramètres avancés',
        negativeLabel: 'Prompt négatif',
        stepsLabel: 'Étapes',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'Tout',
        selectNone: 'Aucun',
        encoderHint: 'Tous = tous les encodeurs combinés. CLIP-L/CLIP-G/T5 = isole un seul encodeur pour l\'analyse.',
        sliderHint: 'Sélectionnez une plage de rangs des dimensions d\'embedding les plus importantes (triées par différence entre A et B).',
        transferHint: 'Transfère les dimensions sélectionnées du Prompt B vers le Prompt A et génère une nouvelle image.',
        downloadOriginal: 'Télécharger l\'original',
        downloadModified: 'Télécharger le modifié'
      },
      algebra: {
        headerTitle: 'Algèbre de concepts \u2014 Arithmétique vectorielle sur les embeddings d\'image',
        headerSubtitle: 'Appliquez la célèbre analogie word2vec à la génération d\'images : Roi \u2212 Homme + Femme \u2248 Reine. Trois prompts sont encodés et combinés algébriquement.',
        explanationToggle: 'Afficher l\'explication détaillée',
        explainWhatTitle: 'Que montre cet outil ?',
        explainWhatText: 'En 2013, Mikolov a montré que les embeddings de mots encodent les relations sémantiques comme des directions linéaires : le vecteur de « Roi » moins « Homme » plus « Femme » donne un vecteur proche de « Reine ». Cet outil applique cette idée aux encodeurs de texte de SD3.5 : au lieu de mots isolés, vous manipulez des embeddings de prompts entiers. Le résultat est une image contenant le concept A mais avec B remplacé par C.',
        explainHowTitle: 'Comment fonctionne l\'algèbre \u2014 et pourquoi ne pas simplement utiliser un prompt négatif ?',
        explainHowText: 'Vous entrez trois prompts : A (base), B (soustraire) et C (ajouter). La formule est : Résultat = A \u2212 Échelle\u2081\u00d7B + Échelle\u2082\u00d7C. Les curseurs d\'échelle contrôlent l\'intensité : à 1.0, B est entièrement soustrait et C entièrement ajouté. À 0.5, seulement la moitié. Les valeurs au-dessus de 1.0 amplifient l\'effet. \u2014 Pourquoi ne pas simplement utiliser « A + C » comme prompt et « B » comme prompt négatif ? Parce que cela fait quelque chose de fondamentalement différent : un prompt négatif dirige le processus de débruitage loin de B à CHACUNE des 25 étapes \u2014 le modèle décide étape par étape comment interpréter « pas B ». L\'algèbre de concepts calcule plutôt un nouveau vecteur AVANT la génération d\'image : la soustraction se fait dans l\'espace d\'embedding, pas dans le processus de diffusion. Le résultat est un vecteur unique qui encode directement « A sans la B-ité plus la C-ité ». Le prompt négatif dit « ne fais pas ça ». L\'algèbre dit « retire ce concept et mets celui-ci » \u2014 une opération chirurgicale dans l\'espace des sens plutôt qu\'une stratégie d\'évitement étape par étape.',
        explainReadTitle: 'Que signifient les résultats ?',
        explainReadText: 'À gauche vous voyez l\'image de référence (prompt A uniquement, même seed). À droite, le résultat de l\'algèbre. Si l\'analogie fonctionne, l\'image de droite devrait montrer le concept A mais avec le changement sémantique B\u2192C. Exemple : « Coucher de soleil sur la plage » \u2212 « Plage » + « Montagnes » \u2248 « Coucher de soleil sur les montagnes ». La distance L2 montre à quel point le résultat s\'est éloigné de l\'original. \u2014 L\'opération est-elle commutative ? Non. La soustraction de B et l\'addition de C se font par rapport au vecteur A. La direction B\u2192C n\'a de sens que dans le contexte de A : « Roi \u2212 Homme » supprime les directions « masculines » du vecteur Roi, « + Femme » ajoute les directions « féminines » \u2014 le résultat atterrit près de « Reine ». C n\'est pas placé chirurgicalement là où B a été supprimé ; il est simplement ajouté. Que cela fonctionne quand même montre que les relations sémantiques sont encodées comme des directions linéaires cohérentes dans l\'espace vectoriel.',
        techTitle: 'Détails techniques',
        techText: 'L\'algèbre est effectuée sur les embeddings de l\'encodeur sélectionné : CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), ou tous combinés (589 tokens \u00d7 4096d). La même opération est aussi appliquée aux embeddings pooled (2048d). Les deux images utilisent le même seed pour une comparaison équitable.',
        referencesTitle: 'Références scientifiques',
        promptALabel: 'Prompt A (Base)',
        promptAPlaceholder: 'p. ex. Coucher de soleil sur la plage avec des palmiers',
        promptBLabel: 'Prompt B (Soustraire)',
        promptBPlaceholder: 'p. ex. Plage avec des palmiers',
        promptCLabel: 'Prompt C (Ajouter)',
        promptCPlaceholder: 'p. ex. Montagnes enneigées',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Encodeur',
        encoderAll: 'Tous (recommandé)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Calculer',
        generating: 'Calcul des embeddings et génération des images...',
        referenceLabel: 'Référence (Prompt A)',
        resultLabel: 'Résultat (A \u2212 B + C)',
        l2Label: 'Distance L2 par rapport à l\'original',
        advancedLabel: 'Paramètres avancés',
        negativeLabel: 'Prompt négatif',
        stepsLabel: 'Étapes',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Échelle de soustraction',
        scaleAddLabel: 'Échelle d\'addition',
        encoderHint: 'Tous = tous les encodeurs combinés. CLIP-L/CLIP-G/T5 = isole un seul encodeur pour l\'arithmétique.',
        scaleSubHint: 'Poids de la soustraction (B). Plus élevé = suppression plus forte du concept B.',
        scaleAddHint: 'Poids de l\'addition (C). Plus élevé = injection plus forte du concept C.',
        l2Hint: 'Distance euclidienne dans l\'espace d\'embedding. Plus petit = plus similaire, plus grand = plus différent.',
        downloadReference: 'Télécharger la référence',
        downloadResult: 'Télécharger le résultat',
        resultHint: 'Entrez trois prompts et cliquez sur Calculer \u2014 le résultat de l\'arithmétique vectorielle apparaîtra ici.'
      },
      archaeology: {
        headerTitle: 'Archéologie du débruitage \u2014 Comment le bruit devient-il une image ?',
        headerSubtitle: 'Observez chaque étape de débruitage. Les modèles de diffusion ne dessinent pas de gauche à droite \u2014 ils travaillent partout simultanément, des formes grossières aux détails fins.',
        explanationToggle: 'Afficher l\'explication détaillée',
        explainWhatTitle: 'Que montre cet outil ?',
        explainWhatText: 'Un modèle de diffusion crée une image en supprimant progressivement le bruit. Contrairement au dessin de gauche à droite, le modèle travaille sur TOUTES les régions de l\'image simultanément. Dans les premières étapes, des structures grossières émergent : où est le haut, où est le bas ? Où est l\'horizon ? Dans les étapes intermédiaires, le contenu sémantique apparaît : objets, formes, couleurs. Les étapes finales affinent les textures et les détails. Cet outil rend chaque étape visible.',
        explainHowTitle: 'Comment utiliser cet outil ?',
        explainHowText: 'Entrez un prompt et cliquez sur Générer. Le modèle produit 25 images intermédiaires (une par étape de débruitage). Elles apparaissent sous forme de pellicule en dessous. Cliquez sur une miniature ou utilisez le curseur de timeline pour voir chaque étape en taille réelle. Comparez les premières et dernières étapes : quand le modèle « sait-il » ce qu\'il dessine ?',
        explainReadTitle: 'Que révèlent les trois phases ?',
        explainReadText: 'Premières étapes (1\u20138) : Composition globale \u2014 structure de base, distribution des couleurs, planification de la mise en page. Étapes intermédiaires (9\u201317) : Émergence sémantique \u2014 les objets deviennent reconnaissables, les formes se cristallisent. Dernières étapes (18\u201325) : Affinement des détails \u2014 textures, bords, motifs fins. Les transitions sont graduelles, mais les phases montrent clairement : le modèle « planifie » d\'abord globalement, puis affine localement. Particulièrement révélateur : la toute première étape ne montre pas des pixels fins, mais des taches colorées. C\'est parce que le bruit est généré dans l\'espace latent (128\u00d7128 à 16 canaux), pas dans l\'espace pixel. Le VAE traduit chaque pixel latent en un patch d\'environ 8\u00d78 pixels \u2014 même du bruit gaussien pur devient des groupes de couleurs cohérents. Le modèle ne « pense » jamais en pixels individuels, mais toujours dans cet espace compressé.',
        techTitle: 'Détails techniques',
        techText: 'SD3.5 Large utilise Rectified Flow comme planificateur avec 25 étapes par défaut. À chaque étape, les vecteurs latents actuels sont décodés par le VAE (1024\u00d71024 JPEG). Le VAE (Variational Autoencoder) traduit l\'espace latent mathématique en pixels. La représentation latente est 128\u00d7128 à 16 canaux \u2014 chaque pixel latent correspond à un patch d\'environ 8\u00d78 pixels dans l\'image. C\'est pourquoi même la première étape montre des groupes colorés plutôt que du bruit pixel fin : le VAE interprète des vecteurs aléatoires à 16 dimensions comme des patches de couleur cohérents.',
        referencesTitle: 'Références scientifiques',
        promptLabel: 'Prompt',
        promptPlaceholder: 'p. ex. Une place de marché dans une ville médiévale avec des gens, des bâtiments et une fontaine',
        generate: 'Générer',
        generating: 'Génération de l\'image \u2014 enregistrement de chaque étape...',
        emptyHint: 'Entrez un prompt et cliquez sur Générer pour visualiser le processus de débruitage.',
        advancedLabel: 'Paramètres avancés',
        negativeLabel: 'Prompt négatif',
        stepsLabel: 'Étapes',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Pellicule de débruitage',
        timelineLabel: 'Étape',
        phaseEarly: 'Composition',
        phaseMid: 'Sémantique',
        phaseLate: 'Détail',
        phaseEarlyDesc: 'La structure globale et la distribution des couleurs émergent',
        phaseMidDesc: 'Les objets et les formes deviennent reconnaissables',
        phaseLateDesc: 'Les textures et les détails fins sont affinés',
        finalImageLabel: 'Image finale (pleine résolution)',
        timelineHint: 'Parcourt les étapes de débruitage — montre comment l\'image émerge du bruit vers la composition finale.',
        download: 'Télécharger l\'image'
      },
      textLab: {
        headerTitle: 'Latent Text Lab \u2014 Déconstruction scientifique de LLM',
        headerSubtitle: 'Ingénierie de représentation, archéologie comparative de modèles et analyse systématique de biais : trois outils basés sur la recherche pour étudier les modèles de langage.',
        explanationToggle: 'Afficher l\'explication',
        modelPanel: {
          presetLabel: 'Préréglage',
          presetNone: 'Pas de préréglage (ID personnalisé)',
          customModelLabel: 'ID modèle HuggingFace',
          customModelPlaceholder: 'p. ex. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Quantification',
          quantAuto: 'Auto',
          quantizationHint: 'bf16 = qualité maximale, int8 = moitié de VRAM, int4 = VRAM minimal mais qualité la plus basse',
        },
        temperatureHint: 'Aléatoire de la génération de texte. Bas = déterministe, haut = plus créatif.',
        maxTokensHint: 'Nombre maximum de tokens générés (morceaux de mots).',
        textSeedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
        tabs: {
          repeng: { label: 'Ing\u00e9nierie des repr\u00e9sentations', short: 'Trouver des vecteurs de guidage dans les LLM' },
          compare: { label: 'Comparaison de mod\u00e8les', short: 'Comparer deux LLM couche par couche' },
          bias: { label: 'Arch\u00e9ologie des biais', short: 'R\u00e9v\u00e9ler les biais cach\u00e9s dans les LLM' },
        },
        repeng: {
          title: 'Ingénierie de représentation',
          subtitle: 'Trouver des directions de concepts dans l\'espace d\'activation et guider la génération',
          explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
          explainWhatText: 'Bas\u00e9 sur Zou et al. (2023) \u00ab Representation Engineering \u00bb et Li et al. (2024) \u00ab Inference-Time Intervention \u00bb. Les LLM encodent des concepts abstraits comme des directions dans l\'espace d\'activation. \u2014 Refs: Zou et al. (arXiv:2310.01405), Li et al. (arXiv:2306.03341)',
          explainHowTitle: 'Comment l\'utiliser ?',
          explainHowText: 'Cette exp\u00e9rience extrait une \u00ab direction de v\u00e9rit\u00e9 \u00bb du mod\u00e8le. Les paires de contraste contiennent chacune une d\u00e9claration vraie et une fausse. Recommandation : les prompts en anglais fonctionnent mieux.',
          referencesTitle: 'Références scientifiques',
          expectedResults: 'Résultats attendus : À α = 0 (baseline), le modèle génère la bonne réponse. À α = -1 (inversion), une mauvaise réponse devrait apparaître — c\'est le cœur de l\'expérience. À α = +1, peu de choses changent car le modèle répond déjà correctement. Au-delà de |α| > 2, les artefacts dominent (répétitions, non-sens). Le « sweet spot » selon Zou et al. est |α| entre 0,5 et 2,0. La variance expliquée > 50% indique une séparation nette — en dessous, les paires de contraste sont trop similaires ou trop peu nombreuses (au moins 3 recommandées).',
          pairsTitle: 'Paires de contraste',
          pairsSubtitle: 'Au moins 3 paires recommandées. Chaque paire ne doit différer que par le concept cible (vrai vs. faux). Les exemples sont modifiables.',
          positiveLabel: 'Positif (vrai)',
          negativeLabel: 'Négatif (faux)',
          positivePlaceholder: 'p. ex. La capitale de la France est Paris',
          negativePlaceholder: 'p. ex. La capitale de la France est Berlin',
          addPair: 'Ajouter une paire',
          removePair: 'Supprimer',
          targetLayerLabel: 'Couche cible',
          targetLayerHint: 'Quelle couche du transformer reçoit le vecteur de direction. Différentes couches influencent différents aspects de la génération de texte.',
          targetLayerAuto: 'Dernière couche',
          findDirection: 'Trouver la direction',
          finding: 'Calcul de la direction du concept...',
          directionFound: 'Direction du concept trouvée',
          varianceLabel: 'Variance expliquée',
          dimLabel: 'Dimensions',
          projectionsTitle: 'Projections des paires de contraste',
          testTitle: 'Test + Manipulation',
          testSubtitle: 'Entrez une phrase et guidez la génération le long de la direction du concept',
          testPromptLabel: 'Prompt de test',
          testPromptPlaceholder: 'p. ex. La capitale de l\'Allemagne est',
          alphaLabel: 'Force de manipulation (α)',
          alphaHint: 'Force du vecteur de direction. 0 = aucun effet, plus élevé = influence plus forte des paires de contraste.',
          temperatureLabel: 'Température',
          maxTokensLabel: 'Tokens max',
          seedLabel: 'Seed (-1 = aléatoire)',
          generateBtn: 'Générer avec manipulation',
          generating: 'Exécution de la génération manipulée...',
          baselineLabel: 'Baseline (sans manipulation)',
          manipulatedLabel: 'Manipulé (α = {alpha})',
          projectionLabel: 'Projection sur la direction du concept',
          interpretationTitle: 'Interprétation',
          interpreting: 'Analyse des résultats...',
          interpretationError: 'Impossible de générer l\'interprétation'
        },
        compare: {
          title: 'Archéologie comparative de modèles',
          subtitle: 'Chargez deux modèles et comparez systématiquement leurs représentations internes',
          explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
          explainWhatText: 'Bas\u00e9 sur Belinkov (2022) et Olsson et al. (2022). La heatmap montre le CKA entre les couches des deux mod\u00e8les. \u2014 Refs: Belinkov (DOI:10.1162/coli_a_00422), Olsson et al. (arXiv:2209.11895)',
          explainHowTitle: 'Comment l\'utiliser ?',
          explainHowText: 'Le mod\u00e8le A est le pr\u00e9r\u00e9glage actif. Choisissez un second mod\u00e8le (B) et chargez-le. Entrez du texte et cliquez \u00ab Comparer \u00bb.',
          referencesTitle: 'Références scientifiques',
          modelATitle: 'Modèle A (du sélecteur de préréglage)',
          modelAHint: 'Changez via le menu déroulant ci-dessus',
          modelBTitle: 'Modèle B (second modèle)',
          modelBPresetLabel: 'Préréglage',
          modelBCustomLabel: 'ID modèle HuggingFace',
          modelBCustomPlaceholder: 'p. ex. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Charger le modèle B',
          modelBLoaded: 'Modèle B chargé',
          modelBNone: 'Modèle B non chargé',
          promptLabel: 'Prompt',
          promptPlaceholder: 'p. ex. Le chat était assis sur le tapis et regardait les oiseaux',
          seedLabel: 'Seed',
          temperatureLabel: 'Température',
          maxTokensLabel: 'Tokens max',
          compareBtn: 'Comparer',
          comparing: 'Comparaison des modèles...',
          heatmapTitle: 'Alignement des couches (CKA)',
          heatmapAxisA: 'Modèle A \u2014 Couches',
          heatmapAxisB: 'Modèle B \u2014 Couches',
          heatmapExplain: 'Cellules claires = forte similarité de représentation. Les motifs diagonaux montrent que les modèles traitent l\'information dans un ordre similaire.',
          attentionTitle: 'Comparaison d\'attention (dernière couche)',
          modelALabel: 'Modèle A',
          modelBLabel: 'Modèle B',
          generationTitle: 'Comparaison de génération (même seed)',
          layerStatsTitle: 'Statistiques des couches',
          interpretationTitle: 'Interprétation',
          interpreting: 'Analyse des résultats...',
          interpretationError: 'Impossible de générer l\'interprétation'
        },
        bias: {
          title: 'Archéologie de biais',
          subtitle: 'Expériences systématiques de biais par manipulation contrôlée de tokens',
          explainWhatTitle: 'Que montre cette exp\u00e9rience ?',
          explainWhatText: 'Bas\u00e9 sur Zou et al. (2023) et Bricken et al. (2023). Cet outil \u00e9tudie les biais syst\u00e9matiques. \u2014 Refs: Zou et al. (arXiv:2310.01405), Bricken et al. (transformer-circuits.pub/2023/monosemantic-features)',
          explainHowTitle: 'Comment l\'utiliser ?',
          explainHowText: 'Choisissez un type d\'exp\u00e9rience. Entrez un prompt qui invite le mod\u00e8le \u00e0 continuer. Les r\u00e9sultats montrent les g\u00e9n\u00e9rations de base vs. manipul\u00e9es.',
          referencesTitle: 'Références scientifiques',
          presetLabel: 'Type d\'expérience',
          presetGender: 'Genre \u2014 Supprimer les pronoms genrés',
          presetSentiment: 'Sentiment \u2014 Amplifier positif/négatif',
          presetDomain: 'Domaine \u2014 Amplifier scientifique/poétique',
          presetCustom: 'Expérience personnalisée',
          promptLabel: 'Prompt',
          promptPlaceholder: 'p. ex. Le médecin dit au patient',
          customBoostLabel: 'Tokens à amplifier (séparés par des virgules)',
          customBoostPlaceholder: 'p. ex. sombre,ombre,nuit',
          customSuppressLabel: 'Tokens à supprimer (séparés par des virgules)',
          customSuppressPlaceholder: 'p. ex. lumière,soleil,clair',
          numSamplesLabel: 'Échantillons par condition',
          temperatureLabel: 'Température',
          maxTokensLabel: 'Tokens max',
          seedLabel: 'Seed de base',
          runBtn: 'Lancer l\'expérience',
          running: 'Expérience de biais en cours...',
          baselineTitle: 'Baseline (sans manipulation)',
          groupTitle: 'Groupe : {name}',
          modeSuppress: 'supprimé',
          modeBoost: 'amplifié',
          tokensLabel: 'Tokens',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Supprime tous les pronoms genrés et observe quels défauts le modèle choisit.',
          sentimentDesc: 'Amplifie les mots positifs ou négatifs et mesure l\'impact sur l\'ensemble du flux textuel.',
          domainDesc: 'Amplifie le vocabulaire scientifique ou poétique et observe les changements de registre.',
          interpretationTitle: 'Interprétation',
          interpreting: 'Analyse des résultats...',
          interpretationError: 'Impossible de générer l\'interprétation'
        },
        error: {
          gpuUnreachable: 'Service GPU inaccessible. Est-il en marche ?',
          loadFailed: 'Échec du chargement du modèle.',
          operationFailed: 'Échec de l\'opération.'
        }
      },
      crossmodal: {
        headerTitle: 'Laboratoire crossmodal',
        headerSubtitle: 'Son à partir d\'espaces latents : manipulation d\'embeddings T5, génération audio guidée par image, transfert crossmodal',
        explanationToggle: 'Afficher l\'explication détaillée',
        generate: 'Générer',
        generating: 'Génération...',
        result: 'Résultat',
        seed: 'Seed',
        generationTime: 'Temps de génération',
        tabs: {
          synth: {
            label: 'Synthé audio latent',
            short: 'Manipulation d\'embeddings T5',
            title: 'Synthé audio latent',
            description: 'Manipulation directe de l\'espace de conditionnement T5 de Stable Audio (768d). Interpolez entre les prompts, extrapolez au-delà du prompt, mettez à l\'échelle les embeddings et injectez du bruit. Boucles ultra-courtes, quasi temps réel.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Image/texte vers audio (CVPR 2025)',
            title: 'MMAudio — Vidéo/Image vers audio',
            description: 'L\'image et le texte entrent dans le même réseau comme signaux séparés — l\'image n\'est pas traduite en langage, les deux guident simultanément la génération sonore. Le modèle a été entraîné conjointement sur vidéo et audio, apprenant des associations directes entre ce qui est vu et ce qui est entendu. Jusqu\'à 8s, 44,1kHz, ~1,2s de calcul. (Cheng et al., CVPR 2025, doi:10.48550/arXiv.2412.15322)'
          },
          guidance: {
            label: 'Guidage ImageBind',
            short: 'Guidage par gradient basé sur l\'image',
            title: 'Guidage par gradient ImageBind',
            description: 'Guidage par gradient pendant le processus de débruitage de Stable Audio. ImageBind fournit un espace partagé de 1024d pour l\'image et l\'audio — le gradient de la similarité cosinus guide la génération audio vers l\'embedding de l\'image.'
          }
        },
        synth: {
          explainWhatTitle: 'Que fait le Synth\u00e9 audio latent ?',
          explainWhatText: 'Stable Audio g\u00e9n\u00e8re du son \u00e0 partir de texte. Le texte est converti par un encodeur T5 en un vecteur num\u00e9rique de 768 dimensions \u2014 c\'est ce vecteur que vous manipulez ici. Au lieu de changer seulement \u00ab ce que \u00bb le mod\u00e8le g\u00e9n\u00e8re (via le prompt), vous changez \u00ab comment \u00bb le mod\u00e8le comprend le texte en interne. Deux prompts qui semblent similaires peuvent \u00eatre tr\u00e8s \u00e9loign\u00e9s dans cet espace \u2014 et vice versa.',
          explainHowTitle: 'Comment utiliser cet outil ?',
          explainHowText: 'Entrez un texte dans le Prompt A \u2014 cela d\u00e9termine le son de base. Optionnel : Prompt B comme point cible. Le curseur Alpha contr\u00f4le le m\u00e9lange : \u00e0 0 vous entendez uniquement A, \u00e0 1 uniquement B, \u00e0 0.5 un m\u00e9lange. Les valeurs au-dessus de 1 extrapolent au-del\u00e0 de B (le son devient plus extr\u00eame), les valeurs en dessous de 0 vont dans la direction oppos\u00e9e. Magnitude met \u00e0 l\'\u00e9chelle l\'ensemble de l\'embedding \u2014 des valeurs plus \u00e9lev\u00e9es produisent des sons plus intenses. Noise injecte de l\'al\u00e9atoire et cr\u00e9e des variations impr\u00e9visibles. La bande spectrale (sous le bouton Generate) affiche les 768 dimensions sous forme de barres. Vous pouvez d\u00e9placer des dimensions individuelles en cliquant et glissant, manipulant directement le son. Un clic droit r\u00e9initialise une dimension.',
          promptA: 'Prompt A (Base)',
          promptAPlaceholder: 'p. ex. vagues de l\'océan',
          promptB: 'Prompt B (Optionnel, pour interpolation)',
          promptBPlaceholder: 'p. ex. mélodie de piano',
          alpha: 'Alpha (Interpolation)',
          alphaHint: '0 = A uniquement, 1 = B uniquement, entre = mélange, >1 ou <0 = extrapolation',
          magnitude: 'Magnitude (Mise à l\'échelle)',
          magnitudeHint: 'Mise à l\'échelle globale de l\'embedding (1.0 = inchangé)',
          noise: 'Bruit',
          noiseHint: 'Bruit gaussien sur l\'embedding (0 = pas de bruit)',
          duration: 'Durée (s)',
          steps: 'Étapes',
          cfg: 'CFG',
          durationHint: 'Durée du clip audio généré en secondes',
          stepsHint: 'Étapes de débruitage. Plus = meilleure qualité.',
          cfgHint: 'Classifier-Free Guidance pour la génération audio',
          seedHint: '-1 = aléatoire, valeur fixe = résultat reproductible',
          loop: 'Lecture en boucle',
          loopOn: 'Boucle activée',
          loopOff: 'Boucle désactivée',
          stop: 'Arrêter',
          looping: 'En boucle',
          playing: 'Lecture',
          stopped: 'Arrêté',
          transpose: 'Transposition (demi-tons)',
          midiSection: 'Contrôle MIDI',
          midiUnsupported: 'Web MIDI n\'est pas pris en charge par ce navigateur.',
          midiInput: 'Entrée MIDI',
          midiNone: '(aucun)',
          midiMappings: 'Mappages CC',
          midiNoteC3: 'Note (C3 = Réf)',
          midiGenerate: 'Générer + Transposer',
          midiPitch: 'Hauteur rel. C3',
          loopInterval: 'Intervalle de boucle',
          loopOptimize: 'Auto-optimiser',
          loopPingPong: 'Ping-pong',
          loopIntervalHint: 'Début/fin de la région de boucle — raccourcissez la fin pour couper le fade-out de Stable Audio',
          modeLoop: 'Boucle',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Wavetable',
          modeRate: 'Tempo (rapide)',
          modePitch: 'Hauteur (OLA)',
          wavetableScan: 'Position de balayage',
          wavetableScanHint: 'Morphing entre les trames (0 = début, 1 = fin)',
          wavetableFrames: '{count} trames',
          midiScan: 'Position de balayage',
          adsrTitle: 'Enveloppe ADSR',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Enveloppe pour les notes MIDI (Attaque/Déclin/Sustain/Relâchement)',
          play: 'Lecture',
          normalize: 'Normaliser le volume',
          peak: 'Crête',
          crossfade: 'Fondu enchaîné',
          transposeHint: 'Décale la hauteur en demi-tons',
          crossfadeHint: 'Durée du fondu enchaîné à la frontière de boucle (ms)',
          normalizeHint: 'Normalise le volume à l\'amplitude maximale',
          saveRaw: 'Enregistrer brut',
          saveLoop: 'Enregistrer boucle',
          embeddingStats: 'Statistiques d\'embedding',
          dimensions: {
            section: 'Explorateur de dimensions',
            hint: 'Glissez sur les barres = définir l\'offset. Peignez horizontalement = plusieurs dimensions.',
            resetAll: 'Tout réinitialiser',
            hoverActivation: 'Activation',
            hoverOffset: 'Offset',
            rightClickReset: 'Clic droit = réinitialiser',
            sortDiff: 'Trié par différence de prompt',
            sortMagnitude: 'Trié par activation',
            activeOffsets: '{count} offsets actifs',
            applyAndGenerate: 'Appliquer et regénérer',
            undo: 'Annuler',
            redo: 'Rétablir'
          }
        },
        mmaudio: {
          explainWhatTitle: 'Que fait MMAudio ?',
          explainWhatText: 'MMAudio (Cheng et al., CVPR 2025) a \u00e9t\u00e9 entra\u00een\u00e9 conjointement sur la vid\u00e9o et l\'audio. Il ne traduit pas une image en texte puis en son, mais traite image et texte comme des signaux parall\u00e8les dans le m\u00eame r\u00e9seau. Le mod\u00e8le a appris quels sons correspondent \u00e0 quelles sc\u00e8nes visuelles \u2014 une for\u00eat produit des chants d\'oiseaux, une rue du bruit de circulation, une guitare des sons de pincement.',
          explainHowTitle: 'Comment utiliser cet outil ?',
          explainHowText: 'T\u00e9l\u00e9chargez une image et/ou entrez un prompt texte \u2014 utiliser les deux ensemble donne les r\u00e9sultats les plus riches. L\'image seule g\u00e9n\u00e8re des sons correspondant au contenu visuel. Le prompt texte peut orienter le son en compl\u00e9ment ou \u00eatre utilis\u00e9 seul sans image. Dans le Prompt n\u00e9gatif, d\u00e9crivez les sons que vous NE voulez PAS entendre (p. ex. \u00ab parole, musique \u00bb). Duration d\u00e9finit la dur\u00e9e (1-8 secondes). CFG Strength contr\u00f4le la fid\u00e9lit\u00e9 du mod\u00e8le au prompt \u2014 des valeurs basses (2-3) produisent des r\u00e9sultats plus vari\u00e9s, des valeurs hautes (6-8) plus fid\u00e8les au prompt.',
          imageUpload: 'Télécharger une image (optionnel)',
          prompt: 'Prompt texte (optionnel)',
          promptPlaceholder: 'p. ex. feu de camp crépitant',
          negativePrompt: 'Prompt négatif',
          duration: 'Durée (s)',
          maxDuration: 'Max 8s (limite du modèle)',
          cfg: 'CFG',
          steps: 'Étapes',
          compareHint: 'Comparer : Texte seul vs. Image + Texte'
        },
        guidance: {
          explainWhatTitle: 'Que fait le Guidage ImageBind ?',
          explainWhatText: 'ImageBind (Girdhar et al., CVPR 2023) r\u00e9unit six sens \u2014 image, son, texte, profondeur, chaleur, mouvement \u2014 dans un \u00ab langage \u00bb commun. Cet outil exploite ce terrain commun : pendant que le son se g\u00e9n\u00e8re \u00e9tape par \u00e9tape, il demande constamment \u00ab Est-ce que \u00e7a ressemble \u00e0 l\'image ? \u00bb et corrige la direction. La similarit\u00e9 cosinus dans le r\u00e9sultat montre \u00e0 quel point le son g\u00e9n\u00e9r\u00e9 s\'est rapproch\u00e9 du contenu de l\'image.',
          explainHowTitle: 'Comment utiliser cet outil ?',
          explainHowText: 'T\u00e9l\u00e9chargez une image \u2014 c\'est la direction cible pour le son. Optionnel : un prompt texte pour un guidage suppl\u00e9mentaire. Le curseur \u00ab \u03bb Guidance Strength \u00bb est le param\u00e8tre le plus important : des valeurs basses (0.01-0.05) laissent beaucoup de libert\u00e9 au son, des valeurs hautes (0.3-1.0) le lient \u00e9troitement \u00e0 l\'image. \u00ab Warmup Steps \u00bb d\u00e9termine \u00e0 partir de quelle \u00e9tape le guidage par image commence \u2014 des valeurs basses d\u00e9marrent imm\u00e9diatement, des valeurs plus \u00e9lev\u00e9es laissent la structure de base se former librement d\'abord. Total Steps et Duration contr\u00f4lent la qualit\u00e9 et la dur\u00e9e.',
          referencesTitle: 'Références scientifiques',
          imageUpload: 'Télécharger une image',
          prompt: 'Prompt de base (optionnel)',
          promptPlaceholder: 'p. ex. paysage sonore ambiant',
          lambda: 'Force de guidage',
          lambdaHint: 'À quel point l\'image guide la génération audio',
          warmupSteps: 'Étapes de préchauffage',
          warmupHint: 'Guidage par gradient uniquement pendant les N premières étapes',
          totalSteps: 'Étapes totales',
          duration: 'Durée (s)',
          cfg: 'CFG',
          totalStepsHint: 'Étapes totales de débruitage. Plus = meilleure qualité.',
          durationHint: 'Durée du clip audio généré en secondes',
          cosineSimilarity: 'Similarité cosinus (proximité image-audio)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '\ud83e\udd14 Le saviez-vous ?',
        learnMore: '\ud83d\udcda En savoir plus',
        currentlyHappening: '\u26a1 En ce moment :',
        energyUsed: 'Énergie utilisée',
        co2Produced: 'CO\u2082 produit'
      },
      energy: {
        kids_1: '\ud83d\udca1 Les images IA ont besoin d\'\u00e9lectricit\u00e9 \u2014 autant que charger votre t\u00e9l\u00e9phone pendant 3 heures !',
        kids_2: '\ud83d\udd0c Le GPU est comme un super calculateur qui consomme beaucoup d\'\u00e9nergie !',
        kids_3: '\u26a1 Chaque image a besoin d\'autant d\'\u00e9nergie qu\'une ampoule LED allum\u00e9e pendant 10 minutes !',
        youth_1: '\u26a1 Un GPU utilise {watts}W pendant la g\u00e9n\u00e9ration \u2014 comme un petit radiateur !',
        youth_2: '\ud83d\udd0b Une image utilise environ 0,01-0,02 kWh \u2014 \u00e7a semble peu, mais \u00e7a s\'accumule !',
        youth_3: '\ud83c\udf21\ufe0f Le GPU atteint {temp}\u00b0C en ce moment \u2014 c\'est pourquoi il a besoin de refroidissement !',
        expert_1: '\ud83d\udcca Temps r\u00e9el : {watts}W \u00e0 {util}% d\'utilisation = {kwh} kWh jusqu\'ici',
        expert_2: '\ud83d\udd25 Limite TDP : {tdp}W | Actuel : {watts}W ({percent}% de la limite)',
        expert_3: '\ud83d\udcbe VRAM : {used}/{total} Go ({percent}%) \u2014 mod\u00e8le + activations'
      },
      data: {
        kids_1: '\ud83e\uddee Le GPU fait 10 milliards de calculs en ce moment \u2014 plus vite que tu ne peux compter !',
        kids_2: '\ud83c\udfa8 L\'image est cr\u00e9\u00e9e en 50 petites \u00e9tapes \u2014 comme un puzzle qui se r\u00e9sout tout seul !',
        kids_3: '\ud83e\udde9 Des millions de nombres volent \u00e0 travers le GPU en ce moment !',
        youth_1: '\ud83d\udd04 Chaque image passe par ~50 \u00ab \u00e9tapes de d\u00e9bruitage \u00bb \u2014 50 tours de suppression de bruit !',
        youth_2: '\ud83d\udcd0 8 milliards de param\u00e8tres sont interrog\u00e9s \u2014 par image !',
        youth_3: '\ud83e\udde0 L\'IA \u00ab pense \u00bb en vecteurs avec des milliers de dimensions \u2014 comme des coordonn\u00e9es dans un espace.',
        expert_1: '\ud83d\udd2c MMDiT : Multimodal Diffusion Transformer \u2014 texte + image dans des couches d\'attention jointes',
        expert_2: '\ud83d\udcc8 Self-Attention : complexit\u00e9 O(n\u00b2) \u2014 chaque token \u00ab voit \u00bb tous les autres',
        expert_3: '\u2699\ufe0f Classifier-Free Guidance : \u00e9quilibre entre influence du prompt et cr\u00e9ativit\u00e9'
      },
      model: {
        kids_1: '\ud83c\udf93 Le mod\u00e8le IA a regard\u00e9 des millions d\'images pour apprendre \u00e0 peindre !',
        kids_2: '\ud83e\udd16 L\'IA est comme un artiste qui n\'oublie jamais ce qu\'il a vu !',
        kids_3: '\u2728 8 milliards de connexions dans le mod\u00e8le \u2014 plus que les \u00e9toiles visibles dans le ciel !',
        youth_1: '\ud83e\udde0 SD3.5 Large a 8 milliards de param\u00e8tres \u2014 comme 8 milliards de n\u0153uds de d\u00e9cision.',
        youth_2: '\ud83d\udcda 3 encodeurs de texte travaillent ensemble : CLIP-L, CLIP-G et T5-XXL',
        youth_3: '\ud83d\udd22 Le mod\u00e8le a besoin de {vram} Go de VRAM rien que pour \u00eatre charg\u00e9 !',
        expert_1: '\ud83c\udfd7\ufe0f Architecture : Rectified Flow + MMDiT avec 38 blocs transformer',
        expert_2: '\ud83d\udcca Quantification FP16/FP8 : compromis pr\u00e9cision vs. VRAM',
        expert_3: '\ud83d\udd17 LoRA : Low-Rank Adaptation \u2014 seulement 0,1% des param\u00e8tres r\u00e9-entra\u00een\u00e9s'
      },
      ethics: {
        kids_1: '\ud83c\udf0d L\'IA apprend \u00e0 partir d\'images sur internet \u2014 c\'est pourquoi il est important de respecter l\'art des autres !',
        kids_2: '\u2696\ufe0f Tous les artistes n\'ont pas \u00e9t\u00e9 consult\u00e9s pour savoir si l\'IA pouvait apprendre d\'eux.',
        kids_3: '\ud83e\udd1d Une bonne IA respecte le travail des gens !',
        youth_1: '\ud83d\udcdc Les donn\u00e9es d\'entra\u00eenement proviennent souvent d\'internet. Les artistes d\u00e9battent : usage \u00e9quitable ou copie ?',
        youth_2: '\ud83c\udfdb\ufe0f Le r\u00e8glement IA de l\'UE exige la transparence : d\'o\u00f9 viennent les donn\u00e9es d\'entra\u00eenement ?',
        youth_3: '\ud83d\udcad Question : \u00e0 qui appartient r\u00e9ellement une image g\u00e9n\u00e9r\u00e9e par IA ?',
        expert_1: '\u26a0\ufe0f LAION-5B a \u00e9t\u00e9 partiellement cr\u00e9\u00e9 sans le consentement des cr\u00e9ateurs \u2014 zone grise juridique.',
        expert_2: '\ud83d\udccb R\u00e8glement IA de l\'UE Art. 52 : obligation d\'\u00e9tiquetage pour le contenu g\u00e9n\u00e9r\u00e9 par IA',
        expert_3: '\ud83d\udd0d Model Cards & Datasheets : bonne pratique pour la transparence ML'
      },
      environment: {
        kids_1: '\u2601\ufe0f Chaque image IA produit un peu de CO\u2082 \u2014 comme conduire une voiture, mais moins !',
        kids_2: '\ud83c\udf31 R\u00e9fl\u00e9chis : cette image vaut-elle l\'\u00e9lectricit\u00e9 ?',
        kids_3: '\u2600\ufe0f L\'\u00e9nergie pour l\'IA vient souvent de centrales \u00e9lectriques \u2014 certaines propres, d\'autres non.',
        youth_1: '\ud83c\udfed Mix \u00e9lectrique allemand : ~400g CO\u2082 par kWh \u2014 \u00e7a s\'accumule !',
        youth_2: '\ud83d\udcc8 {co2}g CO\u2082 pour cette image \u2014 avec 1000 images, cela ferait {totalKg} kg !',
        youth_3: '\ud83d\udca1 Conseil : g\u00e9n\u00e9rez moins d\'images, mais de mani\u00e8re plus r\u00e9fl\u00e9chie \u2014 \u00e9conomise de l\'\u00e9nergie et du CO\u2082.',
        expert_1: '\ud83d\udcca Calcul : {watts}W \u00d7 {seconds}s \u00f7 3600 \u00d7 400g/kWh = {co2}g CO\u2082',
        expert_2: '\ud83d\udd2c \u00c9missions Scope 2 : la localisation du centre de donn\u00e9es est d\u00e9cisive',
        expert_3: '\u26a1 PUE (Power Usage Effectiveness) : surco\u00fbt \u00e9nerg\u00e9tique suppl\u00e9mentaire pour le refroidissement'
      },
      iceberg: {
        drawPrompt: 'La g\u00e9n\u00e9ration IA utilise beaucoup d\'\u00e9nergie. Dessinez des icebergs et voyez ce qui se passe...',
        redraw: 'Redessiner',
        startMelting: 'Commencer la fonte',
        melting: 'L\'iceberg fond...',
        melted: 'Fondu !',
        meltedMessage: '{co2}g CO\u2082 produits',
        comparison: 'Cette quantit\u00e9 de CO\u2082 fait fondre environ {volume} cm\u00b3 de glace arctique.',
        comparisonInfo: '(Chaque tonne de CO\u2082 = environ 6m\u00b3 de glace de mer perdue)',
        gpuPower: 'Consommation \u00e9lectrique de la carte graphique',
        gpuTemp: 'Temp\u00e9rature de la carte graphique',
        co2Info: '\u00c9missions de CO\u2082 li\u00e9es \u00e0 la consommation \u00e9lectrique (bas\u00e9 sur le mix \u00e9nerg\u00e9tique allemand)',
        drawAgain: 'Dessiner plus d\'icebergs...'
      },
      pixel: {
        grafikkarte: 'Carte graphique',
        energieverbrauch: 'Consommation d\'\u00e9nergie',
        co2Menge: 'Quantit\u00e9 de CO\u2082',
        smartphoneComparison: 'Il faudrait \u00e9teindre votre t\u00e9l\u00e9phone pendant {minutes} minutes pour compenser cette consommation de CO\u2082 !',
        clickToProcess: 'Cliquez sur les pixels de donn\u00e9es pour g\u00e9n\u00e9rer une mini image !'
      },
      forest: {
        trees: 'Arbres',
        clickToPlant: 'Cliquez pour planter des arbres ! L\u00e0 o\u00f9 vous plantez un arbre, l\'usine dispara\u00eetra.',
        gameOver: 'La for\u00eat est perdue !',
        treesPlanted: 'Vous avez plant\u00e9 {count} arbres.',
        complete: 'G\u00e9n\u00e9ration termin\u00e9e',
        comparison: 'Un arbre moyen a besoin de {minutes} minutes pour absorber cette quantit\u00e9 de CO\u2082.'
      },
      rareearth: {
        clickToClean: 'Cliquez sur le lac pour retirer les boues toxiques !',
        sludgeRemoved: 'Boues retir\u00e9es',
        environmentHealth: 'Environnement',
        gameOverInactive: 'Vous avez abandonn\u00e9... l\'extraction continue',
        infoBanner: 'L\'extraction de terres rares pour les puces GPU laisse des boues toxiques et d\u00e9truit les \u00e9cosyst\u00e8mes. Vos efforts de nettoyage ne peuvent pas suivre la vitesse d\'extraction.',
        instructionsCooldown: '\u23f3 {seconds} s',
        statsGpu: 'GPU',
        statsHealth: 'Environnement',
        statsSludge: 'Boues retir\u00e9es'
      }
    }
  }
}

// Cast locale to SupportedLanguage so vue-i18n accepts all supported languages
// (ko/tr messages are intentionally partial — fallbackLocale handles gaps)
export default createI18n({
  legacy: false,
  locale: 'de' as SupportedLanguage,
  fallbackLocale: 'en',
  messages
})
