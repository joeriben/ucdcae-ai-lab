{
  "pipeline": "single_text_media_generation",
  "name": {
    "en": "Stable Diffusion 3.5 Large",
    "de": "Stable Diffusion 3.5 Large"
  },
  "description": {
    "en": "High-quality image generation using Stable Diffusion 3.5 Large with Dual CLIP text encoding",
    "de": "Hochwertige Bildgenerierung mit Stable Diffusion 3.5 Large und Dual CLIP Text-Encoding"
  },
  "category": {
    "en": "Image Generation",
    "de": "Bildgenerierung"
  },

  "parameters": {
    "OUTPUT_CHUNK": "output_image_sd35_large",
    "NEGATIVE_PROMPT": "blurry, bad quality, watermark, text, distorted, malformed, disfigured, low resolution",
    "WIDTH": 1024,
    "HEIGHT": 1024,
    "STEPS": 25,
    "CFG": 5.5,
    "SAMPLER": "euler",
    "SCHEDULER": "normal",
    "seed": "random",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 512
  },

  "media_preferences": {
    "default_output": "image",
    "supported_types": ["image"]
  },

  "meta": {
    "stage": "output",
    "model": "DEFAULT",
    "model_file": "sd3.5_large.safetensors",
    "backend": "comfyui",
    "backend_type": "comfyui",
    "clip_models": [
      "clip_g.safetensors",
      "t5xxl_enconly.safetensors"
    ],
    "requires_gpu": true,
    "gpu_vram_mb": 8000,
    "estimated_duration_seconds": "20-60",
    "recommended_resolution": "1024x1024",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280",
      "1024x768",
      "768x1024"
    ],
    "notes": "SD3.5 Large uses Triple CLIP (clip_l + clip_g + t5xxl). Optimization generates CLIP-L/G only; T5-XXL receives the user's interception text (translated by Stage 3 at kids level, otherwise user must translate manually).",
    "display_name": "Stable Diffusion 3.5 Large",
    "icon": "\uD83C\uDFA8",
    "publisher": "Stability AI (London, UK)",
    "architecture": "Rectified Flow + MMDiT (triple text encoder)",
    "params": "~8.1B (MMDiT) + ~4.7B T5-XXL + ~0.4B CLIP",
    "text_encoders": ["CLIP-L (124M)", "OpenCLIP-G (354M)", "T5-XXL (4.7B)"],
    "quantization": "FP16",
    "license": "Stability Community License (<$1M)",
    "fair_culture": "Training data undisclosed (\"synthetic + filtered public\")",
    "safety_by_design": "Filtered training data, structured red-teaming",
    "optimization_instruction": "You are a prompt engineer for Stable Diffusion 3.5. Generate TWO CLIP-specific prompts from the Input. T5-XXL receives the user's original text separately â€” do NOT generate a T5 prompt.\n\nOutput ONLY valid JSON with exactly these keys:\n{\"clip_l\": \"...\", \"clip_g\": \"...\"}\n\nNO markdown, NO code fences, NO explanation, NO meta-commentary.\n\n=== CORE TRANSFORMATION: SCENE TO 2D IMAGE ===\nTransform scenic/narrative descriptions into a FLAT 2D IMAGE specification.\nKEY: A 2D image has no story, no before/after - only WHAT IS VISIBLE in ONE FROZEN FRAME.\n\nTRANSFORMATION RULES:\n1. Remove temporal language (before, after, then, suddenly, while)\n2. Remove causality (because, therefore, which causes)\n3. Convert actions to static poses (running -> runner mid-stride)\n4. Convert atmosphere to visible elements (mysterious -> fog, dim lighting, shadow patterns)\n\n=== CLIP-L (clip_l) â€” Visual Keywords ===\nMax 50 words. Comma-separated token-weighted visual keywords.\nFirst 10 tokens = SUBJECT. Next 15 = POSE/STATE + STYLE. Next 25 = CONTEXT + ATTRIBUTES. Last = TECHNICAL (lighting, camera).\nUse (keyword:1.2) weighting syntax for emphasis. ((keyword)) = ~1.21x.\nFocus on composition, lighting, texture, color. Concrete nouns only.\n\n=== CLIP-G (clip_g) â€” Semantic Scene ===\nMax 50 words. Broader semantic scene description. NO (keyword:weight) syntax (OpenCLIP-G is trained differently).\nDescribe atmosphere, cultural context, setting, mood, spatial relationships.\nPlain natural language, comma-separated phrases.\n\n=== CULTURAL RESPECT (BOTH PROMPTS) ===\nFORBIDDEN: \"style of [artist]\", art-historical technique names, exoticizing/romanticizing cultural practices.\nFORBIDDEN: Orientalist tropes (exotic, mysterious, timeless, ancient wisdom).\nUse culturally neutral visual descriptors: light, texture, space, color.\n\n=== EXAMPLE ===\nINPUT: \"A mysterious forest where ancient trees whisper secrets to travelers at twilight\"\nOUTPUT:\n{\"clip_l\": \"ancient forest, (massive gnarled trees:1.2), twilight sky, orange-purple light through canopy, solitary figure on winding path, mist between trees, (dramatic backlighting:1.3), deep shadows, atmospheric perspective\", \"clip_g\": \"dense old-growth forest at twilight, towering ancient trees with twisted branches, a lone traveler on a narrow winding path, mist rising between the trunks, warm orange and purple light filtering through the canopy, deep natural shadows\"}",
    "version": "1.0"
  },

  "display": {
    "icon": "ðŸŽ¨",
    "color": "#FF6B6B",
    "category": "image_generation",
    "difficulty": 2,
    "order": 10
  },

  "tags": {
    "en": ["image", "stable-diffusion", "high-quality", "sd3.5", "comfyui"],
    "de": ["bild", "stable-diffusion", "hohe-qualitÃ¤t", "sd3.5", "comfyui"]
  },

  "audience": {
    "workshop_suitable": true,
    "min_age": 10,
    "complexity": "beginner"
  }
}
