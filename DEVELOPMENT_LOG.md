# Development Log

## Session 208 - Trans-Aktion: Real Poetry as Collision Material
**Date:** 2026-02-24
**Focus:** Replace AI-generated collision materials with genuine poetry

### Background

Session 207.5 developed the Trans-Aktion concept: **Material-Kollision** via model insufficiency. A deliberately tiny LLM (qwen3:1.7b) receives two semantically alien texts — the user's prompt and a "collision material" — and attempts to fuse them. The model is large enough to understand the task but too small for a smooth synthesis. The structural failure produces genuine contingency.

The PoC succeeded (commit `447ea3a`): a prompt "Ein Waldspaziergang am Sonntagmorgen" collided with crystallography text yielded a Baumquerschnitt that was simultaneously wood and crystal.

### Problem: AI-Generated Collision Material = Anthropomorphized Compliance

The initial collision materials (crystallography, surgical anatomy, astronomy, bureaucracy, culinary) were AI-generated. This contradicts the core concept: LLM-generated "foreign" text is itself shaped by RLHF compliance patterns — it's "anthropomorphized sameness," not genuine alterity. Real collision requires material with its own materiality: rhythm, sound, history, cultural weight.

### Solution: Public-Domain Poetry

Replaced all 5 configs with genuine poems (all public domain, all age-appropriate 8-17):

| Config | Poet | Poem | Year | Character |
|--------|------|------|------|-----------|
| `trans_aktion_rilke.json` | Rilke | Der Panther | 1902 | Dense, spatial, bodily |
| `trans_aktion_hoelderlin.json` | Hoelderlin | Haelfte des Lebens | 1805 | Extreme fullness/emptiness contrast |
| `trans_aktion_basho.json` | Basho | 3 Haiku | 17th c. | Maximum compression, JP+DE bilingual |
| `trans_aktion_dickinson.json` | Dickinson | A Bird came down the Walk | ~1862 | Precise observation → uncanny beauty |
| `trans_aktion_whitman.json` | Whitman | Song of Myself §31 | 1855 | Expansive, cataloguing, bodily |

### Technical Details

- Renamed `trans_aktion.json` → `trans_aktion_rilke.json` (Rilke replaces crystallography)
- Deleted 4 AI-slop configs (chirurgisch, astronomisch, buerokratisch, kulinarisch)
- Created 4 new poet-named configs (hoelderlin, basho, dickinson, whitman)
- All configs: identical structure, `model_override: "qwen3:1.7b"`, `temperature: 0.95`, `max_tokens: 400`
- Context instruction pattern: poem text + "Write a single text where the poem above and the prompt below become inseparable"
- Basho config uses Japanese originals alongside German translations (bilingual collision)
- Dickinson/Whitman: English originals in both `en` and `de` context (the poem IS the material, not a translation target)

### Key Insight

The three contingency sources identified in the Trans-Aktion concept:
1. **Model insufficiency** (qwen3:1.7b can't smoothly synthesize) ✓ implemented
2. **Domain mismatch** (poem ↔ prompt semantic distance) ✓ now with genuine material
3. **Vector operations** (Surrealizer-style latent space manipulation) → future work

### Files Changed
| File | Changes |
|------|---------|
| `devserver/schemas/configs/interception/trans_aktion_rilke.json` | NEW (renamed from trans_aktion.json, Rilke replaces crystallography) |
| `devserver/schemas/configs/interception/trans_aktion_hoelderlin.json` | NEW |
| `devserver/schemas/configs/interception/trans_aktion_basho.json` | NEW |
| `devserver/schemas/configs/interception/trans_aktion_dickinson.json` | NEW |
| `devserver/schemas/configs/interception/trans_aktion_whitman.json` | NEW |
| `devserver/schemas/configs/interception/trans_aktion_chirurgisch.json` | DELETED |
| `devserver/schemas/configs/interception/trans_aktion_astronomisch.json` | DELETED |
| `devserver/schemas/configs/interception/trans_aktion_buerokratisch.json` | DELETED |
| `devserver/schemas/configs/interception/trans_aktion_kulinarisch.json` | DELETED |
| `public/ai4artsed-frontend/src/i18n/WORK_ORDERS.md` | Added WO for tr/ko/uk/fr/es/he/ar translations |

### Research: Why the Collision Failed (Rilke Test)

First test with real poetry (Rilke + "Waldspaziergang mit Hund und Kind") showed: model echoes poem verbatim, then writes a separate conventional text. No fusion. Systematic literature review revealed multiple converging causes:

**1. Qwen3 Thinking Mode ON by default** — Both transformers (`apply_chat_template`) and Ollama generate `<think>` blocks, consuming token budget on garbled reasoning before output. `llm_inference_backend.py` extracts them post-hoc but the tokens are already spent. Fix: `/no_think` in system prompt or suppress `<think>` token generation.

**2. No repetition penalty** — `repetition_penalty: 1.5` in transformers `gen_kwargs` (Qwen3's own recommendation). Currently not passed through `llm_inference_backend.py` at all. Single highest-impact parameter.

**3. Induction Head Toxicity** (ACL 2025, arXiv 2504.14218) — Attention heads copy patterns from context. RLHF models have prior toward preserving user input verbatim. Formatted poem = immutable block.

**4. CS4 Constraint Degradation** (arXiv 2410.04197) — Small models satisfy easiest constraints, drop hard ones. "Include poem" (easy: copy) + "include prompt" (easy: write about) + "make inseparable" (hard: dropped).

**5. LLM-Prompted Fusion is fundamentally wrong** — EBR cut-up study (2024): even GPT-4o inserts "thematic materials." CHI 2024: LLMs produce "convergent, mid-novelty outputs." Research consistently identifies prompted fusion as lowest-genuineness approach.

**Solution: Three-layer collision architecture:**
1. Mechanical (no model): N+7, sentence-interleaving, SpaCy-based
2. Insufficient model (qwen3:1.7b, `/no_think`, `presence_penalty: 1.5`): partial-failure art
3. Embedding (future): SLERP between T5 encodings → diffusion model conditioned on mathematical phantom

Full analysis with 20+ sources documented in `docs/DEVELOPMENT_DECISIONS.md` under "Trans-Aktion: Forschungsstand und Konsequenzen".

### Next Steps
1. Add `repetition_penalty` support to `llm_inference_backend.py` gen_kwargs + `llm_client.py` API (currently no penalty parameter at all)
2. Disable thinking for Trans-Aktion: `/no_think` in system prompt via config context, or suppress `<think>` token generation in transformers
3. Prompt restructure: fragment poem, use completion mode, concrete constraints — empirical testing
4. Mechanical pre-processing layer (SpaCy sentence-interleaving + N+7) as Python chunk
5. SLERP on T5 embeddings (connects to T5 SAE research, Session 192)
6. Test across SD3.5/FLUX/Wan2.1

---

## Session 207 - GPU Service VRAM/RAM Bugs: LLM Migration + Flux2
**Date:** 2026-02-24
**Focus:** Fix 4 critical VRAM/RAM management bugs in GPU Service

### Background
The LLM-Inference-Migration (commit `ac047ce`) introduced 3 bugs. Testing also revealed a 4th bug in DiffusersBackend that made Flux2 via Diffusers unusable. Previous sessions (2-3) had failed to get Flux2 working through the GPU Service.

### Bug 1: VRAM Overhead — `device_map="auto"` (LLM + Text Backend)
**Problem:** `device_map="auto"` in both `llm_inference_backend.py` and `text_backend.py` creates CPU-staging + GPU-residency simultaneously. Qwen3-4B consumed 36GB instead of ~8GB.

**Fix:** Removed `device_map="auto"`, added explicit `model.to(device)` after `from_pretrained()` — same pattern DiffusersBackend uses.

**Result:** Qwen3-4B: 7698 MB (was 36000 MB). Llama-Guard-3-1B: 2864 MB.

### Bug 2: Global `_load_lock` Blocking All Requests
**Problem:** A single `threading.Lock()` serialized ALL model loads. Loading safety model blocked translation, chat, interception — everything queued behind one download.

**Fix:** Per-model locks with double-checked locking pattern. `_model_locks` dict protected by lightweight `_model_locks_lock`. Fast path (already loaded) needs no lock at all.

**Result:** Qwen3-4B and Llama-Guard-3-1B load concurrently, no mutual blocking.

### Bug 3: Ollama Fallback for `qwen3:4b`
**Problem:** `qwen3:4b` is `LOCAL_DEFAULT_MODEL` but was missing from Ollama. GPU Service error → Ollama fallback → model not found → silent failure.

**Fix:** `ollama pull qwen3:4b` (one-time).

### Bug 4: Flux2 — The One That Failed 2-3 Sessions

**The core challenge:** Flux2-dev is ~106GB in float32 / ~24GB in bf16. With 96GB VRAM and 64GB RAM, loading strategy is critical. Previous sessions tried various approaches and failed.

**Phase 1 — Initial fixes (3 layered):**

1. **`enable_model_cpu_offload()` instead of `.to(device)`** — The original code did `pipe.to(self.device)` which tries to load the entire model onto GPU at once. For Flux2 at 106GB float32, this overflows VRAM into RAM+swap. `enable_model_cpu_offload()` keeps components in CPU RAM and moves them to GPU one-at-a-time during inference. Peak VRAM ~63GB during generation.

2. **`torch_dtype` vs `dtype` — library-specific behavior** — `transformers` migrated to `dtype`, but `diffusers` 0.36.0 is inconsistent. `Flux2Pipeline.from_pretrained()` **silently ignores** `dtype` ("not expected...will be ignored"). Solution: dynamic `dtype_key` — `"torch_dtype"` for Flux2, `"dtype"` for all other pipelines.

3. **Flux2Pipeline-specific generation kwargs:**
   - `negative_prompt` → unsupported, raises TypeError. Fix: skip for Flux/Flux2 pipelines.
   - `torch.Generator(device=self.device)` → crashes with CPU offload. Fix: `device="cpu"` when using `enable_model_cpu_offload()`.

**Phase 2 — The real breakthrough: component-level loading**

Phase 1 worked but was painfully slow: `Flux2Pipeline.from_pretrained()` silently ignores `torch_dtype` and loads ALL weights in float32 (~106GB into 64GB RAM + 70GB swap). The subsequent `pipe.to(bfloat16)` cast added minutes of CPU work. Total load time: several minutes with the system nearly unusable.

**Key discovery:** The safetensors files on disk are already bfloat16 (verified with `safe_open`). The float32 doubling happens entirely inside `from_pretrained` because Flux2Pipeline doesn't pass `torch_dtype` through to its subcomponents.

**Solution:** `_load_flux2_pipeline()` loads each component individually — `Flux2Transformer2DModel`, `AutoModelForImageTextToText` (Mistral3), `AutoencoderKLFlux2`, `PixtralProcessor`, `FlowMatchEulerDiscreteScheduler` — with explicit `torch_dtype=bfloat16`. These component classes DO respect the kwarg. Then assembles the pipeline manually via `Flux2Pipeline(transformer=..., text_encoder=..., ...)`.

**Result:**
- RAM: 1.2 GB (was 106 GB) — `low_cpu_mem_usage=True` memory-maps tensors from disk
- Load time: 4 seconds (was several minutes)
- Generation: 2:03 at 1024x1024 (20 steps, ~6s/step)
- Peak VRAM: ~63 GB during generation, released after

**Why previous sessions failed:** Each session hit a different layer of the problem — `.to(device)` OOM, `dtype` silently ignored, `negative_prompt` TypeError, generator device mismatch — but couldn't see all of them together. The component-level loading was the key insight that none of the previous sessions reached.

### Additional Fixes
- **Stale `'eco'` argument** in `schema_pipeline_routes.py:2383` — leftover from Session 205's `execution_mode` removal, caused TypeError in Stage 3 streaming path.
- **Flux2 config consolidation** — `flux2.json` now points to Diffusers chunk (primary) with ComfyUI fallback, matching SD3.5 pattern. Removed redundant `flux2_diffusers.json` and `flux2_fp8.json`.

### Files Changed
| File | Changes |
|------|---------|
| `gpu_service/services/llm_inference_backend.py` | Remove `device_map="auto"`, add `.to(device)`, per-model locks |
| `gpu_service/services/text_backend.py` | Remove `device_map="auto"`, add `.to(device)`, `torch_dtype`→`dtype` (transformers) |
| `gpu_service/services/diffusers_backend.py` | `_load_flux2_pipeline()` component-level bf16, dynamic `dtype_key`, skip `negative_prompt`, CPU generator |
| `devserver/my_app/routes/schema_pipeline_routes.py` | Remove stale `'eco'` arg from Stage 3 call |
| `devserver/schemas/configs/output/flux2.json` | Switch to Diffusers primary with ComfyUI fallback |
| `devserver/schemas/configs/output/flux2_diffusers.json` | Deleted (redundant) |
| `devserver/schemas/configs/output/flux2_fp8.json` | Deleted (redundant) |

### Commits
- `c447791` — `fix(gpu-service): VRAM/RAM management bugs in LLM + Diffusers backends`
- `e73c08c` — `fix(diffusers): Flux2Pipeline negative_prompt + generator device`
- `0c9affb` — `docs: Add Session 207 devlog`
- `42c6caf` — `refactor(flux2): Diffusers primary with ComfyUI fallback (like SD3.5)`
- `f5c0181` — `fix: remove leftover 'eco' execution_mode arg from Stage 3 call`
- `39779d2` — `fix(diffusers): use dtype for modern pipelines, torch_dtype for Flux2`
- `f790cb3` — `fix(diffusers): Flux2 bf16 cast before CPU offload`
- `4e987fd` — `perf(diffusers): Flux2 component-level bf16 loading (1.2GB RAM, 4s)`

### Key Lessons (for future sessions)
- **`device_map="auto"` is not free** — it creates CPU mirror + GPU copy. For models that fit in VRAM, explicit `.to(device)` is always better.
- **`torch_dtype` vs `dtype` is library-specific** — transformers uses `dtype`, diffusers still uses `torch_dtype`. Flux2Pipeline silently ignores BOTH. Always verify with `safetensors.safe_open()` and `psutil` whether kwargs are actually being applied.
- **`Flux2Pipeline.from_pretrained` is broken for dtype** — it doesn't pass `torch_dtype` to subcomponents. Load components individually with explicit dtype, then assemble. This is likely a diffusers bug that will be fixed eventually.
- **`low_cpu_mem_usage=True` on components = memory-mapped loading** — tensors reference disk directly, not copied to RAM. This is why 105GB of weights load in 1.2GB RAM.
- **`enable_model_cpu_offload()` has side effects** — generator must be on CPU, not GPU. `negative_prompt` not supported by Flux/Flux2.
- **Per-model locking > global locking** — for multi-model services, a single lock serializes everything. Double-checked locking with per-model granularity eliminates cross-model contention.

---

## Session 206 - Complete Hebrew and Arabic i18n Translations
**Date:** 2026-02-24
**Focus:** Full translation of all ~1370 i18n keys into Hebrew (he) and Arabic (ar)

### Background
Hebrew and Arabic infrastructure (stub files, index.ts imports, SUPPORTED_LANGUAGES with `dir: 'rtl'`) was added in Session 201. Both `.ts` files were empty stubs falling back to English. This session populates them with complete translations.

### Changes

**Hebrew (`he.ts`)** — 1369 lines added
- Single-pass translation by i18n-translator agent
- All 30 top-level sections translated
- `vue-tsc --build` and `npm run build-only` passed

**Arabic (`ar.ts`)** — 1367 lines added
- Initial single-agent attempt hit 32K output token limit
- Successfully completed via 3-way parallel split (parts 1-3), then assembled
- All sections translated in Modern Standard Arabic (MSA)
- Western numerals (0-9) used throughout
- `vue-tsc --build` and `npm run build-only` passed

**WORK_ORDERS.md** — Both work orders moved from Pending to Completed

### Translation Quality Notes
- All template variables preserved (`{count}`, `{watts}`, `{co2}`, etc.)
- All Unicode escapes preserved (`\u2014`, `\u2212`, `\u00d7`, etc.)
- `{'|'}` vue-i18n pipe escape syntax preserved
- All technical terms kept as-is (GPU, VRAM, CLIP, T5, CFG, LLM, MMDiT, etc.)
- Brand/model names preserved verbatim
- Emoji prefixes preserved in edutainment section

### Commits
- `fdf120c` — `chore(i18n): Add complete Hebrew (he) translation — all ~1370 keys`
- `7daedaa` — `chore(i18n): Add complete Arabic (ar) translation — all ~1370 keys`

### Remaining Pending Work Orders
- `WO-2026-02-23-hebrew-arabic-language-labels` — language label translations in de/tr/ko/uk/fr/es
- `WO-2026-02-23-hebrew-arabic-interception-configs` — he/ar entries in 36 interception JSON files
- `WO-2026-02-23-spanish-language-label` — Spanish language label in other language files

---

## Session 205 - Remove Deprecated execution_mode (eco/fast) Parameter
**Date:** 2026-02-23
**Focus:** Remove the deprecated execution_mode parameter that threaded through ~80 call sites across ~50 files but did nothing

### Background
`execution_mode` ("eco"/"fast") was deprecated in Session 55. Model selection moved to centralized `config.py` STAGE*_MODEL variables. The parameter persisted as dead code — always defaulted to "eco", and the only consumer (`lookup_output_config`) just did `defaults[media_type].get("eco")`. The "fast" mode was never sent by any client.

### Changes (50 files, -314/+65 lines)

**Backend — Core pipeline:**
- `output_config_defaults.json` — flattened from `{"image": {"eco": "sd35_large", "fast": "gpt5_image"}}` to `{"image": "sd35_large"}`
- `schema_pipeline_routes.py` — removed from ~40 sites (function signatures, data.get(), call sites, log messages)
- `stage_orchestrator.py` — removed from all 6 stage execution functions
- `pipeline_executor.py` — removed from execute_pipeline, stream_pipeline, all internal methods
- `output_config_selector.py` — removed from class, simplified to flat lookup
- `pipeline_recorder.py` — removed from constructor, get_recorder, load_recorder
- `execution_history/models.py` — removed field from ExecutionRecord
- `execution_history/tracker.py` — removed from constructor and finalize methods
- 5 route files (media, favorites, settings, pipeline, execution) — removed from response dicts

**Frontend:**
- `api.ts` — removed from PipelineExecuteRequest and TransformRequest interfaces
- `pipelineExecution.ts` — removed executionMode ref, setExecutionMode action
- `PipelineExecutionView.vue` — removed eco/fast/best selector UI
- `Phase2VectorFusionInterface.vue` — removed executionMode prop
- `text_transformation.vue`, `SessionExportView.vue`, `favorites.ts` — removed references
- All 7 i18n files (en, de, tr, ko, uk, fr, es) — removed `executionModes` keys

**Tests:** 19 test files cleaned (mechanical removal of `execution_mode='eco'` from payloads)

**Deleted:** `docs/archive/REFACTORING_PLAN_EXECUTION_MODE_REMOVAL.md` (now executed)

### Not Changed (intentionally)
- `backend_router.py` — chunk-level `execution_mode` ("legacy_workflow"/"standard") is a different concept, actively used for ComfyUI routing
- `model_selector.py` — used for model discovery, unrelated
- Chunk JSON files — `"execution_mode": "legacy_workflow"` fields are chunk routing, not request params

### Verification
- `npm run type-check` — zero errors
- All Python files parse clean (AST verified)
- Grep confirms zero remaining request-parameter `execution_mode` references in active code
- Backend smoke tests (all pass):
  1. `GET /api/schema/info` — 116 schemas loaded
  2. `GET /api/models/availability` — GPU service reachable
  3. `POST /api/schema/pipeline/safety` — `safe: true` (Stage 1 safety without execution_mode)
  4. `POST /api/schema/pipeline/stage2` — full Stage 1→2 pipeline (overdrive, 17s) — interception + optimization complete, zero parameter errors

---

## Session 204 - TODO List Cleanup (~2100 → ~220 Lines)
**Date:** 2026-02-23
**Focus:** Purge obsolete items from devserver_todos.md, update statuses from DevLog

### Problem
`docs/devserver_todos.md` had grown to ~2100 lines over 200+ sessions. Most content was completed work from Sessions 14-135 (Nov-Dec 2025), already documented in DEVELOPMENT_LOG.md. Several items from Sessions 36-98 were obsolete due to architectural changes but never removed.

### Changes

**Removed (completed, documented in DevLog):**
- 15+ completed features: Execution History Tracker, Unified Media Storage, LivePipelineRecorder, v2.0.0-alpha.1 release, Pipeline Rename, GPT-OSS Stage 1/3, keep_alive, Model Availability Check, Prompt Optimization Fix, Crossmodal Lab install, etc.
- Redundant Quick Reference / Architecture Status section (duplicates CLAUDE.md)

**Removed (obsolete — architecture changed since):**
- Stage 1-3 Translation Refactoring (Session 59) — pipeline architecture fundamentally different now
- Youth Mode & Pipeline Visualization (Session 52) — UI rebuilt from scratch
- Phase 2 Frontend redesign (Session 36) — rebuilt multiple times since
- Automate configsByCategory (Session 91) — frontend rebuilt
- Vector Fusion UI (Session 40) — deactivated per user decision
- Canvas Eval Nodes Phase 3b / Loop Controller Phase 4 (Session 134) — contradictory status (title says DONE, body says BLOCKED)
- partial_elimination 2 Open Issues (Session 98) — 3 months without mention

**Status updates:**
- LLM Inference Migration → ✅ DONE (Session 202)
- T5 Interpretability Research → ✅ CODE COMPLETE (Session 203)
- Stage 3 Negative Prompts Bug → confirmed still open (verified in code: `safety_result['negative_prompt']` never passed to Stage 4)

**New structure:** Recently completed → Critical → High → Medium → Low/Deferred → Minigames → Archive

### Files Changed
- `docs/devserver_todos.md` — rewritten (~2100 → ~220 lines)

---

## Session 203 - T5 Interpretability Research: Full 7-Phase Pipeline Implementation
**Date:** 2026-02-23
**Focus:** Implement SAE-based T5 audio-semantic interpretability research pipeline

### Context

Approved research plan (Session 192): Train a Sparse Autoencoder on T5-Base activations from ~101K audio prompts, decompose 768d into 12,288 monosemantic features, then sonify them through Stable Audio. No existing work sonifies SAE features through an audio diffusion model — SAEdit does it for images, this is the audio equivalent.

### Implementation (10 files, 4,078 lines)

**`research/t5_interpretability/`** — standalone scripts, each phase runnable independently:

| File | Phase | Purpose |
|------|-------|---------|
| `config.py` | — | Shared paths, hyperparams (SAE 768→12288, k=64), dataset IDs |
| `probing_specs.py` | — | 15 TraditionSpec dataclasses (full vocab), Pillar 2 material-physical vocab, controls |
| `build_corpus.py` | 1a | Download AudioCaps/MusicCaps/WavCaps (~95K), deduplicate |
| `build_probing_corpus.py` | 1b | Template-based generation: 15×250 traditions + 2000 physical + 500 controls |
| `encode_corpus.py` | 2 | Standalone T5-Base, batch 64, mean-pool → [N, 768] fp16 |
| `dimension_atlas.py` | 3 | Per-dim stats, 768×768 correlation clustering (Ward), probing analysis |
| `train_sae.py` | 4 | TopK SAE: pre-bias, encoder, unit-norm decoder, MSE loss, decoder renorm |
| `analyze_features.py` | 5 | Top-20 prompts, Pearson r per category, cultural bias, co-activation clustering |
| `sonify_features.py` | 6 | Feature direction injection into T5 embedding, GPU service HTTP generation |
| `cultural_analysis.py` | 7 | 15×15 cosine distance matrix, default-encoding bias, permutation test (p-value) |

**GPU Service modification:**
- `gpu_service/routes/stable_audio_routes.py`: Added `POST /api/stable_audio/generate_from_embeddings` — accepts base64-encoded numpy arrays, calls existing `backend.generate_from_embeddings()`. ~40 lines.

### Probing Corpus Design (pedagogical core)

**Symmetrical by design** — no tradition framed as default:
- 15 traditions × 5 templates × 50 prompts = 3,750 (Pillar 1)
- Each tradition: ~20 instruments, ~10 vocal styles, ~10 spatial/ensemble contexts, 50 blind descriptors
- Blind descriptors: acoustic descriptions WITHOUT naming the tradition (tests whether T5 maps similar sounds to same features regardless of cultural label)
- Pillar 2: 6 excitation types, 10 materials, 8 environments, 6 time patterns, 4 spectral axes, dynamics

### Key Design Decisions

1. **T5-Base standalone** (not via Stable Audio pipeline) — `text_projection = nn.Identity()` at `stable_audio_backend.py:423`, so hidden states are identical. Avoids loading the full diffusion model for encoding.
2. **TopK SAE** (not L1-penalized) — sparsity by construction (k=64), no hyperparameter tuning for L1 coefficient.
3. **Mean-pooling** over non-padding tokens — sentence-level representation, not per-token.
4. **Feature direction injection** — add SAE decoder column (768d unit vector) uniformly across all sequence positions of neutral embedding. Strength parameter controls intensity.
5. **Permutation test** for cultural significance — 1000 random label shuffles, compare observed mean pairwise distance.

### Status

Code complete, all files syntax-checked. Erstdurchlauf steht aus (Testanleitung in `docs/devserver_todos.md`).

---

## Session 202 - LLM Inference Migration: Ollama → GPU Service
**Date:** 2026-02-23
**Focus:** Route all LLM inference through GPU Service's VRAMCoordinator with Ollama fallback

### Problem
3 separate inference backends (Ollama/GGUF, GPU Service/safetensors, SwarmUI/ComfyUI) competed for the same GPU with no VRAM coordination. Ollama and GPU Service ran blind — loading a safety model via Ollama could evict a Diffusers pipeline from VRAM without the VRAMCoordinator knowing.

### Solution — 6 Phases

**Phase 1: GPU Service Infrastructure (3 new files)**
- `llm_inference_backend.py`: Multi-model VRAMBackend (mirrors TextBackend pattern). Auto-detects vision vs text models, extracts `<think>` blocks centrally, resolves Ollama model names via `LLM_MODEL_MAP`. NO `output_hidden_states`/`output_attentions` (pure inference, saves VRAM).
- `llm_inference_routes.py`: 5 REST endpoints (`/api/llm/chat`, `/generate`, `/available`, `/models`, `/unload`)
- Config: `LLM_INFERENCE_ENABLED`, `LLM_MODEL_MAP` (6 models), `LLM_QUANT_MULTIPLIERS`

**Phase 2: DevServer Client (2 new files)**
- `llm_client.py`: HTTP wrapper following DiffusersClient pattern. GPU Service primary, Ollama fallback on `ConnectionError`/`Timeout`. LLM errors (OOM, bad model) propagated, no fallback.
- `llm_backend.py`: Singleton factory (`get_llm_backend()`)

**Phase 3: Migrate Safety (stage_orchestrator.py)**
- 3 functions → `get_llm_backend().chat()`: `llm_verify_person_name`, `llm_dsgvo_fallback_check`, `llm_verify_age_filter_context`
- Fail-closed semantics preserved (None → block)
- Thinking-field fallback preserved

**Phase 4: Migrate Vision Models**
- `vlm_safety.py` → `get_llm_backend().chat()` with `images` param. Fail-open preserved.
- `image_analysis.py` → `get_llm_backend().chat()` with `images` param. Fixed hardcoded `localhost:11434` URL bug.

**Phase 5: Migrate Interception + Chat**
- `prompt_interception_engine.py::_call_ollama()` → `get_llm_backend().generate()`. Removed manual `keep_alive: 0` unload logic (VRAMCoordinator handles lifecycle).
- `chat_routes.py::_call_ollama_chat()` → `get_llm_backend().chat()`
- `schema_pipeline_routes.py`: 2× `ollama_service.translate_text()` → `get_llm_backend().generate()`

**Phase 6: Training Service Cleanup**
- `clear_vram_thoroughly()`: Added Step 4b — unload GPU Service LLM models alongside existing Ollama unload

### Architecture After

```
DevServer ──→ LLMClient ──→ GPU Service :17803  (safetensors, VRAMCoordinator)
                         ╰→ Ollama :11434       (GGUF, fallback on ConnectionError)
```

### NOT in scope (future)
- `settings_routes.py` `/api/tags` discovery — read-only, stays Ollama
- `model_selector.py` `/api/tags` discovery — read-only, stays Ollama
- `ollama_service.py` streaming (`translate_text_stream`) — legacy workflow
- Model list merging (GPU Service + Ollama available models)

### Files Changed
- 4 new files, 9 modified files (189 insertions, 167 deletions)

---

## Session 201 - Hebrew (he) + Arabic (ar) RTL Language Support
**Date:** 2026-02-23
**Focus:** Add RTL infrastructure for Hebrew and Arabic, CSS logical properties, LTR-pinned components

### Problem
The i18n system supported 7 LTR languages but had zero RTL support. Simply adding `he.ts`/`ar.ts` without `dir="rtl"` would render translated text left-to-right — unreadable for native speakers. With `dir="rtl"`, hardcoded `margin-left`/`border-left`/`text-align: left` CSS would partially break.

### Solution — 4 Phases

**Phase 1: RTL Infrastructure (3 files + 2 new)**
- `SUPPORTED_LANGUAGES` gains `dir: 'ltr' | 'rtl'` property per language
- `getLanguageDir()` helper exported from `index.ts`
- `main.ts` dynamically sets `<html dir>` and `<html lang>` on language change
- `index.html` defaults to `lang="de" dir="ltr"`
- Empty `he.ts`/`ar.ts` stubs (fallback to English)

**Phase 2: Pin LTR + CSS Fixes (29 files)**
- 23 visual/technical components pinned with `dir="ltr"` (canvas, pipeline, edutainment) — immune to RTL flip
- 6 text-heavy components converted to CSS logical properties: `margin-inline-start/end`, `border-inline-start`, `text-align: start`, `inset-inline-end` (SettingsView, App.vue, DokumentationModal, ChatOverlay, MediaInputBox, WelcomeItem)

**Phase 3: Language Files**
- `en.ts`: Added `arabicAr`/`hebrewHe` keys
- SettingsView: he/ar in language dropdown
- 3 work orders in `WORK_ORDERS.md` for batch translator

**Phase 4: Verification**
- `vue-tsc --noEmit` passes, `npm run build` passes

### Design Decisions
- **Full layout flip, not just text**: RTL users expect flex/grid to flip. Browser handles this natively via `dir="rtl"` on `<html>` — no custom code needed.
- **CSS logical properties**: `margin-inline-start` behaves identically to `margin-left` in LTR — zero regression risk for existing 7 languages.
- **LTR pins**: Canvas, pipeline arrows, edutainment games are universally left-to-right (data flow direction, not text direction).
- **Translations deferred**: Infrastructure first, translations via batch workflow second. English fallback keeps UI functional.

### Files Changed
36 files (34 modified, 2 new) — commit `d12107b`

---

## Session 199 - i18n Split + Batch Translation Workflow
**Date:** 2026-02-23
**Focus:** Split 8275-line i18n.ts monolith into per-language files, establish batch translation workflow

### Problem
The monolithic `i18n.ts` (568KB, 8275 lines, 6 languages) consumed massive context every UI session. Writing translations for 5 non-English languages was manual and repetitive.

### Solution
1. **Split into `src/i18n/` directory**: `index.ts` barrel + `{en,de,tr,ko,uk,fr}.ts` per-language files (~1370 lines each)
2. **Batch workflow**: Sessions edit only `en.ts`, leave structured work orders in `WORK_ORDERS.md`
3. **`i18n-translator` agent**: Processes pending work orders, translates into 5 languages, type-checks, commits
4. **`6_run_i18n_translator.sh`**: Start script for standalone batch runs

### Implementation
- **`src/i18n/index.ts`**: Barrel export — imports all 6 languages, exports `SUPPORTED_LANGUAGES`, `SupportedLanguage`, `LocalizedString`, `localized()`, `createI18n`
- **`src/i18n/WORK_ORDERS.md`**: Structured template with `(NEW)`/`(MODIFIED)` tags and context fields
- **`.claude/agents/i18n-translator.md`**: Agent with apostrophe escaping, template variable preservation, validation script
- **CLAUDE.md rules**: Rule 9 (i18n workflow), Rule 10 (concurrent session awareness)
- **Import compatibility**: All 13+ consuming files use `@/i18n` or `./i18n` — directory `i18n/index.ts` resolves automatically, zero changes needed

### Impact
| Metric | Before | After |
|---|---|---|
| Context per UI session | ~8275 lines (568KB) | ~1374 lines (~95KB) |
| Translation effort per session | Manual for 5 languages | Zero — work order only |
| Adding a new language | Edit 8K-line monolith | Create one file + 1 import |

### Note
Commit c88f496 (concurrent session) had already created the 6 language files but left the monolith intact and didn't create `index.ts`. This session completed the migration.

---

## Session 194 - Forest MiniGame: Flowerpot Cursor with Seedling Growth
**Date:** 2026-02-22
**Focus:** Replace invisible cooldown indicator with custom flowerpot cursor

### Problem
The hourglass cooldown indicator (`⏳ 1.0s`) was inside the `showInstructions` block which disappears after 5 seconds. After that, the 1-second planting cooldown was invisible — clicks silently swallowed with no feedback.

### Solution
Custom SVG flowerpot cursor replaces the native pointer. During the 1s cooldown after planting, a seedling grows inside the pot (scale 0→1). When fully grown, click to plant. The cooldown is now always visible as a charming micro-animation.

### Implementation
**Modified:** `ForestMiniGame.vue`

- **Mouse tracking**: `mouseX`/`mouseY` refs, `showPotCursor` toggle on mouseenter/mouseleave
- **Seedling growth**: `seedlingGrowth = 1 - plantCooldown` — maps cooldown 1→0 to growth 0→1
- **SVG cursor**: Terracotta pot (trapezoid body + rim + soil) with seedling (stem + 2 leaves + bud) scaled from base point
- **CSS**: `cursor: none` on game area, `.pot-cursor` with `pointer-events: none`, `transform: translate(-50%, -100%)` anchoring bottom-center at mouse
- **Cleanup**: Removed `.plant-instruction.cooldown` and `:empty` styles, simplified instruction to show only before first tree planted

---

## Session 193 - Wavetable Synthesis as Third Playback Mode
**Date:** 2026-02-22
**Focus:** Add wavetable oscillator to Crossmodal Lab Synth tab

### Problem
The Latent Audio Synth generates audio via Stable Audio and plays it as a looping sample (`AudioBufferSourceNode.loop`). The WAV contains complex audio with many oscillations — crossfade at loop boundaries is always audible, and it doesn't behave like an oscillator.

### Solution: Wavetable Oscillator
Extract single-cycle frames from the WAV and use them as a wavetable oscillator (Serum/Vital paradigm). A phase-accumulator AudioWorklet reads through each frame at the desired frequency, and a scan position morphs between frames for timbral control.

Three mutually exclusive modes: **Loop | Ping-Pong | Wavetable**

### Implementation

**New files:**
- `src/audio/wavetable-processor.ts` — AudioWorkletProcessor with phase-accumulator synthesis, bilinear interpolation (between samples within a frame + between adjacent frames), 2048-sample frame size (~21.5 Hz fundamental at 44.1 kHz)
- `src/composables/useWavetableOsc.ts` — Composable managing AudioContext + worklet lifecycle, frame extraction (stereo→mono, pitch-synchronous via pitchy McLeod + Lanczos sinc resampling + Hann windowing, fallback to overlapping windowed extraction), frequency/scan/note control

**Modified files:**
- `useAudioLooper.ts` — Added `getOriginalBuffer()` getter (1 line)
- `crossmodal_lab.vue` — Segmented control replacing ping-pong checkbox, wavetable scan slider, conditional visibility (transpose mode/crossfade/save-loop hidden in wavetable mode), MIDI CC5→scan mapping, note handler branches on mode, frame re-extraction on regeneration
- `i18n.ts` — 7 keys (DE+EN): modeLoop, modePingPong, modeWavetable, wavetableScan, wavetableScanHint, wavetableFrames, midiScan

### Key Design Decisions
- **Phase accumulator in AudioWorklet**: Runs in audio thread — no main-thread jitter. Phase wraps to prevent float precision loss at high frequencies.
- **Bilinear interpolation**: Smooth both within-frame (sample interpolation) and between-frame (scan morphing) — no audible stepping.
- **Pitch-synchronous extraction** (user enhancement): Uses pitchy MPM for pitch detection, extracts exact periods at zero-crossings, Lanczos-resamples to FRAME_SIZE. Falls back to overlapping windowed extraction for noisy/unpitched audio.
- **Mode switching stops both engines**: Clean handoff — no orphaned AudioNodes.
- **Wavetable mode reuses looper for decode**: `looper.play()` still decodes base64 WAV, then `getOriginalBuffer()` provides the AudioBuffer for frame extraction. Looper is stopped after frames load.

### Commits
- `cbb9f96` feat(crossmodal-lab): Wavetable synthesis as third playback mode

---

## Session 192 - Latent Lab Research Data Export (LatentLabRecorder)
**Date:** 2026-02-22
**Focus:** Research data recording for all Latent Lab experiments

### Problem
Canvas records research data via `CanvasRecorder`/`LivePipelineRecorder` because it runs through the 4-Stage Orchestrator. The Latent Lab bypasses this flow — 11 tool types across 3 backend services (GPU Service, Ollama, DevServer) with direct API calls. No backend chokepoint exists for a recorder.

### Solution: Frontend-Primary Hybrid
Frontend knows the full context (parameters, tab, tool type). After each generation, it POSTs to a new lightweight backend endpoint which writes to disk in the same folder structure as Canvas (`exports/json/YYYY-MM-DD/device_id/run_xxx/`).

### Implementation

**Backend (2 new files):**
- `latent_lab_recorder.py` — Lightweight `LatentLabRecorder` class (no stage tracking, no expected_outputs)
- `latent_lab_recorder_routes.py` — 3 endpoints: `POST /api/latent-lab/record/{start,save,end}`

**Frontend (1 new file):**
- `useLatentLabRecorder.ts` — Vue composable with lazy lifecycle (run folder created on first `record()`, not on mount)

**Integration (7 views + 1 child):**
- `denoising_archaeology.vue` — params + final PNG + all step JPGs
- `concept_algebra.vue` — params + reference + result images
- `feature_probing.vue` — params + original/modified images (both analyze + transfer)
- `attention_cartography.vue` — params + output image (attention maps too large for export)
- `crossmodal_lab.vue` — 3 tabs: synth/mmaudio/guidance, params + audio WAV
- `latent_text_lab.vue` — 4 functions: findDirection/repGen/compare/biasProbe, params + metadata
- `surrealizer.vue` — params only (image already recorded by pipeline recorder)

**i18n:** 3 keys (`recordingActive`, `recordingCount`, `recordingTooltip`) across 6 languages

### Key Design Decision: Lazy Start
Initial implementation created a run folder on every `onMounted()` — navigating between tabs created empty folders. Fixed by deferring `startRun()` to the first actual `record()` call. `isRecording` indicator still shows immediately.

### metadata.json Format
```json
{
  "type": "latent_lab",
  "latent_lab_tool": "denoising_archaeology",
  "entities": [
    { "type": "latent_lab_params", "filename": "prompting_process/001_parameters.json" },
    { "type": "output_image", "filename": "01_output_image.png" }
  ]
}
```
Compatible with `SessionExportView.vue` — Latent Lab runs appear alongside Canvas/Pipeline runs.

### Commits
- `b8c7665` feat(latent-lab): Add research data export with LatentLabRecorder
- `83b6e7a` fix(latent-lab): Lazy-start recorder to avoid empty run folders

---

## Session 190 - Fix Age-Filter Fail-Open Bug + DSGVO Fallback
**Date:** 2026-02-21
**Focus:** Fix critical safety bug where age-filter context check used dead pipeline → fail-open, letting blocked content through

### Problem
"Ein gruseliger Vampir mit Blut" passed kids safety despite all terms (Vampir, Blut, gruselig) being in `youth_kids_safety_filters.json`. Root cause: `fast_filter_check()` found terms correctly, but the LLM context verification called `pipeline_executor.execute_pipeline('pre_interception/gpt_oss_safety', ...)` — a pipeline depending on a model no longer loaded. Pipeline failure → line 784: `continuing (fail-open)` → scary content passed through unblocked.

Same pattern existed in the DSGVO SpaCy-unavailable fallback (line 869-911): dead `gpt_oss_safety` pipeline + fail-open on failure.

### Fix
1. **Age-filter** (lines 769-867): Replaced 91-line dead pipeline call + BLOCKED:/SAFE: response parsing with direct call to `llm_verify_age_filter_context()` — a function already in the same file (line 276) using `DSGVO_VERIFY_MODEL` (qwen3:1.7b, always loaded). Fail-open → **fail-closed**.

2. **DSGVO fallback** (lines 869-911): Created new `llm_dsgvo_fallback_check()` function for when SpaCy is unavailable (can't use `llm_verify_person_name()` which needs NER entities). Same model, same SAFE/UNSAFE pattern. Fail-open → **fail-closed**.

### Key Decisions
- Both paths now use `DSGVO_VERIFY_MODEL` (qwen3:1.7b) — always loaded, 1.5 GB VRAM, 60s timeout
- Age-filter `llm_verify_age_filter_context()` has 60s result cache (prevents inconsistent re-checks)
- `llm_dsgvo_fallback_check()` asks LLM to *discover* names (vs `llm_verify_person_name()` which *verifies* SpaCy hits)
- Zero references to `gpt_oss_safety` pipeline remain in stage_orchestrator.py

### Test Results (12/12 pass)
- "Ein gruseliger Vampir mit Blut" → **BLOCKED** (kids, both /safety/quick and unified pipeline)
- "Ein freundlicher Hund im Park" → PASS
- "Angela Merkel im Garten" → **BLOCKED** (DSGVO)
- "Hakenkreuz auf rotem Hintergrund" → **BLOCKED** (§86a, instant)
- "Tod und Verderben überall" → **BLOCKED** (kids)
- "Ein bunter Schmetterling auf einer Blume" → PASS (9ms Stage 1 → full Stage 2)

### Changes
- `devserver/schemas/engine/stage_orchestrator.py` — 1 file, 148 insertions, 127 deletions (-51 net in age-filter, +68 new DSGVO fallback function)

---

## Session 189 - Fix Cross-Aesthetic Guided Audio Generation (ImageBind + StableAudio)
**Date:** 2026-02-21
**Focus:** Fix 4 critical bugs in `_guided_generate()` that caused the denoising loop to produce only noise/chaos

### Problem
The custom guided denoising loop in `stable_audio_backend.py` reimplemented the EDMDPMSolverMultistep scheduler incorrectly. ImageBind mel-spec encoding was already fixed (Session ~174), but the denoising math had 4 bugs making output completely broken regardless of guidance quality.

### Bugs Fixed
1. **Missing `init_noise_sigma` scaling** (~80x magnitude error): Pipeline multiplies initial noise by `(σ_max² + 1)^0.5 ≈ 80.006`, custom code skipped this entirely → latents started at wrong noise level
2. **Wrong `predicted_clean` formula**: Used `latents - sigma * noise_pred` (DDPM) instead of EDM preconditioning (`c_skip * sample + c_out * model_output` via `scheduler.precondition_outputs()`) → garbage decoded audio for ImageBind
3. **Manual Euler step instead of `scheduler.step()`**: First-order Euler on raw output vs DPMSolver++ with preconditioning + multi-step integration → wrong solver, numerical instability
4. **Scheduler state corruption**: `scheduler.step()` was only called during non-warmup → `step_index` never incremented during warmup → all subsequent steps used wrong sigma values + empty model_outputs buffer

### Architecture Change
Guidance now modifies latents **before** `scheduler.step()` (per Xing et al. CVPR 2024), and `scheduler.step()` runs on **every** iteration. No manual Euler needed.

### Reference
- Xing et al., "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners" (CVPR 2024)
- ImageBind (Girdhar et al., CVPR 2023) — joint embedding space
- Pipeline source: `diffusers/pipelines/stable_audio/pipeline_stable_audio.py:445,736`
- Scheduler source: `diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py:192-206,632-708`

### Changes
- `gpu_service/services/stable_audio_backend.py` — 1 file, 32 insertions, 22 deletions

---

## Session 188 - Session Export: Device-Filter statt User-Filter
**Date:** 2026-02-21
**Focus:** Replace non-functional user_id filter with device_id in Session Data Export (Forschungsdaten tab)

### Problem
The "User" filter in session export was useless — `user_id` is almost always "anonymous". Meanwhile, `device_id` (from the favorites/browser-ID system) is stored in every `metadata.json` and uniquely identifies devices.

### Changes
- **Backend** (`settings_routes.py`): Query param `user_id` → `device_id`, filter logic, session summary field, sortable fields, unique filter collection, response key `"devices"` instead of `"users"`
- **Frontend** (`SessionExportView.vue`): Stats card, filter dropdown (truncated to 8 chars), state/clearFilters, URL params, table header+row, detail modal, both PDF export functions (single + ZIP)

### Verification
- `npm run type-check` passes cleanly
- No remaining `user_id`/`unique_users`/`.users` references in either file
- 2 files changed, 30 insertions, 30 deletions

---

## Session 187 - i18n Infrastructure: Generic 3rd-Language Readiness
**Date:** 2026-02-20
**Focus:** Make i18n infrastructure extensible so adding ANY language requires only "add translations" — no code changes needed.

### Changes
- **`LocalizedString` type + `localized()` helper** (i18n.ts): Central type for all bilingual string objects, with English fallback resolution
- **Replaced hardcoded `'de' | 'en'` unions** with `SupportedLanguage` across 7 locations in api.ts, pipelineExecution.ts, configSelection.ts, ConfigTile.vue
- **Replaced `{ en: string; de: string }` interface fields** with `LocalizedString` across 8 locations in types/canvas.ts, 3 in api.ts, 3 in configSelection.ts
- **Converted ternary language lookups** (`locale === 'de' ? x.de : x.en`) to `localized(x, locale)` in ModulePalette.vue (2), ConfigSelectorModal.vue (4), CanvasWorkspace.vue (1), MusicTagSelector.vue (1)
- **Fixed hardcoded `loadMetaPromptForLanguage('de')`** in image_transformation.vue and multi_image_transformation.vue — now uses actual user locale
- **Backend fallback**: `build_safety_message()` in stage_orchestrator.py uses `.get(lang, ...['en'])` instead of direct key access — prevents KeyError on unknown languages
- **Removed dead keys**: `language.switch` / `language.switchTo` from both DE and EN i18n sections (unused anywhere)

### Scope discipline
- 13 files, 57 insertions, 46 deletions — all targeted
- NOT touched: doc components (255 ternaries, adult-facing), canvas/StageModule.vue (80 ternaries, advanced feature), backend Python code, safety-critical files
- `npm run type-check` passes cleanly

### Verification
Adding a 3rd language to `SUPPORTED_LANGUAGES` now requires only: (1) add translations to i18n.ts message object, (2) add language entry to `SUPPORTED_LANGUAGES` array. All type lookups, config name resolution, and backend safety messages automatically fall back to English.

### Turkish (tr) — First 3rd Language Added
- **1071 keys** fully translated (Sonnet agent, not Opus — translation doesn't need reasoning)
- `LocalizedString` type fixed: `Record<SupportedLanguage, string>` → `{ en: string; [key: string]: string }` — otherwise adding `'tr'` would require `tr` key in every existing object literal
- Additional type fixes: `ConfigContextResponse.context` → `Record<string, string>`, `NoMatchState.vue` prop → `SupportedLanguage`, `createI18n` locale cast
- Infrastructure proof: adding Turkish required **1 line** in SUPPORTED_LANGUAGES + translations. Zero code changes.
- KVKK (Turkish GDPR equivalent) used instead of DSGVO/GDPR in safety messages
- `npm run type-check` passes, `fallbackLocale: 'en'` handles any missing tr keys

## Session 185 - i18n Batch 6: Infrastructure Code + LLM Meta-Prompts
**Date:** 2026-02-20
**Focus:** Final i18n sweep — convert remaining German strings in infrastructure code; convert LLM meta-prompts to English for better instruction adherence

### Key Distinction Established
**Meta-prompts are NOT i18n.** LLM system prompts are internal steering instructions written in the language that works best for the model (English). They must not be conflated with localized user-facing content. Bilingual Stage2 configs (`{'de': ..., 'en': ...}`) are a separate category — that IS localized content.

All converted meta-prompts include language guards (`"Respond in the language of the input"`) to preserve output language adaptation.

### Group A: Infrastructure Code (8 files)
Docstrings, comments, log messages, fallback strings → English:
- `devserver/schemas/__init__.py`, `devserver/schemas/engine/__init__.py` — module docstrings
- `devserver/my_app/services/comfyui_client.py` — class docstring
- `devserver/my_app/services/streaming_response.py` — SSE status messages
- `devserver/schemas/engine/comfyui_workflow_generator.py` — all docstrings, comments, log messages
- `devserver/my_app/utils/workflow_node_injection.py` — all comments
- `devserver/my_app/routes/chat_routes.py` — fallback defaults (`unbekannt`→`unknown`, etc.)
- `devserver/my_app/services/wikipedia_service.py` — disambiguation error message

### Group B: LLM Meta-Prompts (3 files)
- `devserver/my_app/routes/text_routes.py` — `INTERPRETATION_SYSTEM_PROMPTS` (repeng, bias, compare) + `_build_interpretation_prompt()` labels
- `devserver/my_app/routes/canvas_routes.py` — comparison evaluator prompt
- `devserver/my_app/services/canvas_executor.py` — same comparison evaluator prompt

### Group C: Archive Files (gitignored)
Changed on disk but not tracked by git (`**archive/` in `.gitignore`). No commit impact.

### Verification
- `ast.parse()` on all 11 files → OK
- Träshy language guard already present (`chat_routes.py:43`, `:62`)

### Commits
- `e34c006` — `fix(i18n): Convert German strings in infrastructure code to English (Batch 6)`

---

## Session 183 - Tiered Translation: Auto for Kids, Optional for Youth+
**Date:** 2026-02-19
**Focus:** Decouple translation-for-safety from translation-for-generation in Stage 3

### Problem
`execute_stage3_safety()` always auto-translated prompts to English before generation, coupling two purposes: safety (llama-guard works better on English) and generation quality (models produce better results with English). This prevented youth+ users from exploring how models react to their native language.

### Changes
**Single function change** in `devserver/schemas/engine/stage_orchestrator.py`:

1. **Moved `research`/`adult` early-return before translation** — these levels now skip translation entirely and return the original prompt (previously wasted an LLM translation call)
2. **Tiered prompt selection after safety passes** — `kids` gets auto-translated English prompt, `youth` gets original-language prompt back (safety still checked on translated text internally)
3. **Fixed latent bug** — §86a block's `execution_time` referenced undefined `translate_start` on cache hit (replaced with `translate_time`)

### Verification
Tested all 4 safety levels with same German prompt:
- **Kids**: Translated → safety check → English prompt to model, `was_translated=True`, badge shows
- **Youth**: Translated internally → safety check → German prompt to model, `was_translated=False`, no badge
- **Adult**: No translation, no safety → German prompt to model
- **Research**: No translation, no safety → German prompt to model

### Affected Files
- `devserver/schemas/engine/stage_orchestrator.py` — `execute_stage3_safety()` restructured
- `docs/ARCHITECTURE PART 29 - Safety-System.md` — Updated Stage 3 flow description
- `docs/DEVELOPMENT_DECISIONS.md` — Decision documented

---

## Session 182 - Real Diffusers Progress for Edutainment Animations
**Date:** 2026-02-19
**Focus:** Replace faked time-based progress with real Diffusers step callbacks

### Problem

Edutainment animations (Forest, Iceberg, Pixel) showed a faked progress loop based on `estimatedSeconds` from output configs. The Diffusers backend already fires step callbacks (`step 5/25`) but this data had no path to the frontend.

### Architecture: Polling Side-Channel (3 layers)

```
Diffusers callback → _generation_progress dict → GET /api/diffusers/progress
  → DevServer proxy /api/settings/generation-progress → Frontend polls every 2s
```

No SSE changes. No threading complexity. Fail-open by design.

### Changes

**Layer 1: GPU Service** (`gpu_service/services/diffusers_backend.py`)
- Added module-level `_generation_progress` dict + `get_generation_progress()` getter
- Wired progress callbacks into all 5 generation methods: `generate_image()`, `generate_video()`, `generate_image_with_fusion()`, `generate_image_with_attention()`, `generate_image_with_archaeology()`
- Each sets `active=True` before inference, updates `step/total_steps` per callback, sets `active=False` in `finally`

**Layer 1b: GPU Service Route** (`gpu_service/routes/diffusers_routes.py`)
- Added `GET /api/diffusers/progress` endpoint

**Layer 2: DevServer Proxy** (`devserver/my_app/routes/settings_routes.py`)
- Added `GET /api/settings/generation-progress` with 2s timeout, fail-open to `{step:0, total_steps:0, active:false}`

**Layer 3: Frontend Composable** (`useAnimationProgress.ts`)
- Added `backendProgress` ref + `fetchGenerationProgress()` polled every 2s alongside GPU stats
- `animationLoop` uses real step progress when backend reports `active=true`
- `hasSeenBackendProgress` latch: once real data is seen, progress holds at last value when inference finishes (no reset during VLM safety check)
- Falls back to time-based loop for ComfyUI/HeartMuLa/GPU-service-down

### Behavior Matrix

| Backend | Progress behavior |
|---------|------------------|
| Diffusers | Real steps 0→100%, holds at 100% during safety check |
| ComfyUI | Time-based loop 0→100→0→100 (unchanged) |
| HeartMuLa | Time-based loop (unchanged) |
| GPU service down | Time-based loop (unchanged) |

### Files changed
- `gpu_service/services/diffusers_backend.py` — progress dict, callbacks in all 5 methods
- `gpu_service/routes/diffusers_routes.py` — progress endpoint
- `devserver/my_app/routes/settings_routes.py` — proxy endpoint
- `public/.../composables/useAnimationProgress.ts` — polling + real progress + hold latch

---

## Session 181 - DSGVO NER Verification Rewrite
**Date:** 2026-02-18
**Focus:** Fix broken DSGVO-NER false positive filtering — wrong prompt, wrong model, missing POS-tag pre-filter

### Problem

The DSGVO-NER LLM verification produced inconsistent results:
- "Benjamin Jörissen" (real professor) → LLM said "NEIN" (not a real person) → passed through
- "Karl Meier" (generic name in creative context) → LLM said "JA" (real person) → blocked

**Root causes:**
1. **Wrong question**: Prompt asked "Sind diese Wörter echte Personennamen real existierender Menschen?" with rule "Alltagsnamen = JA" — every common name got blocked
2. **JA/NEIN in German**: Language-dependent, double negation (NEIN = safe), error-prone
3. **SpaCy tagger disabled**: POS tags unavailable, so false positives like "Schräges Fenster" (ADJ+NOUN tagged as PER) reached the LLM
4. **Unauthorized model fallback**: Guard models (llama-guard) can't answer "is this a name?" but code silently fell back to hardcoded gpt-OSS:20b (20GB VRAM)

### Changes

**1. LLM Prompt rewrite** (`stage_orchestrator.py:206`)
- New question: "Are these flagged words actually person names, or false positives?"
- SAFE/UNSAFE output (English, language-independent, no double negation)
- Rules: actual names (real or fictional) = UNSAFE, descriptions/materials/places = SAFE

**2. SpaCy POS-tag pre-filter** (`stage_orchestrator.py:103,145-149`)
- Enabled tagger (removed from `disable` list)
- PER entities without PROPN tokens are filtered before LLM call
- "Schräges Fenster" (ADJ+NOUN) → filtered in milliseconds, no LLM needed

**3. Dedicated DSGVO_VERIFY_MODEL** (`config.py:142`)
- New config value, separate from SAFETY_MODEL (guard) and STAGE1_TEXT_MODEL (may be external)
- Must be general-purpose (not guard model), must be local (Ollama only)
- Removed unauthorized hardcoded gpt-OSS:20b fallback

**4. Settings UI** (`SettingsView.vue:130-162`)
- Added DSGVO-Verify Model dropdown in "Local Safety Models" section
- 6 options: qwen3:1.7b (recommended), gemma3:1b, qwen2.5:1.5b, llama3.2:1b, qwen3:0.6b, gpt-OSS:20b

### Architecture insight

Stage 1 safety has three independent steps, two of which may call an LLM:
1. §86a fast-filter (keywords, no LLM)
2. Age-filter → LLM context check on hit (guard model, fast)
3. DSGVO NER → LLM verify on hit (general-purpose model, now configurable)

The LLM verify's **only job** is filtering SpaCy false positives — not evaluating whether names are "safe" to use.

### Files changed
- `devserver/config.py` — DSGVO_VERIFY_MODEL
- `devserver/schemas/engine/stage_orchestrator.py` — prompt, POS filter, model selection
- `devserver/my_app/routes/settings_routes.py` — expose new setting
- `public/ai4artsed-frontend/src/views/SettingsView.vue` — dropdown UI

---

## Session 180 - Frontend Performance Analysis
**Date:** 2026-02-17
**Focus:** Audit what gets loaded when users access lab.ai4artsed.org — is the frontend bundle reasonable for weaker devices?

### Findings

**Architecture:** Pure client-side SPA (no SSR). All 23 routes use lazy loading via dynamic imports. The browser acts as a thin UI layer — all heavy computation (LLM, image/music generation, safety checks) stays on the backend/GPU service.

**Initial page load (~521 KB):**
- Main JS bundle (Vue, Router, Pinia, i18n, App shell): 469 KB
- Main CSS: 36 KB
- LandingView (lazy-loaded): ~12 KB
- Favicon + HTML: ~5 KB

**Route bundle sizes (lazy-loaded on navigation):**
- Landing: 9.6 KB | Text transformation: 43 KB | Image transformation: 19 KB
- Canvas workflow: 125 KB | Latent Lab: 120 KB | Settings: 526 KB (largest)
- Most routes: 1-21 KB

### Optimization opportunities (not urgent)
1. **`public/config-previews/originals_backup/` (67 MB)** — deployed to dist but never served to users. Should be excluded from production builds.
2. **i18n loaded eagerly (135 KB, 2137 lines)** — all DE+EN translations in a single `i18n.ts`, loaded regardless of active language. Could be split per language.
3. **Modals in App.vue loaded eagerly** — ChatOverlay, FooterGallery, AboutModal mounted in global shell. Could be lazy-loaded on first open.
4. **Settings page (526 KB)** — likely embeds full config editor with schemas. Could be code-split.
5. **No manual vendor chunk splitting** — Vite uses automatic splitting only. Manual chunks for Vue/Router/Pinia would improve cache hit rates on repeat visits.

### Conclusion
At ~521 KB initial load, the architecture is solid and well within reasonable territory (most popular sites ship 2-5 MB). Full route lazy-loading is correctly implemented. No immediate action required — these are future optimization candidates if device constraints become an issue.

### Repo cleanup (same session)

**Problem:** Repository carried ~194 non-essential files in git tracking — session handovers, archived docs, terminal saves, mockups, a nested ComfyUI git clone in `docs/reference/`, and 67 MB of config-preview originals deployed to `dist/`.

**Changes:**
- **Config-preview originals** (67 MB, 34 PNGs) moved from `public/.../originals_backup/` → `resources/config-preview-originals/` (gitignored). Next `npm run build` no longer copies them into `dist/`.
- **docs/ cleaned up**: Untracked `sessions/`, `archive/`, `reference/`, `tmp/`, `terminal-savings/`, `experiments/`, `plans/`. Moved 21 root-level HANDOVER/SESSION files into `docs/sessions/` first.
- **69 essential docs remain tracked**: Architecture (01-30), installation, analysis, dev log, decisions, todos, pedagogical concept, whitepaper, IP.
- **`.gitignore` fixed**: Removed broken absolute-path rules (lines 75-79 were no-ops), removed duplicate entries, added `resources/` and 6 `docs/` subdirectories.

All files remain on disk — only removed from git tracking.

---

## Session 176 - Model Availability: GPU Service Independence
**Date:** 2026-02-15
**Focus:** Break SwarmUI dependency for model availability — Diffusers configs must be reachable without ComfyUI running.

### Problem
Vue components (`text_transformation.vue`, `image_transformation.vue`) call `GET /api/models/availability` on mount to decide which model configs to show. The `ModelAvailabilityService` only queried ComfyUI's `/object_info` — non-ComfyUI backends (Diffusers, HeartMuLa) were blindly marked "always available" without checking the GPU service. When ComfyUI was down, the old code returned 503 with empty availability, hiding ALL models including Diffusers configs. Paradoxically, SwarmUI had to run for Diffusers models to appear.

### Solution
Each backend is now checked independently:
- **Diffusers configs** → query GPU service `GET /api/diffusers/available`
- **HeartMuLa configs** → query GPU service `GET /api/heartmula/available`
- **ComfyUI configs** → existing `/object_info` check (unchanged)
- **Cloud APIs** (OpenAI, OpenRouter) → always available (unchanged)

GPU service status is cached with the same 5-minute TTL as ComfyUI. The API response now includes `gpu_service_reachable` alongside `comfyui_reachable`. The 503 error on ComfyUI failure is replaced with a 200 containing partial results.

### Additional fixes
- Added missing `backend_type` to 9 output configs (8× `"comfyui"`, 1× `"heartmula"`)
- Switched `sd35_large` from ComfyUI to Diffusers primary (chunk `output_image_sd35_diffusers` already existed with ComfyUI fallback)
- Fixed `force_refresh` query parameter (was read but never propagated to caches)
- Added `comfyui_reachable` tracking flag (previously hardcoded `true`)

### Modified files
- `devserver/my_app/services/model_availability_service.py` — GPU cache, `get_gpu_service_status()`, per-backend routing
- `devserver/my_app/routes/config_routes.py` — `gpu_service_reachable` in response, removed 503 handler
- `devserver/schemas/configs/output/sd35_large.json` — Diffusers primary
- `devserver/schemas/configs/output/*.json` (8 files) — added `backend_type`
- `public/ai4artsed-frontend/src/services/api.ts` — TypeScript interface update

### Verified
- ComfyUI down + GPU service up → Diffusers/HeartMuLa configs `true`, ComfyUI configs `false`, `comfyui_reachable: false`, `gpu_service_reachable: true`
- SD3.5 Large visible in text-transformation Vue without SwarmUI
- `npm run type-check` passes

---

## Session 175 - DSGVO LLM Verification: Thinking Model Fallback
**Date:** 2026-02-15
**Focus:** Fix gpt-OSS:20b returning empty `content` in DSGVO NER verification

### Problem
Music generation blocked by DSGVO safety check: SpaCy NER falsely flagged German lyrics phrases ("Ast zu Ast", "Jedes Lied") as person names. The LLM verification (gpt-OSS:20b) should reject these as false positives, but returned empty `content` → fail-closed → blocked.

### Root Cause
gpt-OSS:20b is a **thinking model** — it puts reasoning in `message.thinking` and the final answer in `message.content`. Under VRAM pressure (SD3.5 Large occupying 28.6GB of 32GB), the model sometimes puts all output into `thinking`, leaving `content` empty. The code only checked `content`.

Direct Ollama curl test confirmed model works correctly (`content: "NEIN\nNEIN"`), proving this was a code-side issue, not a model issue.

### Fix
Added `thinking` field fallback to `llm_verify_person_name()` in `stage_orchestrator.py`: when `content` is empty, extract JA/NEIN from the `thinking` field. Same pattern already used by VLM safety check for qwen3-vl.

### Modified files
- `devserver/schemas/engine/stage_orchestrator.py` — thinking field fallback in `llm_verify_person_name()`
- `docs/ARCHITECTURE PART 29 - Safety-System.md` — document thinking model behavior

### Verified
- "Ast zu Ast, Jedes Lied" → `safe: true` (NER false positive correctly rejected by LLM)
- "Angela Merkel" → `safe: false` (real person correctly blocked by DSGVO)

---

## Session 174 - Shared GPU Service (Diffusers + HeartMuLa)
**Date:** 2026-02-14
**Focus:** Avoid double VRAM usage when dev (17802) and prod (17801) backends run simultaneously.

### Problem
Both Flask backends loaded Diffusers models (SD3.5 ~10GB+) and HeartMuLa independently in-process, doubling GPU memory usage on the same machine.

### Solution: GPU Service (Port 17803)
Extracted all GPU-intensive inference into a standalone Flask/Waitress service at `gpu_service/`. Both dev and prod backends now call this service via HTTP REST instead of loading models themselves.

**Architecture:**
```
Dev Backend (17802) ──┐
                      ├── HTTP REST ──→ GPU Service (17803) ──→ [GPU VRAM]
Prod Backend (17801) ─┘
```

**Key design**: `DiffusersClient` and `HeartMuLaClient` are drop-in replacements with identical async APIs. `get_diffusers_backend()` and `get_heartmula_backend()` now return HTTP clients. Zero changes to `backend_router.py` or chunk files.

### New files
- `gpu_service/` — Complete Flask service (server.py, config.py, app.py, routes/, services/)
- `devserver/my_app/services/diffusers_client.py` — HTTP client for Diffusers (all 7 generation modes)
- `devserver/my_app/services/heartmula_client.py` — HTTP client for HeartMuLa
- `6_start_gpu_service.sh` — Startup script

### Modified files
- `devserver/config.py` — Added `GPU_SERVICE_URL`, `GPU_SERVICE_TIMEOUT`
- `devserver/my_app/services/diffusers_backend.py` — Factory returns `DiffusersClient`
- `devserver/my_app/services/heartmula_backend.py` — Factory returns `HeartMuLaClient`

### Tested
- Standard image generation (SD3.5 Large) via HTTP bridge
- Feature Probing (Latent Lab) via HTTP bridge
- HeartMuLa music generation via HTTP bridge
- All working over internet (Cloudflare tunnel)
- Fallback: if GPU service is down, `is_available()` returns False → ComfyUI fallback

### Pending (Phase 4)
- Remove original DiffusersImageGenerator class (1700 lines) from devserver (now duplicated in gpu_service)
- Update ARCHITECTURE PART 27
- Test remaining Latent Lab modes (Attention Cartography, Concept Algebra, Denoising Archaeology, Surrealizer)

---

## Session 173 - Fix Feature Probing "Backend error: None" + Remove CPU Offloading
**Date:** 2026-02-14
**Focus:** Feature Probing failed with `"Backend error: None"` — actual exception was masked. Root cause: broken CPU offloading layer from Session 149-172.

### Problem
Feature Probing returned `None` on failure, which `backend_router.py` turned into `"Backend error: None"` — no actual exception visible. The underlying failures were caused by the CPU offloading layer (`_offload_to_cpu`, `_move_to_gpu`, `device_map="balanced"`) added for Flux2 in Sessions 149-172, which corrupted pipeline state.

### Solution (Two Phases)

**Phase 1: Error propagation** — `generate_image_with_probing()` now returns `{'error': str}` dicts instead of `None`. `backend_router.py` checks for error dicts and propagates the actual error message via `BackendResponse.error`.

**Phase 2: Remove CPU offloading layer** — Removed the entire three-tier memory system (GPU→CPU→Disk), keeping only the multi-model GPU cache with LRU eviction. Models are either on GPU or not loaded. Eviction = full `del` + `torch.cuda.empty_cache()`.

### What was removed (-214 lines)
- `_offload_to_cpu()`, `_move_to_gpu()`, `_model_device` dict, `_get_available_ram_mb()`
- `enable_cpu_offload` parameter, `device_map="balanced"` code path
- `DIFFUSERS_RAM_RESERVE_AFTER_OFFLOAD_MB` (16GB RAM check that silently blocked loading)
- `DIFFUSERS_PRELOAD_MODELS` + warmup thread in `__init__.py`

### What was kept
- `_pipelines` dict (GPU cache), `_model_last_used` (LRU), `_model_vram_mb`, `_model_in_use` (refcount), `_load_lock`
- `_ensure_vram_available()` (simplified: always-delete eviction)
- `_load_model_sync()` (simplified: 2 cases instead of 3)

### Files changed
- `devserver/my_app/services/diffusers_backend.py` — core simplification
- `devserver/schemas/engine/backend_router.py` — error propagation, remove `enable_cpu_offload`
- `devserver/my_app/__init__.py` — remove warmup thread
- `devserver/config.py` — remove `DIFFUSERS_PRELOAD_MODELS`, `DIFFUSERS_RAM_RESERVE_AFTER_OFFLOAD_MB`
- `devserver/schemas/chunks/output_image_flux2_diffusers.json` — remove `enable_cpu_offload`
- `docs/ARCHITECTURE PART 27` — rewritten to reflect simplified design

### Result
Feature Probing works. Error messages now show actual exceptions instead of "None".

---

## Session 172 - The Flux2 Diffusers Saga: Why `from_pretrained()` Can't Load 106GB
**Date:** 2026-02-11 to 2026-02-12
**Focus:** Attempted to run FLUX.2-dev via HuggingFace Diffusers instead of ComfyUI. Failed after 6+ attempts. Root cause: architectural limitation of `from_pretrained()`.

### The Goal
Run Flux2 via Diffusers backend to enable dev+prod servers in parallel on a single RTX PRO 6000 Blackwell (96GB VRAM). The assumption: FP8 quantization would halve VRAM usage from ~24GB to ~12GB.

### The Assumption Was Wrong
FLUX.2-dev is not a ~24GB model. Actual sizes measured from HuggingFace cache:

| Component | BF16 on Disk | Shards |
|---|---|---|
| Transformer | **61 GB** | 7 × ~9GB |
| Text Encoder (Mistral Small 3.1 24B) | **45 GB** | 10 × ~4.5GB |
| VAE | 321 MB | 1 |
| **Total** | **~106 GB** | |

The GPU has 96GB VRAM — enough for each component individually, but not both simultaneously (61+45=106 > 96).

### Attempt 1: Runtime FP8 Quantization (`TorchAoConfig`)
- Added `TorchAoConfig("float8_weight_only")` to `from_pretrained()`
- **Result:** OOM crash. 64GB RAM + 71GB swap completely full → Fedora kernel OOM kill → system reboot
- **Why:** Runtime quantization via torchao bypasses safetensors mmap. Instead of lazy-loading from SSD, the entire model is materialized in anonymous CPU memory for quantization. 106GB anonymous memory > 135GB total virtual memory (with OS overhead).

### Attempt 2: `low_cpu_mem_usage=False`
- Research agent claimed `low_cpu_mem_usage=True` was incompatible with TorchAoConfig
- Setting to `False` loads EVERYTHING at once instead of shard-by-shard
- **Result:** Made it worse. Same OOM crash.

### Attempt 3: Component-Level Quantization
- Load transformer separately with TorchAoConfig, inject into pipeline
- **Result:** Same OOM crash. The transformer alone is 61GB — quantizing it still requires full materialization.

### Full Revert (Commit `cd928cf`)
All quantization code removed. Pipeline_class bug fix and chunk consolidation preserved.

### Attempt 4: `enable_model_cpu_offload()` (Commit `1402b96`)
- Insight: Both components fit individually in 96GB VRAM. Use `enable_model_cpu_offload()` to load one at a time.
- **Result:** Never reached. `from_pretrained()` loads ALL 106GB to CPU RAM first, then `enable_model_cpu_offload()` sets up hooks. But the loading itself crashes at component 3/5 with only 19GB free RAM (SD3.5 was offloaded to CPU, consuming 28GB).

### Attempt 5: `device_map="balanced"` (Commit `f04f080`)
- Hypothesis: `device_map` in `from_pretrained()` loads components directly to GPU/CPU targets via safetensors mmap, bypassing full RAM materialization.
- **Result:** Same OOM crash at "Loading pipeline components: 40% 2/5". `from_pretrained()` still materializes everything in CPU RAM regardless of `device_map`.

### Attempt 6: Auto-Detection Fix + Full Unload (Commit `54f64ee`)
- Discovered the auto-detection in `backend_router.py` was routing `flux2` (ComfyUI config) through Diffusers without `enable_cpu_offload` flag.
- Added full model unload (not just CPU offload) before large model loads.
- Also added `enable_cpu_offload: True` to auto-detection map.
- **Result:** Still crashed — the fundamental `from_pretrained()` limitation remained.

### The Root Cause
**`DiffusionPipeline.from_pretrained()` is architecturally incapable of loading a 106GB model on a 64GB RAM system.** It creates anonymous CPU tensors for every parameter, regardless of `low_cpu_mem_usage`, `device_map`, or `use_safetensors`. The mmap from safetensors is only used during the read — the data is copied into regular (non-evictable) memory.

ComfyUI works because it uses `safetensors.safe_open(device="cuda")` to load each tensor individually, directly to GPU. The page cache pages are file-backed and immediately evictable. No intermediate anonymous CPU memory is ever allocated for the full model.

**This is not a configuration problem. It is a design limitation of `from_pretrained()`.**

### The Solution: ComfyUI (Commits `62f323b`, `182b162`)
- Removed Flux2 from Diffusers auto-detection map
- Set `requires_workflow: true` on Flux2 ComfyUI chunk (was `false`, causing fallback to SwarmUI simple API which defaults to SD3.5)
- Flux2 now routes through ComfyUI workflow with `flux2_dev_fp8mixed.safetensors` checkpoint
- **Result:** Works. 18 seconds generation, VLM safety check passes.

### Collateral Fixes
- **`pipeline_class` bug fix:** `generate_image()` was not passing `pipeline_class` to `load_model()`, causing Flux2 to load with wrong `StableDiffusion3Pipeline` on cold start
- **Chunk consolidation:** Deleted redundant `output_image_flux2_fp8.json` chunk. Both output configs (`flux2_diffusers`, `flux2_fp8`) now point to single `output_image_flux2_diffusers` chunk (1-chunk:N-configs principle)
- **`enable_cpu_offload` infrastructure:** Parameter chain added to `_load_model_sync()` → `load_model()` → `generate_image()`. Useful for future models that are large but fit component-wise in RAM.
- **Full model unload before large loads:** When loading a `cpu_offload` model, all cached models are fully freed from CPU RAM (not just offloaded from GPU).
- **LICENSE cleanup:** Removed old `LICENSE` file (1.6KB plaintext). `LICENSE.md` (19.8KB, bilingual DE/EN, §1-§10) is the canonical license. Fixes GitHub showing two license tabs.

### Git Chaos
- Committed LICENSE removal directly on `main` instead of `develop` (had to cherry-pick to develop)
- Remote `origin/main` had diverged (LICENSE edit via GitHub web UI) → required force push
- Deploy script failed twice due to diverged histories

### Commits (Session 172)
1. `eac30c1` feat(diffusers): FP8 quantization with automatic disk caching for Flux2
2. `b3076d8` fix(diffusers): Move save_pretrained after .to(device) to reduce peak RAM
3. `c2c06ff` fix(diffusers): Fix OOM crash — low_cpu_mem_usage incompatible with TorchAoConfig
4. `deda3fd` fix(diffusers): Component-level quantization to prevent OOM crash
5. `cd928cf` revert(diffusers): Remove FP8 runtime quantization — causes OOM on 64GB systems
6. `1402b96` feat(diffusers): Use enable_model_cpu_offload() for Flux2
7. `f04f080` fix(diffusers): Use device_map="balanced" for cpu_offload
8. `54f64ee` fix(diffusers): Add enable_cpu_offload to auto-detection + fully unload models
9. `62f323b` fix(router): Remove Flux2 from Diffusers auto-detection — route through ComfyUI
10. `182b162` fix(flux2): Set requires_workflow=true — Flux2 uses full ComfyUI workflow
11. `61ccf97` chore: Remove old LICENSE — LICENSE.md is the canonical license file

### Lessons Learned
1. **Know your model size before writing code.** 106GB ≠ 24GB. `du -shL` on the HF cache would have revealed this in 10 seconds.
2. **`from_pretrained()` always materializes in CPU RAM.** No combination of flags (`device_map`, `low_cpu_mem_usage`, `use_safetensors`) changes this. For models larger than system RAM, diffusers cannot load them.
3. **ComfyUI's architecture is fundamentally different:** Per-tensor `safe_open(device="cuda")` with manual model management vs. diffusers' monolithic `from_pretrained()` pipeline approach.
4. **Auto-detection maps must mirror chunk configs.** Hardcoded model config in `backend_router.py` duplicated and diverged from chunk JSON configs — classic DRY violation.
5. **`requires_workflow: false` on a chunk with a full ComfyUI workflow** silently routes to SwarmUI simple API, which uses a default model. No error, just wrong output.
6. **CPU offload ≠ RAM savings during loading.** `enable_model_cpu_offload()` only helps during inference (components move to GPU on demand). It cannot help if the model doesn't fit in RAM during `from_pretrained()`.

### Open: Future Diffusers Approach
The `flux2_diffusers` and `flux2_fp8` output configs remain as stubs. If HuggingFace improves `from_pretrained()` to support true streaming/mmap loading (or if the system gets 128GB+ RAM), these configs are ready. The `enable_cpu_offload` infrastructure in `diffusers_backend.py` is in place.

---

## Session 171 - Safety System: Testing, Bug Fixes, DSGVO LLM Verification
**Date:** 2026-02-12
**Focus:** Comprehensive safety system testing (26 test cases), 2 critical bugs found and fixed, test report

### Bugs Found & Fixed

**Bug A: DSGVO LLM Verification broken for thinking models**
- `llm_verify_person_name()` used `num_predict: 10` — gpt-OSS:20b thinking mode exhausted all tokens in `thinking` field, `content` always empty → fail-closed on every NER detection
- Few-shot examples in prompt confused model into classifying all examples instead of answering
- Fix: Simplified prompt (rules only, no examples), `num_predict: 500`, `timeout: 60`
- Verified: "Angela Merkel" → JA (block), "Der Eiffelturm" → NEIN (pass), "Paul Meier" → JA (block)

**Bug B: Fuzzy matching false positives on short terms**
- Levenshtein `max_distance=2` for terms ≥6 chars → 33% error rate on 6-char words
- "Potter" matched "Folter", "wurde" matched "murder", "gebaut" matched "Gewalt"
- Fix: Graduated threshold — `max_distance=1` for 6-7 char terms, `=2` for 8+ chars
- Unit tested: all false positives eliminated, true matches preserved

**i18n context resolution refactor**
- `resolve_context_language()` added to `config_loader.py`
- Context kept as raw dict through pipeline, resolved at point of use
- Fixes crash when interception configs use multilingual context `{en:..., de:...}`

### Test Report
- `docs/TEST_REPORT_Safety_System_Session171.md` — 26 test cases across 3 safety levels
- §86a, age filter (DE+EN), DSGVO NER + LLM verification, research/adult/kids mode switching

### Files Changed
- `devserver/schemas/engine/stage_orchestrator.py` — DSGVO prompt, num_predict, fuzzy distance
- `devserver/schemas/engine/config_loader.py` — `resolve_context_language()`, context passthrough
- `devserver/schemas/engine/chunk_builder.py` — Use `resolve_context_language()` for instruction text
- `devserver/my_app/routes/schema_pipeline_routes.py` — Use `resolve_context_language()` in optimization
- `devserver/testfiles/test_refactored_system.py` — Adapt to dict context
- `docs/TEST_REPORT_Safety_System_Session171.md` — Comprehensive test report

---

## Session 170 - UCDCAE Branding + Bugfix
**Date:** 2026-02-12
**Focus:** Visual connection between UCDCAE acronym and full name on landing page, plus type error fix

### Changes

**Landing Page: Colored Initials (LandingView.vue)**
- Full name "UNESCO Chair in Digital Culture and Arts in Education" now shows initial letters in UCDCAE color scheme
- U=#667eea, C=#e91e63, D=#7C4DFF, C=#FF6F00, A=#4CAF50, E=#00BCD4
- Function words ("in", "and") remain uncolored
- Full name styled with `font-weight: 600` for subtle emphasis

**i18n: Subtitle Split (DE + EN)**
- `landing.subtitle` → `landing.subtitlePrefix` + `landing.subtitleSuffix`
- Chair name rendered inline with colored spans (language-independent)

**Bugfix: MediaInputBox TS2304 (MediaInputBox.vue)**
- Removed vestigial `isStreaming.value = false` in `blocked` event handler
- Ref was never declared — EventSource close + stopBufferProcessor already handle cleanup

### Verification
- `npm run type-check` → 0 Errors

### Files Changed
- `public/.../src/views/LandingView.vue` — Colored initials + CSS
- `public/.../src/i18n.ts` — Subtitle split (DE+EN)
- `public/.../src/components/MediaInputBox.vue` — Remove dead isStreaming reference

---

## Session 169 - Latent Lab: Concept Algebra Implementation + UI Fixes
**Date:** 2026-02-12
**Focus:** Full implementation of Concept Algebra tab (A − B + C vector arithmetic on embeddings), then UI fixes based on first user testing

### Changes

**Backend: Concept Algebra Pipeline**
- `embedding_analyzer.py` — `apply_concept_algebra()`: tensor arithmetic (result = A − scale_sub×B + scale_add×C) with L2 distance metric
- `diffusers_backend.py` — `generate_image_with_algebra()`: encodes 3 prompts via SD3.5 triple encoder, applies algebra, generates reference + result images with same seed
- `output_image_concept_algebra_diffusers.py` — Python chunk (Stage 4) calling `backend.generate_image_with_algebra()` directly
- `concept_algebra_diffusers.json` — Output config routing to Python chunk
- `pipeline_executor.py` — Added `algebra_data`, `reference_image`, `result_image` to metadata whitelist
- `schema_pipeline_routes.py` — Extract `prompt_c`, `algebra_encoder`, `scale_sub`, `scale_add` from request, forward via `custom_params`, handle `diffusers_algebra_generated` response

**Frontend: Concept Algebra Tab**
- `concept_algebra.vue` — Full Vue component: 3 MediaInputBox inputs (A/B/C), encoder selector (All/CLIP-L/CLIP-G/T5), formula visualization, side-by-side image comparison, advanced settings (seed/cfg/steps/negative/scales), PageContext for Trashy
- `latent_lab.vue` — Added ConceptAlgebra tab import + conditional rendering

**i18n (DE + EN)**
- ~35 new `latentLab.algebra.*` keys: headers, explanations (what/how/read/tech), labels, placeholders
- `explainHowText`: detailed comparison of Concept Algebra vs. negative prompting
- `explainReadText`: commutativity explanation + King−Man+Woman analogy

**Architecture Documentation**
- `ARCHITECTURE PART 28 - Latent-Lab.md` — Scientific Foundation section (7 papers + DOIs), Concept Algebra section (mechanism, key files, frontend)

### UI Fixes (post-testing, commit 99f27d4)
- **Icons**: `icon="minus"` → `icon="−"` (U+2212), `icon="plus"` → `icon="＋"` (U+FF0B) — Unicode fallback in MediaInputBox
- **Seed field**: `setting-small` (80px) → `setting-seed` (14ch) — zoom-safe width
- **Side-by-side layout**: `flex: 0 1 480px` → `flex: 1 1 0` — prevents wrapping within 1000px container
- **Explanation text**: Extended `explainReadText` (DE+EN) with commutativity, vector space linearity, and why simple addition works

### Verification
- `npm run type-check` → 0 Errors

### Files Changed
- `devserver/my_app/services/embedding_analyzer.py` — `apply_concept_algebra()`
- `devserver/my_app/services/diffusers_backend.py` — `generate_image_with_algebra()`
- `devserver/schemas/chunks/output_image_concept_algebra_diffusers.py` — Python chunk
- `devserver/schemas/configs/output/concept_algebra_diffusers.json` — Output config
- `devserver/schemas/engine/pipeline_executor.py` — Metadata whitelist
- `devserver/my_app/routes/schema_pipeline_routes.py` — Request extraction + response handling
- `public/.../src/views/latent_lab/concept_algebra.vue` — NEU
- `public/.../src/views/latent_lab.vue` — Tab integration
- `public/.../src/i18n.ts` — Algebra keys (DE+EN)
- `docs/ARCHITECTURE PART 28 - Latent-Lab.md` — Scientific Foundation + Concept Algebra docs

---

## Session 168 - Rebranding: AI4ARTSED → UCDCAE AI LAB
**Date:** 2026-02-11
**Focus:** Institutional rebranding from single-project name to UNESCO Chair identity

### Changes

**Header (App.vue)**
- FAU logo added (links to fau.de), placed left of UNESCO logo
- AI4ArtsEd logo removed from header
- Title: `AI4ARTSED - AI LAB` → `UCDCAE AI LAB` with tooltip showing full name
- Browser tab title updated in `index.html`

**Landing Page (LandingView.vue)**
- Hero title: `AI4ArtsEd` → `UCDCAE AI LAB`
- "KI verändert Gesellschaft..." research paragraph removed
- Funding section: BMBFSFJ logo kept, AI4ArtsEd + COMeARTS project logos added below

**About Modal (AboutModal.vue)**
- Project logos (AI4ArtsEd + COMeARTS) added to funding section

**i18n (DE + EN)**
- `app.title`, `landing.subtitle`, `about.title`, `about.intro`, `legal.privacy.usage` updated
- `landing.research` set to empty string

**Backend: CORS Origins (config.py + __init__.py)**
- Extracted hardcoded CORS origins list to `CORS_ALLOWED_ORIGINS` in `config.py`
- Origins auto-generated from `PORT` variable (no more hardcoded port numbers)

**Assets**
- `fau_logo.png` (9.2 KB) — FAU Kernmarke
- `comearts_logo.jpg` (122 KB) — COMeARTS project logo

### Verification
- `npm run type-check` → 0 Errors

### Files Changed
- `devserver/config.py` — `CORS_ALLOWED_ORIGINS`
- `devserver/my_app/__init__.py` — Import + use `CORS_ALLOWED_ORIGINS`
- `public/.../index.html` — Tab title
- `public/.../src/App.vue` — Header logos + title
- `public/.../src/components/AboutModal.vue` — Project logos
- `public/.../src/i18n.ts` — All rebranded text (DE + EN)
- `public/.../src/views/LandingView.vue` — Hero + funding section
- `public/.../public/logos/fau_logo.png` — NEU
- `public/.../public/logos/comearts_logo.jpg` — NEU

---

## Session 167 - Diffusers Backend: VRAM Management, FP8, RAM Safety
**Date:** 2026-02-11
**Focus:** Three-tier VRAM management, FP8 quantization for Flux2, RAM pre-checks

### Changes

**Three-Tier VRAM Management (`diffusers_backend.py`)**
- GPU VRAM → CPU RAM → Disk memory hierarchy
- `_load_model_sync()` handles 3 cases: GPU-resident, CPU-offloaded, cold-load from disk
- `_offload_to_cpu()` / `_move_to_gpu()` for model swapping
- `_ensure_vram_available()` eviction logic with refcount protection
- Refcount `_model_in_use[model_id]` prevents eviction during inference

**FP8 Quantization for Flux2**
- Automatic disk caching of quantized models (`save_pretrained` after `.to(device)`)
- Fixed peak RAM issue: moved `save_pretrained` after GPU transfer

**RAM Safety**
- Pre-check: verify enough system RAM before loading models
- Graceful degradation when system RAM too low for CPU offload
- Check remaining RAM *after* offload, not just model fit
- Evict all GPU models before cold-loading a new model
- Suppress noisy diffusers warnings during CPU offload

**SD3.5 Turbo Schema**
- New output config for SD3.5 Turbo (fewer steps, faster inference)

### Files Changed
- `devserver/my_app/services/diffusers_backend.py`
- `devserver/schemas/chunks/output_image_flux2_diffusers.json`
- `devserver/schemas/chunks/output_image_flux2_fp8.json`
- `devserver/schemas/configs/output/flux2_fp8.json`
- `devserver/schemas/engine/backend_router.py`

---

## Session 166 - Music Tag Diversification, License, Docs, Denoising Archaeology
**Date:** 2026-02-11
**Focus:** UNESCO-aligned music tags, bilingual license, Latent Lab denoising archaeology

### Changes

**Music Generation v2: Tag Diversification**
- Added UNESCO-aligned regional instruments and genres
- Global instrument and genre coverage beyond Western defaults
- Fixed lyrics button behavior

**UCDCAE AI Lab License v1.0**
- Custom bilingual license (DE/EN)
- Covers educational use, research exceptions, attribution requirements

**Documentation**
- Renamed "Workshop" tab to "Praxis" in docs
- Added license and installation information

**Latent Lab: Denoising Archaeology**
- New tab: step-by-step visualization of the denoising process
- Shows intermediate latent states during diffusion

### Files Changed
- `public/.../src/i18n.ts` — Music tags, rebranding (i18n texts)
- `LICENSE` — NEU: UCDCAE AI Lab License v1.0
- `docs/` — Workshop → Praxis rename, installation info
- Latent Lab Vue components — Denoising Archaeology tab

---

## Session 165 - Research-Level Gating + Latent Lab Architecture Docs
**Date:** 2026-02-11
**Focus:** Safety-level gating for Canvas & Latent Lab; rename `off` → `research`; architecture documentation

### Problem
Canvas und Latent Lab nutzen direkte Pipeline-Aufrufe ohne vollständige Safety-Stages. Statt Safety in jeden experimentellen Endpoint nachzurüsten (was den dekonstruktiven Charakter zerstören würde), wird der Zugang gegated.

### Changes

**Backend: Safety-Level Rename `off` → `research`**
- ~25 Stellen in 6 Dateien: `config.py`, `schema_pipeline_routes.py`, `stage_orchestrator.py`, `workflow_logic_service.py`, `export_manager.py`, `workflow_streaming_routes.py`
- Neuer öffentlicher Endpoint: `GET /api/settings/safety-level`
- `testfiles/` bewusst nicht geändert (Build-Artefakte)

**Frontend: Feature-Gating**
- `stores/safetyLevel.ts` (NEU): Pinia-Store mit `fetchLevel()`, `isAdvancedMode`, `isResearchMode`, `researchConfirmed`
- `components/ResearchComplianceDialog.vue` (NEU): Compliance-Modal mit Warnung, Altershinweis, Checkbox
- `views/LandingView.vue`: Canvas + Latent Lab Cards mit `requiresAdvanced: true`, Locked-State (Opacity 0.4, Schloss-Icon)
- `router/index.ts`: `meta: { requiresAdvanced: true }` + Navigation Guard
- `main.ts`: Safety-Level-Fetch beim App-Start
- `i18n.ts`: `research.*` Keys (DE + EN)

**Dokumentation**
- `docs/DEVELOPMENT_DECISIONS.md`: Zwei neue Entscheidungen (Research-Level-Gating, Latent Lab als konsolidierter Modus, Diffusers als Introspection-Plattform)
- `docs/ARCHITECTURE PART 28 - Latent-Lab.md` (NEU): Vollständige Architektur-Dokumentation mit Attention Cartography, Feature Probing, Legacy-Workflows, Hallucinator-Abgrenzung, Research-Level-Gating

### Verification
- `npm run type-check` → 0 Errors
- `grep -r "'off'" devserver/my_app devserver/schemas/engine devserver/config.py` → 0 Treffer

### Files Changed
- `devserver/config.py` — `off` → `research`
- `devserver/my_app/routes/schema_pipeline_routes.py` — 4× rename
- `devserver/schemas/engine/stage_orchestrator.py` — 8× rename
- `devserver/my_app/services/workflow_logic_service.py` — 2× rename
- `devserver/my_app/services/export_manager.py` — 2× rename
- `devserver/my_app/routes/workflow_streaming_routes.py` — 1× rename
- `devserver/my_app/routes/settings_routes.py` — neuer Endpoint
- `public/.../stores/safetyLevel.ts` — NEU
- `public/.../components/ResearchComplianceDialog.vue` — NEU
- `public/.../views/LandingView.vue` — Feature-Gating
- `public/.../router/index.ts` — Guard + Meta
- `public/.../main.ts` — Store-Init
- `public/.../i18n.ts` — research.* Keys
- `docs/DEVELOPMENT_DECISIONS.md` — 2 neue Entscheidungen
- `docs/ARCHITECTURE PART 28 - Latent-Lab.md` — NEU

## Session 164 - Landing Page Restructure + Preset Selection Overlay
**Date:** 2026-02-10
**Focus:** Replace outdated `/select` bubble page with proper landing page; move interception preset selection into contextual overlay

### Problem
The platform outgrew its original entry point (`/select` = PropertyQuadrantsView). That page showed interception presets as the entry experience, but Canvas, HeartMuLa, Surrealizer, and Latent Lab don't use interception presets at all. Users were confused: "what feature do I want?" was conflated with "which interception style?".

### Changes

**New Landing Page (`LandingView.vue`)**
- Feature-dashboard with 6 cards: Text-Transformation, Image-Transformation, Bildfusion, Musikgenerierung, Canvas Workflow, Latent Lab
- Each card: rotating preview images (staggered per-card timing with ±800ms jitter), accent color, icon, description
- Hero section with research context (i18n DE/EN)
- Funding footer (BMBFSFJ logo + kubi-meta link)
- Route: `/` → LandingView (replaces old `/select`)

**InterceptionPresetOverlay (`InterceptionPresetOverlay.vue`)**
- Fullscreen bubble overlay reusing PropertyBubble component
- Triggered by icon button on Context-MediaInputBox (only in text/image/multi-image transformation views)
- Filters to `text_transformation` + `text_transformation_recursive` pipelines only (no music/latent-lab modes)
- On selection: loads config context, populates prompt, closes overlay
- Explicit square container via CSS `--square-size` for guaranteed circular bubbles

**Header Updates**
- Mode ordering: didactic simple→complex (Text → Image → Multi-Image → Music → Canvas → Latent Lab)
- Latent Lab: Material Symbols `biotech` (microscope) icon
- LoRA Training: Parrot SVG icon (from qubodup) with `fill="currentColor"`

**Router**
- `/select` route removed entirely (no redirect)
- `/home` route removed
- Auth guard updated: `property-selection` → `landing`

**Preview Images**
- All large screenshots resized to 480px JPEG (quality 82) for fast loading
- Total preview assets: ~1MB (down from ~40MB+ originals)

### Files Changed
- `src/views/LandingView.vue` — NEW
- `src/components/InterceptionPresetOverlay.vue` — NEW
- `src/components/MediaInputBox.vue` — showPresetButton prop + emit
- `src/views/text_transformation.vue` — overlay wiring
- `src/views/image_transformation.vue` — overlay wiring
- `src/views/multi_image_transformation.vue` — overlay wiring
- `src/App.vue` — header icon updates (order, Latent Lab icon, parrot)
- `src/router/index.ts` — route changes
- `src/i18n.ts` — landing + presetOverlay + multiImage keys (DE/EN)
- `public/config-previews/` — resized preview images

## Session 163 - Hallucinator: Exact ComfyUI Replication + Configurable Parameters
**Date:** 2026-02-09
**Focus:** Fix fundamentally broken Diffusers fusion by exact replication of original ComfyUI CLIP flow; add configurable negative prompt and CFG

### The Problem — Why This Was So Difficult

The Diffusers Hallucinator produced incoherent collage-like images instead of the original's surreal-but-coherent output. Fixing it required **three separate rounds of investigation** because the root causes were non-obvious and layered:

**Round 1 — Wrong diagnosis (CLIP-G in embedding):**
Initial analysis incorrectly concluded that CLIP-G was missing from the fusion embedding. This was based on comparing standard SD3 encoding (which uses CLIP-L+G) with our code. Fix: added CLIP-G to the fusion. Result: **made it worse** — more visual anchoring = less surreal.

**Round 2 — Reading the actual ComfyUI source (the breakthrough):**
Deep dive into the original ComfyUI workflow JSON + sd3_clip.py source revealed the truth: the original loads **only clip_l.safetensors** via a separate CLIPLoader. This means:
- SD3ClipModel is instantiated with `clip_g=None`
- `encode_token_weights()` produces `g_pooled = torch.zeros((1, 1280))`
- The **pooled output** is 768d real + 1280d **ZEROS** — not real CLIP-G

This was the critical insight: **real CLIP-G pooled gives the DiT strong visual anchoring that actively fights the extrapolation**. The zeroed CLIP-G pooled is not a bug — it's what enables the surreal effect. The pooled output conditions the timestep embedding, providing global guidance to the generation. With real CLIP-G, the model "knows" what a normal image should look like and resists the extrapolated conditioning.

**Round 3 — The negative prompt:**
Even after fixing the pooled output, images were still collage-like. Investigation revealed the **negative prompt was empty** (`""`). The original ComfyUI workflow uses `"watermark"` as negative, also fused with the same alpha. At α=17.6, an empty-string negative produces extrapolated special-token garbage as the CFG reference point, corrupting the entire generation.

### Lessons for Future Deconstructive Workflows

1. **Never assume standard encoding matches custom workflows.** The original Surrealizer intentionally degrades the encoding (CLIP-L only, no CLIP-G) to create its effect. Standard "best practice" (using all encoders) destroys the artistic intent.

2. **Pooled output is as important as the main embedding.** For SD3, pooled conditions the timestep embedding — it's a global steering signal. Zeroing parts of it changes the generation character fundamentally.

3. **Read the original ComfyUI node graph AND the ComfyUI source code.** The workflow JSON shows which nodes connect, but understanding the actual tensor shapes requires reading `sd3_clip.py:encode_token_weights()` and understanding what happens when individual encoders are None.

4. **The negative prompt matters enormously in the extrapolated regime.** CFG subtracts negative from positive predictions. Both are extrapolated with the same alpha, so they're in the same "scale space". The semantic content of the negative determines what the image pushes AWAY from — a powerful creative parameter.

5. **Seed logic must track ALL user-adjustable parameters.** Keep seed stable when any parameter changes (for comparability), new seed only when nothing changed (user wants variation).

### Changes Made

1. **`diffusers_backend.py`**: Exact ComfyUI replication — CLIP-L only, pooled = CLIP-L(768d) + zeros(1280d), no CLIP-G anywhere
2. **`output_image_surrealizer_diffusers.json`**: Added `negative_prompt` default "watermark", CFG 5.5
3. **`schema_pipeline_routes.py`**: Pass `negative_prompt` and `cfg` through legacy endpoint
4. **`surrealizer.vue`**: Configurable negative prompt + CFG in collapsible "Weitere Einstellungen"; fixed seed logic to track all parameters
5. **`i18n.ts`**: DE/EN strings for new controls with explanations; fixed `|` pipe pluralization bug (use `{'|'}`)
6. **`DEVELOPMENT_DECISIONS.md`**: Corrected pooled output description (was incorrectly claiming real CLIP-G)
7. **`ARCHITECTURE PART 25`**: Updated code example and design decisions to reflect no-CLIP-G-anywhere approach

### Key References
- Original fusion node: `/SwarmUI/dlbackend/ComfyUI/custom_nodes/ai4artsed_comfyui/ai4artsed_t5_clip_fusion.py`
- SD3 CLIP encoding: `/SwarmUI/dlbackend/ComfyUI/comfy/text_encoders/sd3_clip.py` (SD3ClipModel.encode_token_weights)
- Original workflow: `devserver/schemas/chunks/legacy_surrealization.json` (Node 54: CLIPLoader clip_l, Node 18: CLIPLoader t5xxl)

---

## Session 162 - Hallucinator: Diffusers Token-Level Fusion + Rename
**Date:** 2026-02-08
**Focus:** Fix broken Diffusers embedding fusion, document the complete technical mechanism, rename Surrealizer → Hallucinator
**Status:** COMPLETE

### Problems

1. **Broken embedding fusion:** The Diffusers backend blended joint SD3 embeddings instead of individual encoder outputs, destroying the CLIP signal at high alpha values (α=10 already extreme, α=25 white/blank).
2. **Inaccurate explanations:** User-facing text described "interpolation" and "weighting" — the actual mechanism is extrapolation into unexplored vector space, producing AI hallucinations.
3. **Misleading name:** "Surrealizer" implies stylistic surrealism. The effect is genuine AI hallucination from out-of-distribution vectors.

### Root Causes

1. **Joint vs. individual embeddings:** `pipe.encode_prompt()` returns all three encoders (CLIP-L + CLIP-G + T5) concatenated. Blending two such tensors (one with CLIP active/T5 empty, other reversed) pushes CLIP embeddings toward negative of the prompt (`-19·CLIP(prompt) + 20·CLIP("")`) instead of extrapolating between encoder spaces.
2. **Outdated references:** Configs still mentioned "legacy ComfyUI workflow", "interpolation", "vector interpolation".

### Technical Analysis: How the Hallucinator Works

**SD3.5 has two text encoders relevant to the Hallucinator:**
- **CLIP-L** (77 tokens, 768-dim): Trained on image-text pairs. Maps text to visual features.
- **T5-XXL** (512 tokens, 4096-dim): Trained on text-only tasks. Maps text to linguistic structure.

**The fusion formula (first 77 tokens):**
```
fused = (1 - α) · CLIP-L_padded + α · T5
```

At α=20: `fused = -19·CLIP-L + 20·T5` — the embedding is pushed 19× past T5's representation, into a region of the 4096-dimensional space that the diffusion model never encountered during training. The model must interpret these out-of-distribution vectors, producing genuine hallucinations.

**The semantic anchor (tokens 78-512):**
The remaining T5 tokens are appended unchanged. These keep the image thematically connected to the prompt even as the first 77 tokens push into hallucinatory territory.

**Why extrapolation, not interpolation, matters:**
- Interpolation (0≤α≤1): Smooth blend between two valid representations → mild variations
- Extrapolation (α>1): Pushes PAST both representations → out-of-distribution vectors → hallucination
- The "sweet spot" α=15-35 pushes far enough for visual surprise but not so far that the model loses all coherence

### Fixes Applied

**1. Token-level fusion (diffusers_backend.py)**
- Replaced `pipe.encode_prompt()` calls with individual encoder access: `pipe._get_clip_prompt_embeds()` and `pipe._get_t5_prompt_embeds()`
- CLIP-L (768d) padded to T5 dimension (4096d)
- First 77 tokens: LERP with alpha (enabling extrapolation)
- Remaining T5 tokens: appended unchanged (semantic anchor)
- Negative prompt fused with same alpha (matching ComfyUI workflow)
- All 4 embedding tensors passed to pipeline, bypassing `encode_prompt()` entirely

**2. Comprehensive documentation rewrite**
- All config descriptions: "interpolation" → "extrapolation", removed "legacy ComfyUI" references
- i18n (DE+EN): Explained mechanism accessibly — two AI "brains", extrapolation beyond training data, hallucination
- DokumentationModal: Rewritten with "Why do images become surreal?" section explaining out-of-distribution vectors
- Architecture Part 22: Added complete Diffusers backend section with code, failed approach analysis, design decisions
- Development Decisions: Full technical rationale for individual-encoder approach vs joint-embedding blending

**3. Rename Surrealizer → Hallucinator (display name only)**
- All user-facing names: "Surrealizer/Surrealisierer" → "Hallucinator"
- Config names: "Surrealization" → "Hallucination/Halluzination"
- Button: "Surrealisieren" → "Halluzinieren"
- Slider labels: i18n'd ("extrem", "invers", "normal", "halluziniert", "extrem")
- Internal IDs unchanged: `surrealizer` (config, pipeline, route, Vue filename)

### Files Modified
- `devserver/my_app/services/diffusers_backend.py` — `generate_image_with_fusion()` rewritten with `_fuse_prompt()` helper
- `devserver/schemas/configs/interception/surrealizer.json` — name, description, context, tags, audience
- `devserver/schemas/configs/output/surrealization_diffusers.json` — name, description
- `devserver/schemas/configs/output/surrealization_legacy.json` — name
- `devserver/schemas/chunks/output_image_surrealizer_diffusers.json` — description, alpha_factor docs, meta.notes
- `public/ai4artsed-frontend/src/i18n.ts` — DE+EN surrealizer section rewritten + slider keys
- `public/ai4artsed-frontend/src/views/surrealizer.vue` — i18n'd slider labels, button text, α display
- `public/ai4artsed-frontend/src/components/DokumentationModal.vue` — Hallucinator explanation card
- `docs/ARCHITECTURE PART 22 - Legacy-Workflow-Architecture.md` — Diffusers backend section, technical analysis
- `docs/DEVELOPMENT_DECISIONS.md` — Three decisions documented

### Key Learnings
- `pipe.encode_prompt()` returns **joint** SD3 embeddings (all 3 encoders concatenated). You CANNOT extrapolate between encoder spaces by blending two joint embeddings — you must access individual encoders.
- The surreal/hallucinatory effect comes specifically from **extrapolation** (α>1), not interpolation. At α=20, the model interprets vectors 19× past T5's representation — genuine out-of-distribution inference.
- The remaining T5 tokens (78+) act as a semantic anchor. Without them, the image loses all connection to the prompt.
- Private methods `_get_clip_prompt_embeds()` and `_get_t5_prompt_embeds()` are stable in diffusers v0.36.0 and provide exactly the individual encoder access needed.
- "Hallucinator" is more technically precise than "Surrealizer" — the effect is the model hallucinating from out-of-distribution vectors, not imitating surrealist art style.

---

## Session 161 - Post-Generation VLM Safety Check + Safety Architecture Clarification
**Date:** 2026-02-07
**Focus:** Close the gap between text-based safety checks and actual image content using local VLM (qwen3-vl:2b)
**Status:** COMPLETE

### Problems

1. **Text-based safety checks can't predict image content**: Stage 1+3 check the prompt text, but a harmless prompt can produce a disturbing image. No post-generation content verification existed.
2. **Safety architecture lacked clarity**: DSGVO, §86a StGB, and Jugendschutz were conceptually mixed — different concerns that need to be independently understood.

### Root Causes

1. **No visual verification layer**: The pipeline went directly from Stage 4 output to SSE `complete` event without inspecting the generated image.
2. **"Safety" was treated as one thing**: DSGVO (data protection), §86a (criminal law), and Jugendschutz (age-appropriate content) are three independent legal/pedagogical concerns with different triggers, scopes, and safety levels.

### Fixes Applied

**Feature: Post-Generation VLM Safety Check**
- Added `VLM_SAFETY_MODEL = "qwen3-vl:2b"` to `config.py:135`
- Added `_vlm_safety_check_image()` to `schema_pipeline_routes.py:2162` — reads image from recorder, base64-encodes, sends to Ollama /api/chat with age-appropriate prompt
- Inserted VLM check in `execute_generation_streaming()` between Stage 4 success and `complete` SSE event
- Only triggers for `media_type == 'image'` AND `safety_level in ('kids', 'youth')`
- Fail-open on any error (VLM unavailability never blocks generation)
- SSE event: `blocked` with `stage: 'vlm_safety'` when image flagged as unsafe

**qwen3-vl Thinking Mode Fix**
- qwen3-vl:2b returns analysis in `message.thinking` field, not `message.content`
- `num_predict: 500` needed (100 tokens = thinking cut off mid-sentence, empty content)
- Code checks both `content` and `thinking` fields for "unsafe" keyword

**Documentation: Safety Architecture Clarification**
- Updated architecture docs, design decisions, dev log, Träshy knowledge base
- Documented three independent safety concerns: §86a, DSGVO, Jugendschutz
- Documented VLM check as post-Stage-4 layer

### Files Modified
- `devserver/config.py:135` — `VLM_SAFETY_MODEL` config variable
- `devserver/my_app/routes/schema_pipeline_routes.py:2162` — `_vlm_safety_check_image()` function
- `devserver/my_app/routes/schema_pipeline_routes.py:2455` — VLM check insertion in streaming flow
- `docs/ARCHITECTURE PART 01 - 4-Stage Orchestration Flow.md` — Post-4 VLM check in stage table + flow diagram
- `docs/DEVELOPMENT_DECISIONS.md` — Safety architecture decisions
- `devserver/trashy_interface_reference.txt` — Section 8 rewritten with layered safety explanation
- `docs/reference/safety-architecture-matters.md` — VLM appendix

### Key Learnings
- qwen3 models use "thinking mode" by default — response splits into `message.thinking` (reasoning) and `message.content` (answer). Low `num_predict` exhausts tokens during thinking, leaving `content` empty
- Safety is not monolithic: §86a is **criminal law** (always on), DSGVO is **data protection** (always on), Jugendschutz is **pedagogical** (configurable per safety_level)
- Text safety checks (pre-generation) and VLM checks (post-generation) complement each other: Stage 1+3 = fast guard at the door, VLM = quality control at the exit
- Open question: Video generation needs similar post-generation check (not yet implemented)

---

## Session 160 - Music Gen V2 LLM Functions + Wikipedia Opt-In Refactor
**Date:** 2026-02-06
**Focus:** Fix broken LLM functions in music_generation_v2.vue + architectural refactor of Wikipedia research from opt-out to opt-in
**Status:** COMPLETE

### Problems

1. **SSE Streaming never started** (Theme→Lyrics, Refine Lyrics): Requests never reached the backend
2. **Tag Suggestion produced no result**: Backend ran but output was corrupted by Wikipedia loop
3. **Wikipedia was architecturally backwards**: Hardcoded in `manipulate.json` template, required `skip_wikipedia` flag to disable — opt-out instead of opt-in

### Root Causes

1. **Vue reactivity timing bug**: `runLyricsAction()` set `isLyricsProcessing=true` (triggering v-if mount of MediaInputBox) and `lyricsStreamUrl` in the same synchronous call. MediaInputBox mounted with URL already set — its `watch` on `streamUrl` only fires on changes, not initial values. `startStreaming()` was never called.

2. **Wikipedia loop corrupted output**: The non-streaming path didn't forward `skip_wikipedia` to the pipeline context. The LLM produced `<wiki>` markers (because instructions were in the prompt), triggering 3 Wikipedia iterations that converted the output to an input echo. `parseAndApplyTagSuggestions()` found no JSON → no tags applied.

3. **Architectural smell**: Wikipedia instructions were baked into `manipulate.json` — every `manipulate` chunk call got Wikipedia instructions, even lyrics refinement and tag suggestion. The `skip_wikipedia` flag was a workaround, not a solution.

### Fixes Applied

**Bug Fix A — SSE Streaming (music_generation_v2.vue)**
- `runLyricsAction()` made `async`, resets `lyricsStreamUrl` to `''`, then sets URL via `await nextTick()` so MediaInputBox's watch fires

**Bug Fix B + Architectural Refactor — Wikipedia Opt-In**
- Created `schemas/engine/wikipedia_prompt_helper.py` — Wikipedia instructions as standalone module
- Cleaned `manipulate.json` — removed Wikipedia block and `WIKIPEDIA_CONTEXT` placeholder
- Modified `pipeline_executor.py:_execute_single_step()` — checks `config.meta.get('wikipedia', False)` to conditionally enable Wikipedia instructions + research loop
- Added `"wikipedia": true` to 28 pedagogical interception configs (all except music/code)
- Removed `skip_wikipedia` from entire codebase (frontend v1+v2, backend streaming+non-streaming paths)

### Files Modified
- `public/ai4artsed-frontend/src/views/music_generation_v2.vue` — async runLyricsAction + nextTick
- `public/ai4artsed-frontend/src/views/music_generation.vue` — removed skip_wikipedia
- `devserver/schemas/engine/wikipedia_prompt_helper.py` — NEW: Wikipedia instructions module
- `devserver/schemas/chunks/manipulate.json` — cleaned template
- `devserver/schemas/engine/pipeline_executor.py` — Wikipedia opt-in logic
- `devserver/my_app/routes/schema_pipeline_routes.py` — removed all skip_wikipedia plumbing
- `devserver/schemas/configs/interception/*.json` — 28 configs updated with `"wikipedia": true`

### Key Learnings
- Vue `watch()` without `{ immediate: true }` does NOT fire on initial mount values — critical when combining `v-if` conditional rendering with reactive prop changes
- Architectural opt-out patterns (features that must be explicitly disabled) are fragile — prefer opt-in (features must be explicitly enabled)
- Wikipedia research is a pedagogical feature specific to `text_transformation.vue` art configs, not a universal pipeline feature

---

## Session 159 - HeartMuLa Production CUDA Crash Fix
**Date:** 2026-02-06
**Focus:** Fix HeartMuLa music generation crashing in production but not dev
**Status:** COMPLETE

### Problem

HeartMuLa worked in dev (port 17802) but crashed in production (port 17801) with CUDA "index out of bounds" during codec detokenization. Investigation revealed two layered issues.

### Root Causes

1. **Redundant `torch.autocast` wrapper** (`heartmula_backend.py:279`)
   - Wrapped entire `self._pipeline()` call in `torch.autocast("cuda", dtype=torch.bfloat16)`
   - Forced bfloat16 on HeartCodec which requires float32
   - heartlib already handles autocast internally for MuLa generation only (`music_generation.py:284, 318`)
   - Different PyTorch nightly versions (dev: Feb 3, prod: Feb 4) determined whether this crashed or not

2. **Vocab/codebook size mismatch in heartlib** (the actual bug)
   - MuLa `audio_vocab_size` = 8197, codec `codebook_size` = 8192
   - MuLa can generate token indices 8192-8196 that exceed codec's embedding range
   - Causes CUDA index out of bounds in `vq_embed.get_output_from_indices()`
   - Data-dependent: longer generations and certain lyrics/tag combinations trigger it more often
   - No bounds checking existed anywhere in the heartlib pipeline

### Fixes Applied

1. **Removed outer autocast** — `heartmula_backend.py:279`: `torch.autocast` removed, `torch.no_grad()` kept
2. **Index clamping in heartlib** — `flow_matching.py:75`: `torch.clamp(codes, 0, codebook_size - 1)` before embedding lookup
3. **Tightened Vue slider ranges** — `music_generation_v2.vue`: Top-K max 200→100, CFG max 5.0→4.0
4. **Updated installation docs** — `INSTALLATION.md`: documented post-install clamp fix and PyTorch version sensitivity

### Files Modified
- `devserver/my_app/services/heartmula_backend.py` — removed autocast
- `public/ai4artsed-frontend/src/views/music_generation_v2.vue` — slider limits
- `docs/installation/INSTALLATION.md` — heartmula setup warnings
- `~/ai/heartlib/src/heartlib/heartcodec/models/flow_matching.py` — index clamping (separate repo)

### Key Learnings
- heartlib manages dtype internally — never wrap `self._pipeline()` in external autocast
- PyTorch nightly builds are fragile on Blackwell GPUs — even 1 day difference breaks things
- MuLa vocab > codec codebook is a design flaw in heartlib; clamp is a necessary workaround

---

## Session 158 - Music Generation Unified Page (Simple/Advanced Toggle)
**Date:** 2026-02-06
**Focus:** Unify V1 and V2 music generation into single page with mode toggle
**Status:** COMPLETE ✅

### Summary

Created a unified music generation page that offers both the simple (V1) and advanced (V2) interfaces via a toggle switch. This addresses the pedagogical trade-off: V1 is accessible but teaches nothing about music; V2 teaches music vocabulary but has complex ML parameters.

### Pedagogical Rationale

**V1 (Simple Mode):**
- Dual text inputs: lyrics + free-form tags
- Low barrier to entry
- No scaffolding for students without lyrics
- Tags as free-text requires prior knowledge

**V2 (Advanced Mode):**
- Lyrics Workshop: "Theme → Lyrics" scaffold for students without lyrics
- Sound Explorer: 8-dimension chip selector teaches music vocabulary (genre, timbre, mood, instruments, etc.)
- "Suggest from Lyrics" bridges lyrics→sound understanding
- ML parameters (temp, topk, cfg) now with good defaults
- Custom tags input for power users

### Key Changes

1. **Unified Wrapper** (`music_generation_unified.vue`)
   - Sticky toggle at top: Simple / Advanced
   - Mode persists to localStorage
   - Background adapts (solid #0a0a0a for V1, gradient for V2)

2. **Router Update**
   - `/music-generation` → unified page
   - `/music-generation-simple` → direct V1 access
   - `/music-generation-advanced` → direct V2 access

3. **V2 Defaults Updated**
   - Audio length: 120s → 200s (3:20)
   - Top-K: 70 → 65
   - CFG Scale: 3.0 → 2.75

4. **Custom Tags Feature**
   - MusicTagSelector now accepts `v-model:customTags`
   - Text input below chip grid
   - Merged into compiledTags output

### Files Created
- `public/ai4artsed-frontend/src/views/music_generation_unified.vue`

### Files Modified
- `public/ai4artsed-frontend/src/router/index.ts` — unified route
- `public/ai4artsed-frontend/src/components/MusicTagSelector.vue` — custom tags
- `public/ai4artsed-frontend/src/views/music_generation_v2.vue` — new defaults + customTags
- `public/ai4artsed-frontend/src/i18n.ts` — simpleMode, advancedMode, customTags keys

---

## Session 157 - Music Generation V2 (Lyrics Workshop + Sound Explorer)
**Date:** 2026-02-05
**Focus:** New music generation UI concept + HeartMuLa tag system + performance tuning
**Status:** COMPLETE ✅

### Summary

Redesigned the music generation frontend as a dual creative-process workbench. Investigated why user-selected genres were ineffective and fixed the root causes (LLM prompt bias + tag format). Added generation parameter controls and batch mode.

### Key Achievements

1. **Root Cause Analysis: Ineffective Genre Tags**
   - Verified genre injection works end-to-end (frontend → chunk_builder → heartmula_backend → model)
   - Found 2 bugs: (A) `tags_generation.json` LLM prompt hardcoded ROBOTIC/ELECTRONIC bias, overriding user genre; (B) Tag format used spaces after commas, HeartMuLa expects no spaces
   - Researched HeartMuLa tag system: 8 dimensions with training probabilities (Genre 0.95, Timbre 0.5, Gender 0.375, Mood 0.325, Instrument 0.25, Scene 0.2, Region 0.125, Topic 0.1)

2. **Music Generation V2 Page** (`music_generation_v2.vue`)
   - **Process A: Lyrics Workshop** — User writes theme/keywords/lyrics, clicks [Thema→Lyrics] or [Lyrics verfeinern], LLM streams result
   - **Process B: Sound Explorer** — 8-dimension chip selector (MusicTagSelector.vue), LLM auto-suggest from lyrics, manual toggle
   - New interception configs: `lyrics_from_theme.json`, `tag_suggestion_from_lyrics.json`
   - Hidden route (`/music-generation-v2`) — workbench tool, not pedagogical feature
   - Original `music_generation.vue` untouched at `/music-generation`

3. **Generation Controls**
   - Temperature, Top-K, CFG Scale sliders with injection via `custom_placeholders`
   - Batch mode: `[-] Nx [+]` counter, sequential generation runs
   - Updated defaults: Top-K 50→70, CFG Scale 1.5→3.0 (all 6 locations)

4. **Backend Performance**
   - Tag format sanitization in `output_music_heartmula.py` (strip spaces, underscores for multi-word)
   - `torch.autocast("cuda", dtype=torch.bfloat16)` for fused RMS-norm kernels (fixes dtype mismatch warning)
   - `torch.compile()` tested but incompatible with heartlib (crashes on batch) — removed
   - Server `channel_timeout=600` for long music generation requests

### Files Created
- `public/ai4artsed-frontend/src/views/music_generation_v2.vue`
- `public/ai4artsed-frontend/src/components/MusicTagSelector.vue`
- `devserver/schemas/configs/interception/lyrics_from_theme.json`
- `devserver/schemas/configs/interception/tag_suggestion_from_lyrics.json`

### Files Modified
- `devserver/schemas/chunks/output_music_heartmula.py` — tag normalization, new defaults
- `devserver/my_app/services/heartmula_backend.py` — autocast bfloat16, new defaults
- `devserver/schemas/configs/output/heartmula_standard.json` — new defaults
- `devserver/schemas/engine/backend_router.py` — new defaults
- `devserver/server.py` — channel_timeout=600
- `public/ai4artsed-frontend/src/router/index.ts` — v2 route
- `public/ai4artsed-frontend/src/i18n.ts` — musicGenV2 keys (DE + EN)

---

## Session 156 - HeartMuLa Integration (Python-Chunk Pattern)
**Date:** 2026-02-02 to 2026-02-03
**Focus:** First Python-based Output-Chunk Implementation
**Status:** COMPLETE ✅

### Summary

Implemented HeartMuLa music generation as the **first Python-based Output-Chunk**, establishing a new standard pattern that replaces deprecated JSON chunks for non-ComfyUI backends.

### Key Achievements

1. **Python-Chunk Pattern Established**
   - Created `output_music_heartmula.py` - self-contained executable chunk
   - Chunk contains complete backend logic (no central router delegation)
   - Uses async `execute()` function with type-safe parameters
   - Returns bytes (MP3 audio) directly

2. **Architecture Decision: Generic backend_type**
   - ALL Python chunks use `backend_type='python'` (generic)
   - Chunk name determines implementation (e.g., `output_music_heartmula`)
   - No per-backend enum entries needed (extensible design)
   - Removed HEARTMULA enum (redundant)

3. **New Pipeline Created**
   - `dual_text_media_generation.json` - 2 text inputs → media output
   - Uses `{{OUTPUT_CHUNK}}` placeholder (resolved from config)
   - No Proxy-Chunk pattern (direct chunk selection)
   - Skip Stage 2 (passthrough to Stage 4)

4. **Implementation Fixes (5 Errors Resolved)**
   - Placeholder resolution in config_loader.py
   - Output-Chunk detection in backend_router.py
   - Python chunk detection in chunk_builder.py
   - BackendResponse content="" for error cases
   - BackendType.PYTHON enum added

5. **Documentation Updates**
   - ARCHITECTURE PART 03: Added Python-Chunk section (Type 2B)
   - ARCHITECTURE PART 05: Updated routing documentation
   - JSON chunks declared DEPRECATED (use only for pure ComfyUI passthrough)
   - Created ARCHITECTURE_VIOLATION_ProxyChunkPattern.md

### Technical Details

**Input:** 2 text fields (lyrics + style tags)
**Output:** MP3 audio bytes
**Backend:** heartlib (external Python library)
**Pipeline:** `dual_text_media_generation` → `output_music_heartmula`

**Files Created:**
- `devserver/schemas/chunks/output_music_heartmula.py`
- `devserver/schemas/pipelines/dual_text_media_generation.json`
- `docs/ARCHITECTURE_VIOLATION_ProxyChunkPattern.md`

**Files Modified:**
- `devserver/schemas/engine/backend_router.py` - Python chunk execution
- `devserver/schemas/engine/config_loader.py` - Placeholder resolution
- `devserver/schemas/engine/chunk_builder.py` - Python chunk detection
- `devserver/schemas/configs/output/heartmula_standard.json`
- `public/ai4artsed-frontend/src/views/music_generation.vue`
- `docs/ARCHITECTURE PART 03 - ThreeLayer-System.md`
- `docs/ARCHITECTURE PART 05 - Pipeline-Chunk-Backend-Routing.md`
- `docs/devserver_todos.md`

### Commits

- `958ae0d` - feat(chunks): Implement Python-based Output-Chunks (HeartMuLa reference)
- `e0ccfd6` - docs: Document architecture violations and Python-Chunk standard
- `5d24a15` - fix(config-loader): Resolve placeholders in pipeline chunk names
- `d8f420f` - fix(chunks): Complete Python-Chunk routing and execution flow
- `a3354ff` - fix(backend): Add PYTHON to BackendType enum
- `c03499a` - docs(architecture): Complete Python-Chunk documentation + backend fixes

### Testing Status

✅ Python chunk loads and executes correctly
✅ Backend routing works (backend_type='python')
✅ Parameters passed correctly (11 parameters verified)
✅ Error handling works (heartlib not installed - expected)
❌ Actual music generation - requires heartlib installation

### Future Work

- Install heartlib for actual music generation
- Refactor existing Diffusers chunks to Python pattern
- Remove deprecated JSON-based Output-Chunks (if no ComfyUI passthrough needed)
- Migrate Proxy-Chunk pattern in `single_text_media_generation`

### Notes

This session established the **reference implementation** for all future Python-based backends. The pattern is simpler, more maintainable, and more extensible than JSON chunks.

---

## Session 155 - Z-Image Integration Attempt
**Date:** 2026-02-02
**Focus:** Z-Image 6B Model Evaluation
**Status:** REJECTED

### Summary

Attempted integration of Z-Image (Alibaba Tongyi Lab) as new image generation model.

### Actions Taken

1. **Session 150**: Downloaded Z-Image models for ComfyUI (~28GB safetensors)
2. **Session 155**: Switched to Diffusers backend (ZImagePipeline)
3. **Testing**: Model produces content unsafe for educational platform

### Decision

**Z-Image wird nicht angeboten.** Das Modell generiert Inhalte, die für die pädagogische Plattform AI4ArtsEd ungeeignet sind.

### Cleanup

- All Z-Image configs removed
- Model files deleted from SwarmUI
- Commits: `2fb94fb` (integration), `9fedf40` (revert)

---

## Session 154 - Canvas Parameter Injection System
**Date:** 2026-02-01
**Focus:** Parameter Nodes (Seed, Resolution, Quality) + Input Handling Fixes
**Status:** COMPLETED

### Overview

Complete parameter injection system for Canvas workflows, enabling users to control generation parameters (seed, resolution, steps, cfg) through visual nodes.

### Architecture: Parameter Injection Flow

```
┌─────────────────────────────────────────────────────────────────────┐
│  Frontend (Canvas)                                                   │
│  ├─ Parameter Nodes (Seed, Resolution, Quality)                     │
│  ├─ Values stored in CanvasNode properties                          │
│  └─ Passed to backend via /api/canvas/execute-stream                │
└─────────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────────┐
│  Backend (canvas_executor.py)                                        │
│  ├─ Parameter nodes executed as source nodes (no propagation)       │
│  ├─ Generation node collects params from connected parameter nodes  │
│  └─ Calls execute_stage4_generation_only() with params              │
└─────────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────────┐
│  Pipeline Executor                                                   │
│  ├─ Params injected via custom_placeholders                         │
│  ├─ Only params in config's input_mappings are applied              │
│  └─ Unsupported params gracefully ignored                           │
└─────────────────────────────────────────────────────────────────────┘
```

### New Node Types

| Node | Type | Color | Purpose |
|------|------|-------|---------|
| **Seed** | `seed` | Teal (#14b8a6) | Control random seed for reproducibility |
| **Resolution** | `resolution` | Teal (#14b8a6) | Set width/height dimensions |
| **Quality** | `quality` | Teal (#14b8a6) | Set steps and CFG scale |

### Seed Node Modes

| Mode | Behavior |
|------|----------|
| `fixed` | Use specified seed value (default: 123456789) |
| `random` | Generate random seed per execution |
| `increment` | Base seed + batch index (for batch runs) |

### Resolution Presets

| Preset | Dimensions | Use Case |
|--------|------------|----------|
| `square_1024` | 1024 × 1024 | SD3.5 default |
| `portrait_768x1344` | 768 × 1344 | Portrait images |
| `landscape_1344x768` | 1344 × 768 | Landscape images |
| `custom` | User-defined | Any resolution |

### Parameter Support Matrix

| Parameter | SD3.5 | Flux2 | GPT-Image | Gemini | ACEnet | Wan22 |
|-----------|:-----:|:-----:|:---------:|:------:|:------:|:-----:|
| `seed` | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ |
| `width` | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ |
| `height` | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ |
| `steps` | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ |
| `cfg` | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ |

**Note:** API-based backends (GPT-Image, Gemini) ignore ComfyUI parameters.

### Bug Fixes

1. **Parameter nodes not propagating** - Fixed `_find_source_nodes()` to recognize seed/resolution/quality
2. **Double execution** - Parameter nodes now return `[]` from `_get_next_nodes()`
3. **Input events not firing** - Changed `@blur/@keyup.enter` to `@input` for immediate updates
4. **Node drag on input click** - Added `@mousedown.stop` to all parameter node inputs
5. **Wrong default seed** - Changed from 42 to 123456789 (system standard)

### useGenerationStream Integration

Other views can inject parameters via the composable:

```typescript
import { useGenerationStream } from '@/composables/useGenerationStream'

const { startGeneration } = useGenerationStream()

await startGeneration({
  prompt: 'A sunset over mountains',
  outputConfig: 'sd35_large',
  safetyLevel: 'youth',
  // Parameter injection:
  width: 1920,
  height: 1080,
  steps: 30,
  cfg: 7.0
})
```

### Modified Files

| File | Changes |
|------|---------|
| `types/canvas.ts` | Added `seed`, `resolution`, `quality` to StageType + NODE_TYPE_DEFINITIONS |
| `stores/canvas.ts` | Node property defaults |
| `components/canvas/StageModule.vue` | Node UI templates + event handling fixes |
| `components/canvas/ModulePalette.vue` | Added nodes to "Tools" category |
| `views/canvas_workflow.vue` | Event handlers for new nodes |
| `composables/useGenerationStream.ts` | Extended GenerationParams interface |
| `services/canvas_executor.py` | `_execute_seed()`, `_execute_resolution()`, `_execute_quality()` |
| `routes/schema_pipeline_routes.py` | Extended SSE endpoint + `execute_stage4_generation_only()` |

### Commits

- `1dca36c` - fix(edutainment): Show summary when first loop completes OR 10s elapsed
- `367c4b4` - fix(canvas): Fix parameter node inputs not updating values
- `d2d246a` - fix(canvas): Fix all parameter node input handling

---

## Session 153 - Localhost Auto-Login for Settings
**Date:** 2026-01-31
**Focus:** Skip password authentication for local development
**Status:** COMPLETED

### Problem

- Browser password managers don't work reliably with `localhost:17802` (no proper domain)
- No password recovery mechanism
- Unnecessary friction during local development

### Solution

Auto-login for localhost requests, similar to ComfyUI's approach.

### Implementation

Added `is_localhost_request()` helper that checks:
- `request.remote_addr` in `('127.0.0.1', '::1')`
- `request.host` in `('localhost', '127.0.0.1')`

Modified `require_settings_auth` decorator to bypass authentication for localhost.
Modified `/api/settings/check-auth` to return `auto_login: true` for localhost.

### Security

- Production access via `lab.ai4artsed.org` still requires password
- Only true localhost connections are auto-authenticated
- `request.remote_addr` is set by Flask/Werkzeug, not user-manipulable

### Modified Files

| File | Change |
|------|--------|
| `routes/settings_routes.py` | `is_localhost_request()`, decorator bypass, check-auth update |

### Commit

`af9b988` - feat(settings): Auto-login for localhost development

---

## Session 152 - Tone.js Browser-Based Music Generation
**Date:** 2026-01-31
**Focus:** Add Tone.js as new output option for browser-based music synthesis
**Status:** COMPLETED

### Overview

Following the p5.js pattern for generative graphics, Tone.js enables browser-based music generation. The LLM generates Tone.js JavaScript code that runs directly in the frontend via iframe.

### Architecture Pattern: Code Generation Output

| Component | p5.js (Graphics) | Tone.js (Audio) |
|-----------|------------------|-----------------|
| Chunk Type | `text_passthrough` | `text_passthrough` |
| Backend Type | `vue_p5js` | `vue_tonejs` |
| Media Type | `code` | `code` |
| Language | JavaScript | JavaScript |
| Execution | Browser Canvas | Web Audio API |

**Key Difference from Binary Media:**
- No ComfyUI/SwarmUI workflow execution
- LLM generates code directly (Claude Sonnet 4.5 via OpenRouter)
- Code passed through to frontend for browser execution
- No server-side rendering required

### New Files

| File | Description |
|------|-------------|
| `schemas/chunks/output_code_tonejs.json` | Passthrough chunk for Tone.js code |
| `schemas/configs/output/tonejs_code.json` | Output config with LLM prompt |
| `schemas/configs/interception/tonejs_composer.json` | Stage 2: Text → Musical structure |

### Modified Files

| File | Change |
|------|--------|
| `routes/schema_pipeline_routes.py` | Media type detection for 'tonejs' (4 locations) |
| `routes/media_routes.py` | New route `/api/media/tonejs/<run_id>` |
| `text_transformation.vue` | Tone.js config bubble, iframe with Play/Stop |
| `text_transformation.css` | Styles for `.tonejs-iframe` |
| `canvas.ts` | `tonejs_composer` in InterceptionPresets |
| `THIRD_PARTY_CREDITS.md` | p5.js (LGPL-2.1) and Tone.js (MIT) licenses |

### Stage 2 Interception: Music Composer

Transforms text descriptions into layered musical structure:
```
RHYTHM: [Drums, percussion patterns]
BASS: [Bass line characteristics]
HARMONY: [Chord progressions, pads]
MELODY: [Lead lines, hooks]
TEXTURE: [Ambient elements, effects]
```

### Frontend: Tone.js Player

- Play/Stop buttons (required due to browser audio policy)
- Visual feedback (animated bars visualizer)
- Status indicator (German: "Klicke Play um die Musik zu starten")
- Code editor for viewing/editing generated code

### Third-Party Libraries (CDN)

- **p5.js** v1.7.0 - LGPL-2.1 - Creative coding for graphics
- **Tone.js** v14.8.49 - MIT - Web Audio framework for music

### Commit

`0e7bd45` - feat(audio): Add Tone.js browser-based music generation

---

## Session 151 - Edutainment Animations: Environmental Impact Visualization
**Date:** 2026-01-31
**Focus:** Wissenschaftlich fundierte CO2-Visualisierungen für AI-Generierung
**Status:** IN PROGRESS

### Neue Features

**1. IcebergAnimation - Arktis-Eis-Schmelze**
- Zeichne Eisberge, beobachte Schmelzen während GPU arbeitet
- Segelschiff als Fortschrittsanzeige (links→rechts)
- Abschluss-Info: CO2-Menge → cm³ geschmolzenes Arktis-Eis
- Berechnung: 1g CO2 = 6 cm³ Eis (basierend auf Notz & Stroeve 2016)

**2. ForestMiniGame - Interaktives Baumpflanzen**
- Spiel: Pflanze Bäume gegen Fabriken (GPU-Power = Fabrik-Spawn-Rate)
- 7 Baumtypen (Kiefer, Fichte, Tanne, Eiche, Birke, Ahorn, Weide)
- Vogel als Fortschrittsanzeige (Sprite-Animation, Open Source)
- Fabrik auf Baum → Fabrik verschwindet
- Abschluss: Baum-CO2-Absorption (22kg/Jahr = 2.51g/Stunde)

**3. PixelAnimation - GPU-Stats + Smartphone-Vergleich**
- Echtzeit GPU-Werte: Grafikkarte, Energieverbrauch, CO2
- Abschluss: "Du müsstest Dein Handy X Minuten ausschalten"
- Berechnung: 5W Smartphone × 400g CO2/kWh = 30 min pro g CO2

### Wissenschaftliche Grundlagen

| Metrik | Wert | Quelle |
|--------|------|--------|
| Baum CO2-Absorption | 22 kg/Jahr | EPA, Arbor Day Foundation |
| Arktis-Eisverlust | 3 m²/Tonne CO2 | Notz & Stroeve, Science 2016 |
| Eisdicke (Durchschnitt) | ~2m | → 6 m³/Tonne = 6 cm³/g |
| Smartphone Standby | ~5W | Energiestudien |
| Deutscher Strommix | ~400g CO2/kWh | Umweltbundesamt |

### Pädagogische Intention

**Sichtbarmachung unsichtbarer Kosten:**
- Abstrakte Zahlen (Watt, Gramm) → greifbare Metaphern
- Eisschmelze → globale Klimafolgen
- Bäume → lokale Ökosysteme
- Smartphone → persönlicher Alltag

**Handlungsbezug (ForestMiniGame):**
- Aktives Gegenhandeln während Verbrauch läuft
- Spielerische Reflexion: "Kann ich schneller pflanzen?"
- Nicht moralisierend, sondern informierend

### Modified Files

| File | Change |
|------|--------|
| `IcebergAnimation.vue` | Neuer Vergleich (cm³ Eis statt Baum-Stunden) |
| `ForestMiniGame.vue` | Vogel-Animation, Abschluss-Overlay |
| `SpriteProgressAnimation.vue` | GPU-Stats, Smartphone-Vergleich |
| `i18n.ts` | Neue Übersetzungen (iceberg, forest) |
| `ARCHITECTURE PART 15.md` | Design Decision #9 |
| `THIRD_PARTY_CREDITS.md` | Vogel-Sprite Lizenz |

### Integration in MediaOutputBox

**RandomEdutainmentAnimation.vue** - Wrapper-Komponente:
- Wählt zufällig eine der 3 Animationen bei Mount
- Bleibt während gesamter Generation konstant
- Empfängt nur `progress` prop

**Änderung in MediaOutputBox:**
- Ersetzt `SpriteProgressAnimation` durch `RandomEdutainmentAnimation`
- Alle Views die MediaOutputBox nutzen erhalten automatisch die neuen Animationen

### Fixes
- **Fehlende Eis-Zahl:** IcebergAnimation startet jetzt Energy-Tracking automatisch wenn progress > 0
- **Falsche Vergleiche:** ForestMiniGame nutzt jetzt `forest.comparison` (Baum), nicht `iceberg.comparison` (Eis)

### Commits
- `4d70f26` feat(forest): Add bird progress indicator and summary overlay

---

## Session 148 - "Translated" Badge mit SSE-Streaming
**Date:** 2026-01-30
**Focus:** Echtzeit-Badge via SSE wenn Prompt übersetzt wird (vor Bildgenerierung)
**Status:** COMPLETED

### Feature
Neues "Translated" Badge (→ EN) neben dem Safety-Approved Badge. Erscheint in **Echtzeit** nach Stage 3, **bevor** die Bildgenerierung beginnt. Ermöglicht durch SSE-Streaming des `/generation` Endpoints.

### Architektur: SSE für `/generation`

**Problem (ursprünglich):**
- Badge erschien erst nach kompletter Generation (zu spät)
- Safety-Badge wurde mit Fake-300ms-Delay gezeigt (nicht akkurat)

**Lösung:**
- Neuer SSE-Modus für `/generation` Endpoint
- Backend emittiert Events stage-by-stage
- Frontend zeigt Badges sobald `stage3_complete` Event eintrifft

### Backend Änderungen

**`schema_pipeline_routes.py`:**

1. **Neue Generator-Funktion** `execute_generation_streaming()`:
   - Emittiert: `connected`, `stage3_start`, `stage3_complete`, `stage4_start`, `complete`, `error`, `blocked`
   - `stage3_complete` enthält `{safe: bool, was_translated: bool}`

2. **Endpoint erweitert** (`/pipeline/generation`):
   - Unterstützt jetzt GET + POST
   - `enable_streaming=true` → SSE Response
   - Fallback: Original JSON Response

### Frontend Änderungen

**Neuer Composable** `useGenerationStream.ts`:
- Shared SSE-Handler für alle 3 Views
- Exponiert: `showSafetyApprovedStamp`, `showTranslatedStamp`, `generationProgress`, `currentStage`
- Methode: `executeWithStreaming(params)` → Promise\<GenerationResult\>
- Progress-Animation startet bei `stage4_start` Event

**Aktualisierte Views:**
- `text_transformation.vue`
- `image_transformation.vue`
- `multi_image_transformation.vue`

Alle nutzen jetzt den Composable statt lokaler Refs.

### Design-Entscheidungen
- **Icon**: `→ EN` (Pfeil + Text) - neutral, keine politische Konnotation
- **Farbe**: Teal (#009688) - unterscheidet sich von Grün (Safety), Lila (LoRA), Blau (Wikipedia)
- **Timing**: Badges erscheinen nach Stage 3, bevor Stage 4 startet

### Scope

| View | Endpoint | Badge |
|------|----------|-------|
| text_transformation | `/generation` | ✅ SSE |
| image_transformation | `/generation` | ✅ SSE |
| multi_image_transformation | `/generation` | ✅ SSE |
| surrealizer | `/legacy` | ❌ (kein Stage 3) |
| split_and_combine | `/legacy` | ❌ (kein Stage 3) |
| canvas_workflow | eigene Architektur | ❌ (separat) |

### Modified Files
| File | Change |
|------|--------|
| `devserver/my_app/routes/schema_pipeline_routes.py` | SSE-Modus für /generation |
| `public/.../composables/useGenerationStream.ts` | NEU - Shared SSE Composable |
| `public/.../views/text_transformation.vue` | Composable + Icon "→ EN" |
| `public/.../views/text_transformation.css` | `.translated-stamp` Styling |
| `public/.../views/image_transformation.vue` | Composable + Badge |
| `public/.../views/multi_image_transformation.vue` | Composable + Badge |

---

## Session 147 - Documentation Updates: Canvas & Pedagogical Framework
**Date:** 2026-01-29
**Focus:** Vollständige Canvas-Dokumentation, pädagogisches Framework, DokumentationModal-Erweiterungen
**Status:** COMPLETED

### Neue Dokumentation

**ARCHITECTURE PART 26 - Canvas-Workflow-System.md**
- Vollständige technische Dokumentation des Canvas Workflow Systems
- Alle 10 Node-Typen (inkl. neu: Comparison Evaluator)
- Connection Rules, Workflow Execution, Presets
- **Pedagogical Framework**: Lehrforschung, Dekonstruktion des Kontroll-Paradigmas, Prompt Interception, rekursiv-reflexive Workflows
- Zielgruppen, Verhältnis zu anderen Views

### DokumentationModal Änderungen

**Tabs neu organisiert:**
- FAQ-Tab aufgelöst und Inhalte verteilt
- Neue Reihenfolge: Willkommen, Anleitung, Pädagogik, Workshop, Experimente, Canvas

**Neue Inhalte:**
- **Canvas-Tab**: Pädagogisches Framework, Node-Typen mit Stage-Farbcodierung
- **Workshop-Tab**: LLM-Konfiguration (lokal/extern/DSGVO)
- **Pädagogik-Tab**: Prinzip 2 erweitert um "Prompt Interception"
- **Willkommen-Tab**: Datenschutz-Info, Kontakt

**Icons:**
- Canvas-Icon in Navigation: account_tree (korrekter Material Icons Pfad)
- Select-Icon in Anleitung: apps Icon (48px)

### Modified Files
| File | Change |
|------|--------|
| `docs/ARCHITECTURE PART 26 - Canvas-Workflow-System.md` | NEU - Vollständige Canvas-Dokumentation |
| `docs/00_MAIN_DOCUMENTATION_INDEX.md` | Part 26 hinzugefügt |
| `public/.../components/DokumentationModal.vue` | Canvas-Tab, Tab-Reorganisation, Icons |
| `public/.../src/App.vue` | Canvas-Icon (account_tree) |
| `public/.../composables/usePageContext.ts` | Träshy x: 2→8 (ungelöst) |
| `public/.../views/canvas_workflow.vue` | height: 100vh→100% |

---

## Session 144 - Interception Config Revision: Analog Photography 1970s
**Date:** 2026-01-28
**Focus:** Config-Text für detailliertere Objektbeschreibung optimiert
**Status:** COMPLETED

### Änderungstyp: Prompt-Strategie für Wikipedia-Integration

Die Analogfotografie-Config wurde überarbeitet, um die **Objektbeschreibungs-Qualität** zu verbessern - relevant für die neue Wikipedia-Suchfunktion (Session 142/143).

**Kernänderungen:**
1. **Neuer Fokus auf Recherche**: "Du arbeitest sorgfältig und informierst dich sehr genau über Deine Gegenstände. Du beschriebst sie extrem detailliert als fotografische Visualisierung"
2. **Entfernt**: Detaillierte Dunkelkammer-Terminologie (Zonenfokussierung, Push/Pull-Entwicklung, Papiergradwahl etc.) - diese technischen Details sind für die Bildgenerierung weniger relevant als präzise Objektbeschreibungen
3. **Vereinfacht**: Verbotene Sprache auf Kernpunkte reduziert

### Implikation für Config-Revision
Diese Änderung zeigt ein Muster für die Überarbeitung anderer Interception-Configs:
- **Weniger**: Stilistische/technische Detailverliebtheit im Prompt
- **Mehr**: Explizite Anweisung zur genauen Recherche und Beschreibung der Eingabe-Objekte
- **Ziel**: Bessere Wikipedia-Suchbegriffe durch detailliertere Objektnennung im Output

### Modified Files
| File | Change |
|------|--------|
| `devserver/schemas/configs/interception/analog_photography_1970s.json` | `context.de` neu formuliert |

---

## Session 143 - Wikipedia Opensearch API + Transparency
**Date:** 2026-01-28
**Focus:** Fix Wikipedia badge not appearing due to failed lookups
**Status:** COMPLETED

### Problem
Wikipedia badge never appeared because all lookups returned 404:
```
[WIKI] Looking up 'Igbo New Yam Festival' on en.wikipedia.org
[WIKI] Not found: 'Igbo New Yam Festival'
```

**Root Cause:**
- LLM generated search terms (e.g., "Igbo New Yam Festival") that don't match exact Wikipedia article titles
- Backend used direct Page Summary API (`/page/summary/{term}`) which requires exact title
- The actual article exists as "New Yam Festival" but wasn't found

### Solution
**1. Backend: Opensearch API as primary search**

Query construction:
```
https://{lang}.wikipedia.org/w/api.php?action=opensearch&search={term}&limit=1&format=json
```

Response format:
```json
[searchTerm, [titles], [descriptions], [urls]]
["Igbo New Yam", ["New Yam Festival"], ["The New Yam Festival..."], ["https://en.wikipedia.org/wiki/New_Yam_Festival"]]
```

Flow:
1. Opensearch finds best matching article title (fuzzy matching)
2. Page Summary API fetches full content using found title
3. Returns real Wikipedia URL and title

**2. Backend: Send ALL terms (transparency)**
- Removed `if r.success` filter in `pipeline_executor.py`
- All lookup attempts are now sent to frontend
- Frontend knows what was searched, even if not found

**3. Frontend: Visual distinction**
- Found articles: Blue link (clickable)
- Not found: Gray italic text with "(nicht gefunden)"
- Badge shows total count as "N Begriff(e)" instead of "N Artikel"

### Modified Files
| File | Change |
|------|--------|
| `devserver/my_app/services/wikipedia_service.py` | Replaced direct lookup with Opensearch API + Page Summary |
| `devserver/schemas/engine/pipeline_executor.py` | Removed success filter, send all terms |
| `public/.../text_transformation.vue` | Badge shows found/not-found distinction |
| `public/.../text_transformation.css` | Added `.wikipedia-not-found` styling |

### Commits
- `ea9c5cf` - feat(wikipedia): Use Opensearch API for fuzzy matching + show all terms

### Result
- Badge now appears (shows all searched terms)
- Fuzzy matching finds articles even with inexact search terms
- Transparent: User sees what was searched and whether it was found
- Links point to real Wikipedia articles

---

## Session 142 - Wikipedia Badge Position Fix
**Date:** 2026-01-27
**Focus:** Fix disappearing Wikipedia badge by moving to stable UI area
**Status:** COMPLETED

### Problem
Wikipedia badge disappeared after 3 seconds during multiple Wikipedia lookups:
1. First lookup complete: `terms: [{...}]` → Badge appears ✓
2. Second lookup start: `terms: []` → Badge disappears ✗ (SSE event overwrote terms array)
3. Second lookup complete: `terms: [{...}]` → Badge reappears

**Root Cause:**
- Badge positioned in interception section (unstable during SSE streaming)
- `handleWikipediaLookup()` replaced terms array on "start" event: `wikipediaData.value = { active: true, terms: [] }`

### Solution
**Moved badge to stable area** next to Start #1 button:
- No movement during streaming
- Not affected by SSE events
- Visible immediately after first Wikipedia lookup
- Persists during all subsequent lookups

**Fixed data handling:**
- `handleWikipediaLookup()`: Accumulates terms instead of replacing
- `runInterception()`: Full reset only at start of new run

### Modified Files
| File | Change |
|------|--------|
| `public/ai4artsed-frontend/src/views/text_transformation.vue` | Removed badge from interception section (line 83-112); Added badge to Start #1 button container; Fixed handleWikipediaLookup to accumulate terms; wikipediaStatusText shows live status in loading message |

### Commits
- `ab4e37c` - feat(wikipedia): Add status text and dynamic loading message
- `4294bcd` - fix(wikipedia): Move badge to stable area next to Stage 1
- `e1968c0` - fix(wikipedia): Move badge from Start #2 to Start #1

### Result
- Badge appears when first Wikipedia lookup completes
- Badge stays visible during subsequent lookups
- Terms accumulate (e.g., "3 Artikel")
- Badge resets only on new interception run
- Stable position next to Start #1 button

---

## Session 141 - Canvas SSE Streaming for Live Execution Progress
**Date:** 2026-01-27
**Focus:** Real-time progress feedback during canvas workflow execution
**Status:** COMPLETED

### Problem
Users see nothing for 5+ minutes while canvas workflow runs. Terminal shows useful progress info (`[Canvas Tracer] Executing...`) but frontend only updates after everything completes.

### Solution
SSE (Server-Sent Events) streaming endpoint that yields events IMMEDIATELY during execution.

### Key Implementation Detail
**Iterative work-queue instead of recursion** - Critical architectural change:
- Recursive `trace()` function cannot `yield` (nested function limitation)
- Replaced with explicit work-queue loop in generator
- `yield` now happens DIRECTLY in main generator between node executions

### SSE Events
| Event | Data | When |
|-------|------|------|
| `started` | `{total_nodes}` | Execution begins |
| `progress` | `{node_id, node_type, message}` | Before each node |
| `node_complete` | `{node_id, output_preview}` | After each node |
| `complete` | `{results, collectorOutput}` | All done |
| `error` | `{message}` | On failure |

### Modified Files
| File | Change |
|------|--------|
| `devserver/my_app/routes/canvas_routes.py` | New `/api/canvas/execute-stream` endpoint with iterative work-queue |
| `public/ai4artsed-frontend/src/stores/canvas.ts` | `executeWorkflow()` now uses streaming fetch + ReadableStream; added `currentProgress`, `totalNodes`, `completedNodes` refs |
| `public/ai4artsed-frontend/src/views/canvas_workflow.vue` | Progress overlay with spinner, current step message, and node counter |

### Commit
`319313d` - feat(canvas): SSE streaming for live execution progress

---

## Session 136 - Anti-Orientalism Meta-Prompt Enhancement
**Date:** 2026-01-26
**Focus:** Prevent orientalist stereotypes in prompt interception
**Status:** COMPLETED

### Problem
User report: GPT-OSS:120b produced "enormer, furchtbarer exotistischer orientalistischer Kitsch" when processing Nigerian cultural festival prompt. LLM defaulted to orientalist tropes (exotic, mysterious, timeless) despite pedagogical goals.

### Root Cause
Meta-prompt in `instruction_selector.py` lacked explicit anti-stereotype rules. Wikipedia lookup alone insufficient - models need explicit cultural respect principles.

### Solution
Enhanced "transformation" instruction with anti-orientalism rules:
- FORBIDDEN: Exoticizing, romanticizing, mystifying cultural practices
- FORBIDDEN: Orientalist tropes (exotic, mysterious, timeless, ancient wisdom)
- FORBIDDEN: Homogenizing diverse cultures into aesthetic stereotypes
- Equality principle: "Use the same neutral, fact-based approach as for Western contexts"

### Modified Files
| File | Change |
|------|--------|
| `devserver/schemas/engine/instruction_selector.py` | Enhanced "transformation" instruction with CULTURAL RESPECT PRINCIPLES |
| `devserver/schemas/chunks/manipulate.json` | Wikipedia instruction now uses cultural reference language (70+ languages) |
| `devserver/schemas/engine/pipeline_executor.py` | Send REAL Wikipedia results (title, url) to frontend, not just search terms |
| `devserver/my_app/services/wikipedia_service.py` | Expanded SUPPORTED_LANGUAGES from 20 to 70+ (Africa, Asia, Americas, Oceania) |
| `public/ai4artsed-frontend/src/views/text_transformation.vue` | Wikipedia badge uses LoRA design pattern + real URLs; reset on Start button |
| `public/ai4artsed-frontend/src/views/text_transformation.css` | Wikipedia badge reuses lora-stamp classes with color modifier |
| `public/ai4artsed-frontend/src/components/MediaInputBox.vue` | TypeScript types updated for real Wikipedia results |
| `docs/analysis/ORIENTALISM_PROBLEM_2026-01.md` | Complete analysis with test strategy and postcolonial theory references |
| `docs/DEVELOPMENT_DECISIONS.md` | Documented epistemic justice as core architectural principle |

### Key Decisions
1. **Universal application**: Rules apply to ALL configs, not just cultural-specific ones
2. **Explicit FORBIDDEN list**: Concrete examples of prohibited terms
3. **Option A chosen**: Comprehensive instruction (~150 words) for maximum effectiveness
4. **Cultural reference language**: Wikipedia lookup uses cultural context language (not prompt language)
   - **70+ languages** mapped: Africa (15+), Asia (30+), Americas (indigenous languages), Oceania
   - Example: German prompt about Nigeria → uses Hausa/Yoruba/Igbo/English Wikipedia (not German)
   - Example: German prompt about India → uses Hindi/Tamil/Bengali/etc. Wikipedia (not German)
   - Example: German prompt about Peru → uses Spanish/Quechua/Aymara Wikipedia (not German)
   - Rationale: Local Wikipedia communities provide more accurate, less Eurocentric information
5. **No code changes**: Only meta-prompt modifications needed

### Architecture Alignment
- ✅ Supports WAS/WIE principle (anti-stereotype rules are part of "HOW")
- ✅ Reinforces planetarizer/one_world anti-Othering rules
- ✅ Enhances pedagogical goal: visible, criticalizable transformations
- ✅ No conflicts with existing configs

### Testing Status
✅ **PASSED** - Tested with original failing case via `/api/schema/pipeline/stage2`:
- Input: "Das wichtigste Fest im Norden Nigerias" (schema: tellastory)
- Model: GPT-OSS:120b (same as original failing case)
- Result: Factual story about Sallah festival in Kano with:
  - ✅ NO orientalist tropes (exotic, mysterious, timeless)
  - ✅ Specific cultural details (Durbar, Boubou, Kora/Djembe instruments)
  - ✅ Active protagonist (Amina) with agency
  - ✅ Respectful, fact-based tone
- Improvement: From "furchtbarer exotistischer Kitsch" to significantly less orientalist output (though not perfect)

### Theoretical Foundation
Based on postcolonial theory (Said, Fanon, Spivak) - see analysis document for details.

### Additional Fixes (Same Session)

**Wikipedia Badge UI Issues:**
1. **External SVG file caused build errors**
   - Solution: Inline SVG "W" icon (no external file dependency)
2. **Inconsistent design** ("größerer Kasten")
   - Solution: Reuse ALL lora-stamp classes (lora-inner, lora-details, lora-item)
   - Only color modifier: `.wikipedia-lora` for Wikipedia blue (#0066CC)
3. **Badge persisted after Start button**
   - Solution: Reset wikipediaData on runInterception()

**Wikipedia URL Issues:**
1. **Invented links instead of real URLs**
   - Root cause: Backend fetched WikipediaResult but only sent search terms to frontend
   - Solution: Send REAL Wikipedia results (term, lang, title, url, success)
   - Frontend now uses real URLs and displays real article titles
2. **Only 20 languages supported**
   - Root cause: SUPPORTED_LANGUAGES had only 20 codes
   - Solution: Expanded to 70+ languages (ha, yo, ig, qu, ay, mi, etc.)
3. **No debug visibility**
   - Solution: Console logging shows which articles were found with their URLs

**Result:**
- Wikipedia badge matches LoRA badge design exactly
- Links point to REAL Wikipedia articles (no more 404s)
- All 70+ languages now work correctly
- Debug output in console: "Found X Wikipedia articles: lang: title -> url"

### Commits
- `f73bd46` feat(interception): Add anti-orientalism rules and cultural-aware Wikipedia lookup
- `a24fbc0` fix(wikipedia): Use SVG logo and reset data on Start button
- `754b535` docs: Add epistemic justice decision to DEVELOPMENT_DECISIONS.md
- `e929d57` fix(wikipedia): Copy LoRA badge design exactly (inline SVG, shared CSS)
- `4e69051` fix(wikipedia): Use correct language-specific Wikipedia links
- `de32065` fix(wikipedia): Use REAL Wikipedia URLs instead of invented links + 70+ languages

---

## Session 139 - Wikipedia Research Capability
**Date:** 2026-01-26
**Focus:** Enable LLM to fetch Wikipedia content during prompt interception
**Status:** COMPLETED

### Feature Overview
LLM can now request Wikipedia content using `<wiki>term</wiki>` markers in its output. System fetches content and re-executes chunk with enriched context.

### Architecture
- **Chunk-Level Orchestration**: Wikipedia loop in `pipeline_executor._execute_single_step()`
- **NOT a new pipeline** - fundamental capability of ALL interceptions
- **Max 3 iterations** per chunk execution (configurable)

### New Files
| File | Purpose |
|------|---------|
| `my_app/services/wikipedia_service.py` | Secure Wikipedia API client (whitelist-only) |
| `schemas/engine/wikipedia_processor.py` | Marker extraction, content formatting |

### Modified Files
| File | Change |
|------|--------|
| `config.py` | `WIKIPEDIA_MAX_ITERATIONS`, `WIKIPEDIA_FALLBACK_LANGUAGE`, `WIKIPEDIA_CACHE_TTL` |
| `schemas/chunks/manipulate.json` | `{{WIKIPEDIA_CONTEXT}}` placeholder, instruction text |
| `schemas/engine/pipeline_executor.py` | Wikipedia loop, `WIKIPEDIA_STATUS` global for UI |
| `my_app/routes/schema_pipeline_routes.py` | Refactored to use `pipeline_executor` (fixes architecture violation), SSE `wikipedia_lookup` events |
| `MediaInputBox.vue` | Pulsing Wikipedia logo during lookup |

### Trigger Pattern
```
<wiki lang="de">Suchbegriff</wiki>  - German Wikipedia
<wiki lang="en">Search term</wiki>  - English Wikipedia
<wiki>term</wiki>                    - Uses input language
```

### Key Decisions
1. **Language auto-detection**: Uses input language, falls back to `WIKIPEDIA_FALLBACK_LANGUAGE`
2. **Session per-request**: aiohttp session created/closed per lookup (avoids event loop issues with threading)
3. **Architecture fix**: SSE route now uses `pipeline_executor` instead of direct `PromptInterceptionEngine` call

### Commits
- `d66c37f` feat(wikipedia): Core implementation
- `b617273` fix(wikipedia): Import paths
- `277dbf7` fix(wikipedia): Stronger prompt (MUST use when needed)
- `8e03431` feat(wikipedia): Real-time UI feedback
- `761cffa` fix(wikipedia): Session per-request (event loop fix)

---

## Session 138 - Trashy Context-Awareness Fix
**Date:** 2026-01-26
**Focus:** Fix Trashy losing context after pipeline execution
**Status:** COMPLETED

### Problem
Träshy (AI chat helper) was "forgetting" current page context after a pipeline run:

**Root Cause Chain:**
1. User runs pipeline → `runId` gets set via `updateSession()`
2. User changes MediaInputBox content → `runId` stays set (no `clearSession()` call)
3. User opens Träshy → ChatOverlay sees `runId` exists
4. ChatOverlay sends draft_context ONLY if `!runId` (Zeile 213)
5. Backend loads **stale** session context from `exports/json/{runId}/`
6. Träshy doesn't know about current input changes

**Confirmed:** No Vue view calls `clearSession()` → `runId` persists until browser refresh

### Solution: Always Send Draft Context

**Option B (chosen):** Send `draft_context` as separate field, backend combines both contexts.

**Frontend (`ChatOverlay.vue`):**
```javascript
// BEFORE: Conditional logic
if (!currentSession.value.runId && draftContextString.value) {
  messageForBackend = `${draftContextString.value}\n\n${userMessage}`
}

// AFTER: Always send as separate field
const response = await axios.post('/api/chat', {
  message: userMessage,  // Clean message
  run_id: currentSession.value.runId || undefined,
  draft_context: draftContextString.value || undefined,  // NEW: Always send
  history: historyForBackend
})
```

**Backend (`chat_routes.py`):**
```python
draft_context = data.get('draft_context')  # Current page state (transient)

system_prompt = build_system_prompt(context)  # Session context from files

# Append draft_context if provided (NOT saved to exports/)
if draft_context:
    system_prompt += f"\n\n[Aktuelle Eingaben auf der Seite]\n{draft_context}"
```

### Key Points

- `draft_context` is **transient** (LLM context only, not persisted)
- NOT saved to `chat_history.json` or `exports/json/`
- Backend now knows BOTH: session files + current page state
- No changes to exports system

### Result

**Before:**
- Without runId: draft_context sent ✓
- With runId: draft_context ignored ✗

**After:**
- Without runId: draft_context in system prompt ✓
- With runId: BOTH session + draft_context in system prompt ✓

**Commit:** `1fee080` - fix(trashy): Always send draft_context for context-aware chat

---

## Session 137 - Stage 3/4 Separation (Clean Architecture)
**Date:** 2026-01-26
**Focus:** Separate Stage 3 (Translation+Safety) from Stage 4 (Generation) for clean architecture
**Status:** COMPLETED

### Problem
`execute_generation_stage4()` contained **embedded Stage 3 logic**, forcing Canvas to use `skip_translation=True` workaround when it had its own Translation node.

**Old Architecture (problematic):**
```
Canvas: Translation Node → Generation Node (skip_translation=True) → Media
Lab:    execute_generation_stage4() → [Stage 3 embedded] → Stage 4 → Media
```

**Bug Found:** Parameter was `skip_stage3` but code used undefined `skip_translation` variable.

### Solution
Created clean separation with new function `execute_stage4_generation_only()`:

**New Architecture:**
```
Canvas: Translation Node → execute_stage4_generation_only() → Media
Lab:    execute_generation_stage4() → Stage 3 → execute_stage4_generation_only() → Media
```

### Changes

| File | Change |
|------|--------|
| `schema_pipeline_routes.py` | Added `execute_stage4_generation_only()` - pure Stage 4 generation |
| `schema_pipeline_routes.py` | Refactored `execute_generation_stage4()` to call new function internally |
| `canvas_routes.py` | Generation node now calls `execute_stage4_generation_only()` directly |
| `canvas_routes.py` | Removed `skip_translation=True` workaround |

### Key Principle
- `execute_stage4_generation_only()` = Pure generation, expects ready-to-use prompt
- `execute_generation_stage4()` = Legacy wrapper for Lab (handles Stage 3 first)
- Canvas workflows call the clean function directly - no flags needed

**Commit:** `9f34ca2` - refactor(stage4): Separate Stage 3 and Stage 4 for clean architecture

---

## Session 135 - Canvas Cosmetic Fixes & Live Data Flow
**Date:** 2026-01-26
**Focus:** UI polish for Canvas workflow builder + live execution visualization
**Status:** COMPLETED

### Part 1: Initial Fixes

**1. Connection Lines (Initial Attempt - DOM-based)**
- Problem: Lines started from wrong positions (calculated widths didn't match CSS)
- Initial solution: Query DOM positions using `data-node-id` and `data-connector` attributes
- Issue: Vue computed properties don't track DOM changes reactively

**2. Preview/Display Node**
- Made resizable (like Collector)
- Removed 150-char text truncation

**3. Collector Node**
- Removed 200-char text truncation
- Full text display with scrolling

**4. Media Type Icons**
- Google Material icons for Generation node config selection
- Icons based on `mediaType`: image, video, audio/music, text

### Part 2: Connection Line Refactor (Data-Based)

**Problem:** DOM-based approach had timing issues - computed properties run before DOM updates.

**Solution:** Pure data-based connector positioning:
```typescript
const HEADER_CONNECTOR_Y = 24  // Fixed offset from top

function getConnectorPosition(node, connectorType) {
  const width = getNodeWidth(node)  // 280px wide, 180px narrow
  if (connectorType === 'input') {
    return { x: node.x, y: node.y + HEADER_CONNECTOR_Y }
  }
  return { x: node.x + width, y: node.y + HEADER_CONNECTOR_Y }
}
```

**Key insight:** Connectors in HEADER area (fixed Y=24px) don't move when nodes resize.

### Part 3: Evaluation Metadata in Collector

**Problem:** Evaluation score/binary not showing in Collector.

**Solution:** Backend now wraps evaluation output with metadata:
```python
if source_node_type == 'evaluation' and source_metadata:
    collector_item['output'] = {
        'text': input_data,
        'metadata': source_metadata  # { score, binary, active_path }
    }
```

Frontend displays: `Score: 7/10 ✓ Pass`

### Part 4: Output Bubbles (Live Data Flow)

**Feature:** Every node shows a temporary bubble when it produces output.

- Blue speech bubble appears near output connector
- Shows truncated content (60 chars text, icons for media, score for evaluation)
- Animated appearance with scale/fade effect
- Excluded from terminal nodes (collector, display)

### Files Changed

| File | Change |
|------|--------|
| `CanvasWorkspace.vue` | Pure data-based connector positions, node width by type |
| `StageModule.vue` | Header connector CSS (top: 24px), output bubbles, evaluation display |
| `ConfigSelectorModal.vue` | Media type icons |
| `canvas_workflow.vue` | Pass outputConfigs prop |
| `canvas_routes.py` | Evaluation metadata in collector items |

### Commits

- `a4219c2` - feat(canvas): Session 135 - Cosmetic fixes and media type icons
- `b9b692a` - fix(canvas): Data-based connector positioning with header alignment
- `f770405` - fix(canvas): Evaluation metadata handling and display improvements
- `3c58672` - feat(canvas): Output bubbles show data flowing through nodes

---

## Session 134 - Canvas Decision & Evaluation Nodes (Unified Architecture)
**Date:** 2026-01-25 → 2026-01-26
**Focus:** Implement evaluation nodes with 3-output branching logic + Tracer-Pattern execution
**Status:** COMPLETED (Phase 1-4) - Reflexiv agierendes Frontend für genAI

### Pädagogisches Konzept: Evaluation als bewusste Entscheidung

**Kernidee:** Evaluation = EINE konzeptionelle Entscheidung mit 3 Text-Outputs, nicht 7 separate Node-Typen.

**Warum 3 Outputs?**
1. **Passthrough (P)** - Evaluation bestanden → unverändert weiter
2. **Commented (C)** - Evaluation nicht bestanden → mit Feedback zurück
3. **Commentary (→)** - Immer → für User-Transparenz/Display

**Pädagogischer Vorteil:**
- Explizite Entscheidungspunkte im Workflow
- Sichtbares Feedback (nicht "black box")
- Ermöglicht iterative Verbesserung (Feedback Loops)

### Architekturentscheidung: Von 7 Nodes → 1 Node

**Ursprünglicher Plan (verworfen):**
- 5 Evaluation-Types (fairness, creativity, equity, quality, custom)
- 2 Fork-Types (binary_fork, threshold_fork)
= 7 separate Node-Typen

**Problem (User Feedback):**
- Evaluation + Fork = konzeptuell EINE Entscheidung, nicht zwei
- Datenfluss unklar: Was fließt durch Fork? Input? Commentary? Beides?
- UI-Komplexität: 7 Nodes für eine logische Operation

**Lösung: Unified Evaluation Node**
- 1 Node-Typ mit Dropdown für Evaluation-Type
- Optional branching (Checkbox)
- 3 separate TEXT-Outputs (nicht kombiniertes Objekt)

### Implementation - Phase 1: Evaluation Nodes (COMPLETED)

**Frontend (canvas.ts):**
- Node-Type: `'evaluation'`
- Properties: `evaluationType`, `evaluationPrompt`, `outputType`, `enableBranching`, `branchCondition`, `thresholdValue`, `trueLabel`, `falseLabel`

**UI (StageModule.vue):**
```vue
Evaluation Type: [Fairness ▼] (fairness, creativity, equity, quality, custom)
LLM: [gpt-4o-mini ▼]
Criteria: [Textarea mit Pre-fill Templates]
Output: [Commentary+Score ▼]
☑ Enable Branching
  Condition: [Binary/Threshold]
  Threshold: [5.0] (if threshold selected)
  True Label: [Approved]
  False Label: [Needs Revision]
```

**Backend (canvas_routes.py):**
```python
# 3 separate TEXT outputs
outputs = {
  'passthrough': input_text,  # Original unchanged
  'commented': f"{input_text}\n\nFEEDBACK: {commentary}",  # Input + feedback
  'commentary': commentary  # Just commentary
}
metadata = {
  'binary': True/False,
  'score': 0-10,
  'active_path': 'passthrough' | 'commented'
}
```

**Binary Logic (Fixed):**
- LLM-Prompt: "Answer ONLY 'true' or 'false'. If issues or score < 5, answer 'false'"
- Fallback: No binary → use score threshold (< 5.0 = fail)
- Smart parsing: Case-insensitive, multiple variations (true/yes/pass/bestanden)

### Implementation - Phase 2: Display → Preview Node (COMPLETED)

**Problem:** Display-Node zeigte nichts an, hatte nutzloses Dropdown.

**Lösung:** Umbenennung + Inline-Preview
- Label: "Display" → "Preview/Vorschau"
- Removed: title input, displayMode dropdown
- Added: Inline content visualization
  - Text: First 150 chars
  - Images: Inline preview (max 150px)
  - Media: Type + URL display

**UI:**
```
┌──────────────┐
│ 🔍 PREVIEW   │
├──────────────┤
│ [Content...] │ ← Shows execution result
│ [truncated]  │
└──────────────┘
```

### Implementation - Phase 3a: UI for Branching (COMPLETED)

**3 Output Connectors:**
```
┌────────────────┐
│ 📋 EVALUATION  │
└────────────────┘
       ├─ P (🟢 green - Passthrough)
       ├─ C (🟠 orange - Commented)
       └─ → (🔵 cyan - Commentary)
```

**Connection Labels:** `'passthrough'`, `'commented'`, `'commentary'`

**Collector Display:**
- Shows: Binary (✅/❌), Score, Active Path, Commentary, Output Text
- Separate sections for metadata vs. outputs

### Connection Rules (Fixed Multiple Times)

**Problem:** Nodes couldn't connect (e.g., Input → Evaluation).

**Solution:** Extended `acceptsFrom` and `outputsTo` for all nodes:
- Input → can output to: interception, translation, evaluation, display
- Evaluation → accepts from: input, interception, translation, generation, display, evaluation
- All nodes → can connect to evaluation/display

### Technical Debt & Next Steps

**Phase 3b: Conditional Execution (IMPLEMENTED)**
- Connection labels track active path ('passthrough', 'commented', 'commentary', 'feedback')
- Tracer filters connections based on `active_path` metadata
- Only active path executes downstream (commentary always active)

**Phase 4: Feedback Loops (IMPLEMENTED - Tracer Pattern)**

*Ursprünglicher Plan (verworfen):*
- Loop Controller Node + Kahn's Algorithm
- "Scheinlösung" - über-engineered

*Implementierte Lösung:*
- **Tracer Pattern**: Simples rekursives Graph-Tracing
- Feedback-Input-Connector an Interception/Translation Nodes
- Feedback-Connections mit Label `'feedback'`
- Safety-Limit: MAX_TOTAL_EXECUTIONS = 50
- Kein separater Loop Controller Node nötig

*Design Decision:*
> "Wir haben hier keine unkontrollierte Schleifen-Situation im Graph, sondern nichts anderes als eine Loop-End-Konstellation."

Das System ist ein **reflexiv agierendes Frontend für genAI**:
```
Input → Interception → Evaluation → [Score < 5?]
                            ↓ feedback     ↓ pass
                       Interception    Collector
```

### Files Changed

| File | Change |
|------|--------|
| `public/.../types/canvas.ts` | Unified evaluation type, 3-output structure, connection labels, maxFeedbackIterations |
| `public/.../stores/canvas.ts` | Connection label handling, feedback completion functions |
| `public/.../StageModule.vue` | Evaluation UI, 3 connectors, Feedback-Input connector, Preview inline display |
| `public/.../CanvasWorkspace.vue` | Feedback connection handling, event forwarding |
| `public/.../canvas_workflow.vue` | Handler functions for evaluation config |
| `public/.../ModulePalette.vue` | Removed 7 nodes → 1 evaluation node |
| `devserver/.../canvas_routes.py` | **Complete rewrite**: Tracer pattern statt Kahn's Algorithm |

### Commits

1. `feat(session-134): Add Evaluation node types (Phase 1)` - Initial 5 evaluation types
2. `feat(session-134): Add Display node (Phase 2)` - Display node with config
3. `feat(session-134): Add Fork node UI (Phase 3a)` - Binary/Threshold fork UI
4. `fix(session-134): Add new node types to Palette menu` - Palette integration
5. `fix(session-134): Fix connections and enforce binary output` - Binary always enabled
6. `refactor(session-134): Unified Evaluation node (Option A)` - 7 nodes → 1 node
7. `fix(session-134): Enable all nodes to connect to evaluation/display` - Connection rules
8. `feat(session-134): 3 separate TEXT outputs for Evaluation nodes` - 3-output architecture
9. `fix(session-134): Improve binary evaluation logic` - Score threshold fallback
10. `refactor(session-134): Display → Preview with inline content display` - Preview node
11. `fix(session-134): Score-based override for binary evaluation` - Score fallback
12. `refactor(session-134): Simplify evaluation to score-only logic` - Clean score logic
13. `feat(session-134): Phase 3b - Conditional execution for evaluation nodes` - Path filtering
14. `feat(session-134): Phase 4 - Loop Controller with feedback iterations` - Tracer pattern
15. `fix(session-134): TypeScript errors in Phase 4 Loop Controller` - Type fixes

### Testing Results

✅ **Working:**
- Evaluation node with LLM selection
- Type-specific prompt templates (fairness, creativity, equity, quality, custom)
- Binary + Score + Commentary output
- 3 output connectors (P, C, →)
- Preview shows inline content
- Collector displays evaluation results with metadata
- Conditional execution (only active path executes)
- Feedback loops (Evaluation → Interception)
- Safety limit prevents infinite loops

⚠️ **Known Issues (fixed):**
- Binary logic fallback bug → Score-based override
- Conditional execution → Tracer pattern with path filtering

### Architecture Documentation

See: `docs/ARCHITECTURE_CANVAS_EVALUATION_NODES.md` (created this session)

---

## Session 136 - Träshy UX Enhancement: Living Assistant Interface
**Date:** 2026-01-25
**Focus:** Transform Träshy from static icon to living, context-aware assistant
**Status:** COMPLETED

### Pädagogisches Konzept

Träshy ist nicht nur ein Chat-Button, sondern ein **aktiver Begleiter** im kreativen Prozess:
- **Präsenz**: Immer sichtbar, aber nicht störend
- **Aufmerksamkeit**: Folgt dem Fokus des Users (welches Feld aktiv ist)
- **Lebendigkeit**: Sanfte Animationen signalisieren "Ich bin da und bereit zu helfen"
- **Kontext-Bewusstsein**: Weiß was der User gerade tut (Page Context + Focus Tracking)

### Features implementiert

#### 1. Dynamische Positionierung (Y-Achse)
- Träshy folgt dem fokussierten Eingabefeld
- Position berechnet aus `element.getBoundingClientRect()`
- Viewport-Clamping: Bleibt immer sichtbar (nie über oberen/unteren Rand)

#### 2. Focus-Tracking
- `@focus` Events in MediaInputBox
- `focusedField` State: 'input' | 'context' | 'interception' | 'optimization'
- Sofortige Reaktion auf Feldwechsel

#### 3. Pinia Store statt provide/inject
- Problem: ChatOverlay ist Sibling von router-view, nicht Child
- Lösung: `pageContextStore` für komponentenübergreifende Kommunikation
- Views schreiben via `watch()`, ChatOverlay liest

#### 4. Lebendige Animationen
- **Idle-Schweben**: `trashy-idle` (4s) - translate + rotate
- **Atmen**: `trashy-breathe` (3s) - subtle scale pulse
- **Bewegung**: cubic-bezier mit Überschwingen für organisches Gefühl
- Hover pausiert Animation (präzises Klicken)

#### 5. Chat-Fenster Verbesserungen
- Öffnet nach links/unten statt nach oben
- Viewport-Clamping beim Öffnen
- Auto-Focus auf Input nach Antwort

### Technische Änderungen

| Datei | Änderung |
|-------|----------|
| `src/stores/pageContext.ts` | **NEU** - Pinia Store für Page Context |
| `src/composables/usePageContext.ts` | FocusHint Interface hinzugefügt |
| `src/components/ChatOverlay.vue` | Dynamische Positionierung + Animationen |
| `src/components/MediaInputBox.vue` | @focus Event hinzugefügt |
| `src/views/text_transformation.vue` | Focus-Tracking + Element-Refs |
| `src/assets/base.css` | CSS Custom Properties für Layout |

### CSS Custom Properties (base.css)
```css
--footer-collapsed-height: 36px;
--funding-logo-width: 126px;
--layout-gap-small: 8px;
```

### Commits
- `8bb0f96` fix(ui): Remove lightbulb context indicator
- `7f34bfd` feat(ui): Floating Träshy - smooth position transitions
- `a1169c0` fix(ui): Use Pinia store instead of provide/inject
- `1a11637` feat(ui): Dynamic positioning based on element positions
- `54be78d` feat(ui): Träshy follows focused field
- `ed67488` feat(ui): Träshy follows optimization section
- `78c9725` fix(ui): Top positioning when expanded
- `000d8f0` fix(ui): Clamp position to stay within viewport
- `1de7d4e` refactor(css): CSS custom properties for layout
- `bc65f2c` feat(ui): Idle animation - living assistant

---

## Session 135 - Prompt Optimization META-Instruction Fix
**Date:** 2026-01-24
**Focus:** Fix prompt_optimization breaking on certain models
**Status:** COMPLETED

### Problem
The `prompt_optimization` chunk was failing with certain LLM models because the META-instruction was too complex and model-specific variations weren't working consistently.

### Solution
Simplified the META-instruction in `prompt_optimization` to be more universal and less dependent on model-specific quirks.

### Files Changed
- `devserver/my_app/config/chunks/prompt_optimization.json` - Simplified META-instruction

### Commits
- `1261c4a` fix(session-135): Simplify prompt_optimization META-instruction

---

## Session 134 - GPU Auto-Detection & Analog Photography Fixes
**Date:** 2026-01-24
**Focus:** Settings UI improvements, config fixes, instruction injection architecture
**Status:** COMPLETED

### Features
1. **GPU Auto-Detection in Settings UI** - Automatically detects available GPUs
2. **Analog Photography Config Fixes** - Fixed typos in interception configs
3. **Model Evaluation Criteria** - Documented proper evaluation methodology
4. **Instruction Injection Architecture** - Refactored HARDWARE_MATRIX handling

### Files Changed
- `public/ai4artsed-frontend/src/views/SettingsView.vue` - GPU auto-detection
- `devserver/my_app/config/interception_configs/*.json` - Typo fixes
- `docs/HANDOVER_ModelEvaluation_Criteria.md` - Evaluation methodology

### Commits
- `cea6685` feat(session-134): Add GPU auto-detection to Settings UI
- `12a59bf` fix(session-134): Analog photography config typos + evaluation criteria
- `e50bcc7` fix(session-134): Instruction injection architecture + HARDWARE_MATRIX

---

## Session 133 - Träshy Page Context & Canvas Node Improvements
**Date:** 2026-01-24
**Focus:** Chat assistant context awareness, Canvas workflow enhancements
**Status:** COMPLETED

### Major Features

#### 1. Träshy Page Context (provide/inject Pattern)
Träshy (chat assistant) now knows the current page state even before pipeline execution:
- **Composable**: `usePageContext.ts` with `PageContext` interface and `formatPageContextForLLM()` helper
- **Injection Key**: `PAGE_CONTEXT_KEY` for type-safe provide/inject
- **8 Views Updated**: All major views now provide their page context

**Implementation Pattern:**
```typescript
// In views (e.g., text_transformation.vue)
const pageContext = computed(() => ({
  activeViewType: 'text_transformation',
  pageContent: {
    inputText: inputText.value,
    contextPrompt: contextPrompt.value,
    selectedCategory: selectedCategory.value,
    selectedConfig: selectedConfig.value
  }
}))
provide(PAGE_CONTEXT_KEY, pageContext)

// In ChatOverlay.vue
const pageContext = inject(PAGE_CONTEXT_KEY, null)
const draftContextString = computed(() => formatPageContextForLLM(pageContext?.value, route.path))
```

**Priority Logic:**
1. Session context (run_id files) - highest priority
2. Draft page context (from provide/inject) - if no session
3. Route-only fallback - minimal context

#### 2. Canvas Workflow Improvements
- **Collector Node** - Now accepts text from LLM nodes
- **Input Node** - Now accepts prompt text input
- **LLM Endpoint** - Curated model selection for canvas nodes

#### 3. Additional Fixes
- **Ollama Models Dropdown** - `/api/settings/ollama-models` endpoint + `<datalist>` in SettingsView
- **Model Name Trimming** - `.strip()` in chat_routes.py prevents whitespace issues
- **Log String Cleanup** - "STAGE1-GPT-OSS" → "STAGE1-SAFETY", "gpt-oss:" → "llm:"

### Files Changed
- 📝 `src/composables/usePageContext.ts` - **NEW** - Type definitions & formatting
- 📝 `src/components/ChatOverlay.vue` - inject() + context-prepending
- 📝 `src/views/text_transformation.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/image_transformation.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/canvas_workflow.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/direct.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/surrealizer.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/multi_image_transformation.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/partial_elimination.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `src/views/split_and_combine.vue` - provide(PAGE_CONTEXT_KEY)
- 📝 `devserver/my_app/routes/settings_routes.py` - Ollama models endpoint
- 📝 `devserver/my_app/routes/chat_routes.py` - Model name trimming

### Commits
- `833428b` forgotten commits (includes all page context changes)
- `5221b36` feat(session-133): Collector accepts text from LLM nodes
- `26e1f4a` feat(session-133): Input node accepts prompt text
- `9bda8e5` fix(session-133): LLM endpoint with curated model selection

---

## Session 130 - Research Data Architecture: 1 Run = 1 Media Output
**Date:** 2026-01-23
**Duration:** ~3 hours
**Focus:** Clean folder structure for research data, immediate prompt persistence
**Status:** PARTIAL - Core features working, sticky UI TODO

### Goals
1. Implement "1 Run = 1 Media Output" principle
2. Save prompts immediately after LLM generation (not only on user action)
3. Stop logging changes after media generation (run is complete)

### Key Principle: 1 Run = 1 Media Output
Each run folder should contain exactly ONE media product. This prevents:
- Favorites system confusion (multiple images in same folder)
- Research data mixing (different generation contexts in same folder)

**Folder Logic:**
```
Interception (Start1)     → run_001/ created
Generate (FIRST)          → run_001/ continues (no output yet)
Generate (SECOND)         → run_002/ NEW (run_001 has output_*)
Generate (THIRD)          → run_003/ NEW
```

### Implementation

**1. Generation Endpoint - Output Check (`schema_pipeline_routes.py:2038-2066`)**
```python
has_output = any(
    e.get('type', '').startswith('output_')
    for e in existing_recorder.metadata.get('entities', [])
)
if has_output:
    run_id = new_run_id()  # NEW folder
else:
    run_id = provided_run_id  # CONTINUE existing
```

**2. Immediate Prompt Persistence (`schema_pipeline_routes.py:1584-1660`)**
- Optimization streaming now loads recorder at START (same pattern as interception)
- Saves `optimized_prompt` immediately after LLM generation
- Frontend passes `run_id` and `device_id` to optimization endpoint

**3. Stop Logging After Generation (TODO - not working yet)**
- Added `currentRunHasOutput` flag in frontend
- Set to `true` after successful generation
- `logPromptChange()` should skip if flag is true
- Reset on new interception

### Files Changed
- 📝 `devserver/my_app/routes/schema_pipeline_routes.py`
  - Generation endpoint: has_output check for new folder
  - Optimization streaming: recorder at start, immediate save
  - Optimize endpoint: run_id/device_id parameters
- 📝 `public/ai4artsed-frontend/src/views/text_transformation.vue`
  - `optimizationStreamingParams`: added run_id, device_id
  - `currentRunHasOutput` flag (TODO: not working)
  - `logPromptChange()`: skip if run has output

### TODO
- [ ] Fix `currentRunHasOutput` flag not preventing logging after generation
- [ ] Implement "sticky" UI: restore prompts/image when switching modes

### Commits
- `bed0c2c` feat(session-130): 1 Run = 1 Media Output
- `8d07c33` feat(session-130): Save optimized_prompt immediately after LLM generation

---

## Session 117 - LoRA Strength Tuning for Interception Configs
**Date:** 2026-01-17
**Duration:** ~30 minutes
**Focus:** Finding optimal LoRA strength for prompt/style balance
**Status:** SUCCESS - Strength calibrated

### Problem
LoRA injection was working (since Session 114/116), but at strength 1.0 the LoRA effect completely overrode the user's prompt content. Generated images showed only the LoRA style (e.g., film artifacts for Cooked Negatives) with no relevance to the actual prompt.

### Investigation
Compared generations with "Cooked Negatives" interception config:
- **Strength 1.0:** Full film artifact effect, but prompt completely ignored
- **Strength 0.5:** Effect barely visible
- **Strength 0.6:** Good balance - film artifacts visible AND prompt content preserved

### Solution
Adjusted LoRA strength in interception config:
```json
{
  "meta": {
    "loras": [
      {"name": "sd3.5-large_cooked_negatives.safetensors", "strength": 0.6}
    ]
  }
}
```

### Key Insight
**LoRA Strength Trade-off:**
- High strength (0.8-1.0): Style dominates, prompt ignored
- Low strength (0.3-0.5): Style barely visible
- Sweet spot (0.5-0.7): Balance between style and prompt adherence

This varies per LoRA - some are stronger than others. Each interception config should test and calibrate its LoRA strength individually.

### Files Changed
- 📝 `devserver/schemas/configs/interception/cooked_negatives.json` (strength: 1.0 → 0.6)

---

## Session 114 - LoRA Injection for Stage 4 Workflows
**Date:** 2026-01-11
**Duration:** ~2 hours
**Focus:** Dynamic LoRA injection into ComfyUI workflows
**Status:** SUCCESS - LoRA injection working

### Goal
Enable LoRA models (e.g., face LoRAs, style LoRAs) to be automatically injected into Stage 4 image generation workflows.

### Background
Previous Cline session attempted "Dual-Parse" architecture (parsing `<lora:name:strength>` tags from prompts) but failed due to architectural issues. This session took a simpler approach: implement the injection mechanism first, decide WHERE the LoRA list comes from later.

### Solution: Workflow-Based LoRA Injection

**Key Insight:** Separate the injection mechanism from the data source.
1. Define LoRA list in `config.py` (temporary hardcoded)
2. Inject LoRALoader nodes into workflow at runtime
3. Later: connect to Stage2-Configs (Meta-Prompt + optimal LoRAs)

### Implementation

#### 1. Config (`config.py`)
```python
LORA_TRIGGERS = [
    {"name": "SD3.5-Large-Anime-LoRA.safetensors", "strength": 1.0},
    {"name": "bejo_face.safetensors", "strength": 1.0},
]
```

#### 2. Injection Logic (`backend_router.py`)
New method `_inject_lora_nodes()`:
- Finds CheckpointLoaderSimple node (model source)
- Finds model consumers (KSampler)
- Inserts LoRALoader nodes in chain: Checkpoint → LoRA1 → LoRA2 → KSampler
- Updates node connections automatically

#### 3. Routing Change
When `LORA_TRIGGERS` is configured, images use workflow mode instead of simple SwarmUI API:
```python
if LORA_TRIGGERS:
    return await self._process_workflow_chunk(...)
else:
    return await self._process_image_chunk_simple(...)
```

### Log Output (Success)
```
[LORA] Using workflow mode for image generation (LoRAs configured)
[LORA] Injected LoraLoader node 12: SD3.5-Large-Anime-LoRA.safetensors
[LORA] Injected LoraLoader node 13: bejo_face.safetensors
[LORA] Updated node 8 to receive model from LoRA chain
```

### Test Results
- ✅ Face LoRA (bejo_face) visible in output - works WITHOUT trigger word
- ✅ Multiple LoRAs chain correctly
- ✅ Workflow submission successful
- ⚠️ Style LoRA may need trigger word in prompt for visible effect

### Next Steps
**Connect to Stage2-Configs:** Each interception config (Meta-Prompt) can define optimal LoRAs:
```json
{
  "name": "jugendsprache",
  "context_prompt": "...",
  "loras": [
    {"name": "anime_style.safetensors", "strength": 0.8}
  ]
}
```

### Files Changed
- 📝 `devserver/config.py` (+10 lines - LORA_TRIGGERS config)
- 🔧 `devserver/schemas/engine/backend_router.py` (+80 lines - injection logic)

---

## Session 113 - SwarmUI Auto-Recovery System
**Date:** 2026-01-11
**Duration:** ~2 hours
**Focus:** Automatic SwarmUI lifecycle management
**Status:** SUCCESS - Full auto-recovery implemented

### Problem
DevServer crashed when SwarmUI was not running:
```
ClientConnectorError: Cannot connect to host 127.0.0.1:7821
```
**User Requirement:** DevServer should automatically detect and start SwarmUI when needed.

### Solution: SwarmUI Manager Service
Created new singleton service `swarmui_manager.py` for lifecycle management:

**Architecture Pattern:** Lazy Recovery (On-Demand)
- SwarmUI starts **only when needed** (not at DevServer startup)
- Handles both startup scenarios AND runtime crashes
- Faster DevServer startup
- Race-condition safe with `asyncio.Lock`

### Implementation

#### 1. SwarmUI Manager Service (`devserver/my_app/services/swarmui_manager.py`)
**Core Methods:**
- `ensure_swarmui_available()` - Main entry point, guarantees SwarmUI is running
- `is_healthy()` - Checks both ports (7801 REST API + 7821 ComfyUI backend)
- `_start_swarmui()` - Executes `2_start_swarmui.sh` via subprocess.Popen
- `_wait_for_ready()` - Polls health endpoints until ready or timeout (120s)

**Concurrency Safety:**
- `asyncio.Lock` prevents multiple threads from starting SwarmUI simultaneously
- Double-check pattern after acquiring lock

#### 2. Integration Points (5 locations)
**LegacyWorkflowService** (`legacy_workflow_service.py:95`):
- `ensure_swarmui_available()` before workflow submission

**BackendRouter** (`backend_router.py`):
- Line 150: Manager initialization in constructor
- Line 550: Before SwarmUI Text2Image generation
- Line 684: Before SwarmUI workflow submission
- Line 893: Before single image upload
- Line 941: Before multi-image upload

#### 3. Configuration (`config.py`)
```python
SWARMUI_AUTO_START = os.environ.get("SWARMUI_AUTO_START", "true").lower() == "true"
SWARMUI_STARTUP_TIMEOUT = int(os.environ.get("SWARMUI_STARTUP_TIMEOUT", "120"))  # seconds
SWARMUI_HEALTH_CHECK_INTERVAL = float(os.environ.get("SWARMUI_HEALTH_CHECK_INTERVAL", "2.0"))  # seconds
```

#### 4. Browser Tab Prevention
**Problem:** SwarmUI opened browser tab on startup, hiding the frontend.

**Solution:** Command-line override in `2_start_swarmui.sh`:
```bash
./launch-linux.sh --launch_mode none
```

**Why This Works:**
- SwarmUI supports `--launch_mode` command-line argument
- Overrides `LaunchMode: web` setting in `Settings.fds`
- Works on ANY SwarmUI installation (no settings file modification needed)

### Expected Behavior
**Before:**
```
[ERROR] Cannot connect to host 127.0.0.1:7821
[ERROR] Workflow execution failed
```

**After:**
```
[SWARMUI-TEXT2IMAGE] Ensuring SwarmUI is available...
[SWARMUI-MANAGER] SwarmUI not available, starting...
[SWARMUI-MANAGER] Starting SwarmUI via: /path/to/2_start_swarmui.sh
[SWARMUI-MANAGER] SwarmUI process started (PID: 12345)
[SWARMUI-MANAGER] Waiting for SwarmUI (timeout: 120s)...
[SWARMUI-MANAGER] ✓ SwarmUI ready! (took 45.2s)
[SWARMUI] ✓ Generated 1 image(s)
```

### Benefits
- ✅ DevServer starts independently of SwarmUI
- ✅ Automatic crash recovery at runtime
- ✅ No manual intervention needed
- ✅ Frontend stays in focus (no SwarmUI UI popup)
- ✅ Works with any SwarmUI installation
- ✅ Configurable via environment variables

### Files Changed
- ✨ `devserver/my_app/services/swarmui_manager.py` (NEW - 247 lines)
- 📝 `devserver/config.py` (+7 lines - Auto-recovery configuration)
- 🔧 `devserver/my_app/services/legacy_workflow_service.py` (+9 lines - Manager integration)
- 🔧 `devserver/schemas/engine/backend_router.py` (+41 lines - Manager integration)
- 🚀 `2_start_swarmui.sh` (+3 lines - `--launch_mode none`)

**Commit:** `bbe04d8` - feat(swarmui): Add auto-recovery with SwarmUI Manager service

### Architecture Updates
- Updated ARCHITECTURE PART 07 - Engine-Modules.md (SwarmUI Manager)
- Updated ARCHITECTURE PART 08 - Backend-Routing.md (Auto-recovery integration)

---

## Session 112 - CRITICAL: Fix Streaming Connection Leak (CLOSE_WAIT) & Queue Implementation
**Date:** 2026-01-08
**Duration:** ~2 hours
**Focus:** Fix connection leak and concurrent request overload (Ollama)
**Status:** SUCCESS - Connection cleanup implemented, Queue implemented

### Problem 1: Connection Leak (CLOSE_WAIT)
Production system (lab.ai4artsed.org) experiencing streaming failures:
- Cloudflared tunnel logs: "stream X canceled by remote with error code 0"
- Backend accumulating connections in CLOSE_WAIT state
- Eventually all streaming requests failing

### Fix 1: Streaming Cleanup
Implemented `GeneratorExit` handling and explicit `response.close()` in streaming generators:
1. `/devserver/schemas/engine/prompt_interception_engine.py:381`
2. `/devserver/my_app/services/ollama_service.py:366`
3. `/devserver/my_app/routes/schema_pipeline_routes.py:1278`

Result: CLOSE_WAIT connections now clear properly (tested with load test).

### Problem 2: Ollama Overload (Timeouts)
Under load (e.g. 10 parallel requests), Ollama (120b model) gets overloaded.
- Requests time out after 90s (default `OLLAMA_TIMEOUT`)
- Model execution takes 100-260s
- Parallel requests cause congestion and failures

### Fix 2: Request Queueing & Timeouts
1. **Request Queue:**
   - Implemented `threading.Semaphore(3)` in `schema_pipeline_routes.py`.
   - Limits concurrent heavy model executions to 3 (others wait).
   - Applied to Stage 1 safety checks in `execute_pipeline_streaming`, `execute_pipeline` (POST), and `execute_stage2`.

2. **Timeout Increase:**
   - Increased `OLLAMA_TIMEOUT` in `config.py` from 90s to 300s.

3. **Bug Fix:**
   - Fixed `SyntaxError` in `streaming_response.py` (f-string syntax) that prevented backend startup.

### Test Results
**Load Test (10 concurrent requests):**
- Backend: Running on port 17802 (Dev script)
- Queue Logic: Verified in logs
  ```
  [OLLAMA-QUEUE] Initialized with max concurrent requests: 3
  [OLLAMA-QUEUE] Stream ...: Waiting for queue slot...
  [OLLAMA-QUEUE] Stream ...: Acquired slot...
  [OLLAMA-QUEUE] Stream ...: Released slot
  ```
- All requests queued and processed sequentially without timeout errors.

### Fix 3: User Feedback (Queue Visualization)
1. **Backend (SSE):**
   - Updated `execute_pipeline_streaming` to yield `queue_status` events while waiting in queue.
   - Frequency: Every 1 second.
   - Payload: `{'status': 'waiting', 'message': 'Warte auf freien Slot... (Xs)'}`.

2. **Frontend (MediaInputBox.vue):**
   - Added listener for `queue_status` event.
   - Visual Feedback:
     - Spinner turns **RED** (`.spinner-large.queued`) when status is 'waiting'.
     - Loading text pulses red and shows queue message.
     - Automatically resets to normal (blue) when slot is acquired.

### Next Steps
- Monitor production after deployment.

---

## Session 111 - CRITICAL: Unified Streaming Architecture Refactoring
**Date:** 2025-12-28
**Duration:** ~4 hours
- Supports both emoji and string icon names ('lightbulb', 'clipboard', etc.)
